
from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])
        src_crop = sct.add_suffix(src, '_crop')
        sct.run('sct_crop_image -i '+src+' -o '+src_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        dest_crop = sct.add_suffix(dest, '_crop')
        sct.run('sct_crop_image -i '+dest+' -o '+dest_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        src = src_crop
        dest = dest_crop
        cmd = ('isct_antsSliceRegularizedRegistration '
               '-t Translation[0.5] '
               '-m '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+',Regular,0.2] '
               '-p '+paramreg.steps[i_step_str].poly+' '
               '-i '+paramreg.steps[i_step_str].iter+' '
               '-f 1 '
               '-s '+paramreg.steps[i_step_str].smooth+' '
               +masking)
        warp_forward_out = 'step'+i_step_str+'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str+'InverseWarp.nii.gz'

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_pointwise':
        from msct_register import register_slicereg2d_pointwise
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_pointwise(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                      warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, verbose=param.verbose)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_translation':
        from msct_register import register_slicereg2d_translation
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_translation(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                        fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                        verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_rigid':
        from msct_register import register_slicereg2d_rigid
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_rigid(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Rigid', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                  fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                  verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_affine':
        from msct_register import register_slicereg2d_affine
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_affine(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Affine', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                   fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                   verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_syn':
        from msct_register import register_slicereg2d_syn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_syn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='SyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_bsplinesyn':
        from msct_register import register_slicereg2d_bsplinesyn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_bsplinesyn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='BSplineSyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                       fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                       verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo.lower() in ants_registration_params:
        if not paramreg.steps[i_step_str].iter == '0':
            dest_pad = sct.add_suffix(dest, '_pad')
            sct.run('sct_maths -i '+dest+' -o '+dest_pad+' -pad 0x0x'+str(param.padding))
            dest = dest_pad

        cmd = ('isct_antsRegistration '
               '--dimensionality 3 '
               '--transform '+paramreg.steps[i_step_str].algo+'['+paramreg.steps[i_step_str].gradStep +
               ants_registration_params[paramreg.steps[i_step_str].algo.lower()]+'] '
               '--metric '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+'] '
               '--convergence '+paramreg.steps[i_step_str].iter+' '
               '--shrink-factors '+paramreg.steps[i_step_str].shrink+' '
               '--smoothing-sigmas '+paramreg.steps[i_step_str].smooth+'mm '
               '--restrict-deformation 1x1x0 '
               '--output [step'+i_step_str+','+src+'_regStep'+i_step_str+'.nii] '
               '--interpolation BSpline[3] '
               +masking)
        if param.verbose >= 1:
            cmd += ' --verbose 1'
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward_out = 'step'+i_step_str+'0GenericAffine.mat'
            warp_inverse_out = '-step'+i_step_str+'0GenericAffine.mat'
        else:
            warp_forward_out = 'step'+i_step_str+'0Warp.nii.gz'
            warp_inverse_out = 'step'+i_step_str+'0InverseWarp.nii.gz'
    else:
        sct.printv('\nERROR: algo '+paramreg.steps[i_step_str].algo+' does not exist. Exit program\n', 1, 'error')

    status, output = sct.run(cmd, param.verbose)

    if not os.path.isfile(warp_forward_out):
        sct.printv('\nERROR: file '+warp_forward_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    elif not os.path.isfile(warp_inverse_out):
        sct.printv('\nERROR: file '+warp_inverse_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    else:
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward = 'warp_forward_'+i_step_str+'.mat'
            os.rename(warp_forward_out, warp_forward)
            warp_inverse = '-warp_forward_'+i_step_str+'.mat'
        else:
            warp_forward = 'warp_forward_'+i_step_str+'.nii.gz'
            warp_inverse = 'warp_inverse_'+i_step_str+'.nii.gz'
            os.rename(warp_forward_out, warp_forward)
            os.rename(warp_inverse_out, warp_inverse)

    print '*********************************************************************************************************************\n'
    print warp_forward, warp_inverse
    print '*********************************************************************************************************************\n'

    return warp_forward, warp_inverse



from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])

from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])
        src_crop = sct.add_suffix(src, '_crop')
        sct.run('sct_crop_image -i '+src+' -o '+src_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        dest_crop = sct.add_suffix(dest, '_crop')
        sct.run('sct_crop_image -i '+dest+' -o '+dest_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        src = src_crop
        dest = dest_crop
        cmd = ('isct_antsSliceRegularizedRegistration '
               '-t Translation[0.5] '
               '-m '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+',Regular,0.2] '
               '-p '+paramreg.steps[i_step_str].poly+' '
               '-i '+paramreg.steps[i_step_str].iter+' '
               '-f 1 '
               '-s '+paramreg.steps[i_step_str].smooth+' '
               +masking)
        warp_forward_out = 'step'+i_step_str+'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str+'InverseWarp.nii.gz'

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_pointwise':
        from msct_register import register_slicereg2d_pointwise
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_pointwise(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                      warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, verbose=param.verbose)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_translation':
        from msct_register import register_slicereg2d_translation
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_translation(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                        fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                        verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_rigid':
        from msct_register import register_slicereg2d_rigid
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_rigid(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Rigid', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                  fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                  verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_affine':
        from msct_register import register_slicereg2d_affine
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_affine(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Affine', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                   fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                   verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_syn':
        from msct_register import register_slicereg2d_syn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_syn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='SyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_bsplinesyn':
        from msct_register import register_slicereg2d_bsplinesyn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_bsplinesyn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='BSplineSyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                       fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                       verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo.lower() in ants_registration_params:
        if not paramreg.steps[i_step_str].iter == '0':
            dest_pad = sct.add_suffix(dest, '_pad')
            sct.run('sct_maths -i '+dest+' -o '+dest_pad+' -pad 0x0x'+str(param.padding))
            dest = dest_pad

        cmd = ('isct_antsRegistration '
               '--dimensionality 3 '
               '--transform '+paramreg.steps[i_step_str].algo+'['+paramreg.steps[i_step_str].gradStep +
               ants_registration_params[paramreg.steps[i_step_str].algo.lower()]+'] '
               '--metric '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+'] '
               '--convergence '+paramreg.steps[i_step_str].iter+' '
               '--shrink-factors '+paramreg.steps[i_step_str].shrink+' '
               '--smoothing-sigmas '+paramreg.steps[i_step_str].smooth+'mm '
               '--restrict-deformation 1x1x0 '
               '--output [step'+i_step_str+','+src+'_regStep'+i_step_str+'.nii] '
               '--interpolation BSpline[3] '
               +masking)
        if param.verbose >= 1:
            cmd += ' --verbose 1'
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward_out = 'step'+i_step_str+'0GenericAffine.mat'
            warp_inverse_out = '-step'+i_step_str+'0GenericAffine.mat'
        else:
            warp_forward_out = 'step'+i_step_str+'0Warp.nii.gz'
            warp_inverse_out = 'step'+i_step_str+'0InverseWarp.nii.gz'
    else:
        sct.printv('\nERROR: algo '+paramreg.steps[i_step_str].algo+' does not exist. Exit program\n', 1, 'error')

    status, output = sct.run(cmd, param.verbose)

    if not os.path.isfile(warp_forward_out):
        sct.printv('\nERROR: file '+warp_forward_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    elif not os.path.isfile(warp_inverse_out):
        sct.printv('\nERROR: file '+warp_inverse_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    else:
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward = 'warp_forward_'+i_step_str+'.mat'
            os.rename(warp_forward_out, warp_forward)
            warp_inverse = '-warp_forward_'+i_step_str+'.mat'
        else:
            warp_forward = 'warp_forward_'+i_step_str+'.nii.gz'
            warp_inverse = 'warp_inverse_'+i_step_str+'.nii.gz'
            os.rename(warp_forward_out, warp_forward)
            os.rename(warp_inverse_out, warp_inverse)

    print '*********************************************************************************************************************\n'
    print warp_forward, warp_inverse
    print '*********************************************************************************************************************\n'

    return warp_forward, warp_inverse



from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])
        src_crop = sct.add_suffix(src, '_crop')
        sct.run('sct_crop_image -i '+src+' -o '+src_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        dest_crop = sct.add_suffix(dest, '_crop')
        sct.run('sct_crop_image -i '+dest+' -o '+dest_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        src = src_crop
        dest = dest_crop
        cmd = ('isct_antsSliceRegularizedRegistration '
               '-t Translation[0.5] '
               '-m '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+',Regular,0.2] '
               '-p '+paramreg.steps[i_step_str].poly+' '
               '-i '+paramreg.steps[i_step_str].iter+' '
               '-f 1 '
               '-s '+paramreg.steps[i_step_str].smooth+' '
               +masking)
        warp_forward_out = 'step'+i_step_str+'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str+'InverseWarp.nii.gz'

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_pointwise':
        from msct_register import register_slicereg2d_pointwise
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_pointwise(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                      warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, verbose=param.verbose)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_translation':
        from msct_register import register_slicereg2d_translation
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_translation(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                        fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                        verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_rigid':
        from msct_register import register_slicereg2d_rigid
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_rigid(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Rigid', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                  fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                  verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_affine':
        from msct_register import register_slicereg2d_affine
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_affine(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Affine', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                   fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                   verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_syn':
        from msct_register import register_slicereg2d_syn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_syn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='SyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_bsplinesyn':
        from msct_register import register_slicereg2d_bsplinesyn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_bsplinesyn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='BSplineSyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                       fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                       verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo.lower() in ants_registration_params:
        if not paramreg.steps[i_step_str].iter == '0':
            dest_pad = sct.add_suffix(dest, '_pad')
            sct.run('sct_maths -i '+dest+' -o '+dest_pad+' -pad 0x0x'+str(param.padding))
            dest = dest_pad

        cmd = ('isct_antsRegistration '
               '--dimensionality 3 '
               '--transform '+paramreg.steps[i_step_str].algo+'['+paramreg.steps[i_step_str].gradStep +
               ants_registration_params[paramreg.steps[i_step_str].algo.lower()]+'] '
               '--metric '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+'] '
               '--convergence '+paramreg.steps[i_step_str].iter+' '
               '--shrink-factors '+paramreg.steps[i_step_str].shrink+' '
               '--smoothing-sigmas '+paramreg.steps[i_step_str].smooth+'mm '
               '--restrict-deformation 1x1x0 '
               '--output [step'+i_step_str+','+src+'_regStep'+i_step_str+'.nii] '
               '--interpolation BSpline[3] '
               +masking)
        if param.verbose >= 1:
            cmd += ' --verbose 1'
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward_out = 'step'+i_step_str+'0GenericAffine.mat'
            warp_inverse_out = '-step'+i_step_str+'0GenericAffine.mat'
        else:
            warp_forward_out = 'step'+i_step_str+'0Warp.nii.gz'
            warp_inverse_out = 'step'+i_step_str+'0InverseWarp.nii.gz'
    else:
        sct.printv('\nERROR: algo '+paramreg.steps[i_step_str].algo+' does not exist. Exit program\n', 1, 'error')

    status, output = sct.run(cmd, param.verbose)

    if not os.path.isfile(warp_forward_out):
        sct.printv('\nERROR: file '+warp_forward_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    elif not os.path.isfile(warp_inverse_out):
        sct.printv('\nERROR: file '+warp_inverse_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    else:
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward = 'warp_forward_'+i_step_str+'.mat'
            os.rename(warp_forward_out, warp_forward)
            warp_inverse = '-warp_forward_'+i_step_str+'.mat'
        else:
            warp_forward = 'warp_forward_'+i_step_str+'.nii.gz'
            warp_inverse = 'warp_inverse_'+i_step_str+'.nii.gz'
            os.rename(warp_forward_out, warp_forward)
            os.rename(warp_inverse_out, warp_inverse)

    print '*********************************************************************************************************************\n'
    print warp_forward, warp_inverse
    print '*********************************************************************************************************************\n'

    return warp_forward, warp_inverse



from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])
        src_crop = sct.add_suffix(src, '_crop')
        sct.run('sct_crop_image -i '+src+' -o '+src_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        dest_crop = sct.add_suffix(dest, '_crop')
        sct.run('sct_crop_image -i '+dest+' -o '+dest_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        src = src_crop
        dest = dest_crop
        cmd = ('isct_antsSliceRegularizedRegistration '
               '-t Translation[0.5] '
               '-m '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+',Regular,0.2] '
               '-p '+paramreg.steps[i_step_str].poly+' '
               '-i '+paramreg.steps[i_step_str].iter+' '
               '-f 1 '
               '-s '+paramreg.steps[i_step_str].smooth+' '
               +masking)
        warp_forward_out = 'step'+i_step_str+'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str+'InverseWarp.nii.gz'

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_pointwise':
        from msct_register import register_slicereg2d_pointwise
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_pointwise(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                      warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, verbose=param.verbose)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_translation':
        from msct_register import register_slicereg2d_translation
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_translation(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                        fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                        verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_rigid':
        from msct_register import register_slicereg2d_rigid
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_rigid(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Rigid', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                  fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                  verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_affine':
        from msct_register import register_slicereg2d_affine
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_affine(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Affine', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                   fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                   verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_syn':
        from msct_register import register_slicereg2d_syn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_syn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='SyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_bsplinesyn':
        from msct_register import register_slicereg2d_bsplinesyn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_bsplinesyn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='BSplineSyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                       fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                       verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo.lower() in ants_registration_params:
        if not paramreg.steps[i_step_str].iter == '0':
            dest_pad = sct.add_suffix(dest, '_pad')
            sct.run('sct_maths -i '+dest+' -o '+dest_pad+' -pad 0x0x'+str(param.padding))
            dest = dest_pad

        cmd = ('isct_antsRegistration '
               '--dimensionality 3 '
               '--transform '+paramreg.steps[i_step_str].algo+'['+paramreg.steps[i_step_str].gradStep +
               ants_registration_params[paramreg.steps[i_step_str].algo.lower()]+'] '
               '--metric '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+'] '
               '--convergence '+paramreg.steps[i_step_str].iter+' '
               '--shrink-factors '+paramreg.steps[i_step_str].shrink+' '
               '--smoothing-sigmas '+paramreg.steps[i_step_str].smooth+'mm '
               '--restrict-deformation 1x1x0 '
               '--output [step'+i_step_str+','+src+'_regStep'+i_step_str+'.nii] '
               '--interpolation BSpline[3] '
               +masking)
        if param.verbose >= 1:
            cmd += ' --verbose 1'
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward_out = 'step'+i_step_str+'0GenericAffine.mat'
            warp_inverse_out = '-step'+i_step_str+'0GenericAffine.mat'
        else:
            warp_forward_out = 'step'+i_step_str+'0Warp.nii.gz'
            warp_inverse_out = 'step'+i_step_str+'0InverseWarp.nii.gz'
    else:
        sct.printv('\nERROR: algo '+paramreg.steps[i_step_str].algo+' does not exist. Exit program\n', 1, 'error')

    status, output = sct.run(cmd, param.verbose)

    if not os.path.isfile(warp_forward_out):
        sct.printv('\nERROR: file '+warp_forward_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    elif not os.path.isfile(warp_inverse_out):
        sct.printv('\nERROR: file '+warp_inverse_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    else:
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward = 'warp_forward_'+i_step_str+'.mat'
            os.rename(warp_forward_out, warp_forward)
            warp_inverse = '-warp_forward_'+i_step_str+'.mat'
        else:
            warp_forward = 'warp_forward_'+i_step_str+'.nii.gz'
            warp_inverse = 'warp_inverse_'+i_step_str+'.nii.gz'
            os.rename(warp_forward_out, warp_forward)
            os.rename(warp_inverse_out, warp_inverse)

    print '*********************************************************************************************************************\n'
    print warp_forward, warp_inverse
    print '*********************************************************************************************************************\n'

    return warp_forward, warp_inverse



from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])
        src_crop = sct.add_suffix(src, '_crop')
        sct.run('sct_crop_image -i '+src+' -o '+src_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        dest_crop = sct.add_suffix(dest, '_crop')
        sct.run('sct_crop_image -i '+dest+' -o '+dest_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        src = src_crop
        dest = dest_crop
        cmd = ('isct_antsSliceRegularizedRegistration '
               '-t Translation[0.5] '
               '-m '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+',Regular,0.2] '
               '-p '+paramreg.steps[i_step_str].poly+' '
               '-i '+paramreg.steps[i_step_str].iter+' '
               '-f 1 '
               '-s '+paramreg.steps[i_step_str].smooth+' '
               +masking)
        warp_forward_out = 'step'+i_step_str+'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str+'InverseWarp.nii.gz'

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_pointwise':
        from msct_register import register_slicereg2d_pointwise
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_pointwise(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                      warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, verbose=param.verbose)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_translation':
        from msct_register import register_slicereg2d_translation
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_translation(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                        fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                        verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_rigid':
        from msct_register import register_slicereg2d_rigid
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_rigid(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Rigid', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                  fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                  verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_affine':
        from msct_register import register_slicereg2d_affine
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_affine(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Affine', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                   fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                   verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_syn':
        from msct_register import register_slicereg2d_syn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_syn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='SyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_bsplinesyn':
        from msct_register import register_slicereg2d_bsplinesyn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_bsplinesyn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='BSplineSyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                       fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                       verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo.lower() in ants_registration_params:
        if not paramreg.steps[i_step_str].iter == '0':
            dest_pad = sct.add_suffix(dest, '_pad')
            sct.run('sct_maths -i '+dest+' -o '+dest_pad+' -pad 0x0x'+str(param.padding))
            dest = dest_pad

        cmd = ('isct_antsRegistration '
               '--dimensionality 3 '
               '--transform '+paramreg.steps[i_step_str].algo+'['+paramreg.steps[i_step_str].gradStep +
               ants_registration_params[paramreg.steps[i_step_str].algo.lower()]+'] '
               '--metric '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+'] '
               '--convergence '+paramreg.steps[i_step_str].iter+' '
               '--shrink-factors '+paramreg.steps[i_step_str].shrink+' '
               '--smoothing-sigmas '+paramreg.steps[i_step_str].smooth+'mm '
               '--restrict-deformation 1x1x0 '
               '--output [step'+i_step_str+','+src+'_regStep'+i_step_str+'.nii] '
               '--interpolation BSpline[3] '
               +masking)
        if param.verbose >= 1:
            cmd += ' --verbose 1'
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward_out = 'step'+i_step_str+'0GenericAffine.mat'
            warp_inverse_out = '-step'+i_step_str+'0GenericAffine.mat'
        else:
            warp_forward_out = 'step'+i_step_str+'0Warp.nii.gz'
            warp_inverse_out = 'step'+i_step_str+'0InverseWarp.nii.gz'
    else:
        sct.printv('\nERROR: algo '+paramreg.steps[i_step_str].algo+' does not exist. Exit program\n', 1, 'error')

    status, output = sct.run(cmd, param.verbose)

    if not os.path.isfile(warp_forward_out):
        sct.printv('\nERROR: file '+warp_forward_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    elif not os.path.isfile(warp_inverse_out):
        sct.printv('\nERROR: file '+warp_inverse_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    else:
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward = 'warp_forward_'+i_step_str+'.mat'
            os.rename(warp_forward_out, warp_forward)
            warp_inverse = '-warp_forward_'+i_step_str+'.mat'
        else:
            warp_forward = 'warp_forward_'+i_step_str+'.nii.gz'
            warp_inverse = 'warp_inverse_'+i_step_str+'.nii.gz'
            os.rename(warp_forward_out, warp_forward)
            os.rename(warp_inverse_out, warp_inverse)

    print '*********************************************************************************************************************\n'
    print warp_forward, warp_inverse
    print '*********************************************************************************************************************\n'

    return warp_forward, warp_inverse




from autobahn.twisted.websocket import WebSocketClientProtocol, \
    WebSocketClientFactory

from twisted.internet.defer import Deferred, inlineCallbacks


def sleep(delay):
    d = Deferred()
    reactor.callLater(delay, d.callback, None)
    return d


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print("Server connected: {0}".format(response.peer))

    @inlineCallbacks
    def onOpen(self):
        print("WebSocket connection open.")

        while True:
            self.sendMessage(u"Hello, world!".encode('utf8'))
            self.sendMessage(b"\x00\x01\x03\x04", isBinary=True)
            yield sleep(1)

    def onMessage(self, payload, isBinary):
        if isBinary:
            print("Binary message received: {0} bytes".format(len(payload)))
        else:
            print("Text message received: {0}".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print("WebSocket connection closed: {0}".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(u"ws://127.0.0.1:9000", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP("127.0.0.1", 9000, factory)
    reactor.run()
import threading
from binascii import b2a_hex


def main():
    num_threads = 10
    use_threads = True

    if not use_threads:
        runShapelyBuilding()
    else:
        threads = [threading.Thread(target=runShapelyBuilding, name=str(i),
                                    args=(i,)) for i in range(num_threads)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()


def runShapelyBuilding(num):
    print("%s: Running shapely tests on wkb" % num)
    import shapely.geos
    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    import shapely.wkt
    import shapely.wkb
    p = shapely.wkt.loads("POINT (0 0)")
    print("%s WKT: %s" % (num, shapely.wkt.dumps(p)))
    wkb = shapely.wkb.dumps(p)
    print("%s WKB: %s" % (num, b2a_hex(wkb)))

    for i in range(10):
        shapely.wkb.loads(wkb)

    print("%s GEOS Handle: %s" % (num, shapely.geos.lgeos.geos_handle))
    print("Done %s" % num)


if __name__ == '__main__':
    main()
import gc
import os
import time
from datetime import datetime, date, timedelta
from optparse import make_option

from django.core.files.storage import get_storage_class
from django.core.management.base import BaseCommand
from easy_thumbnails.conf import settings
from easy_thumbnails.models import Source


class ThumbnailCollectionCleaner(object):
    

    sources = 0
    thumbnails = 0
    thumbnails_deleted = 0
    source_refs_deleted = 0
    execution_time = 0

    def _get_absolute_path(self, path):
        return os.path.join(settings.MEDIA_ROOT, path)

    def _get_relative_path(self, path):
        return os.path.relpath(path, settings.MEDIA_ROOT)

    def _check_if_exists(self, storage, path):
        try:
            return storage.exists(path)
        except Exception as e:
            print("Something went wrong when checking existance of %s:" % path)
            print(str(e))

    def _delete_sources_by_id(self, ids):
        Source.objects.all().filter(id__in=ids).delete()

    def clean_up(self, dry_run=False, verbosity=1, last_n_days=0,
                 cleanup_path=None, storage=None):
        

        if dry_run:
            print ("Dry run...")

        if not storage:
            storage = get_storage_class(settings.THUMBNAIL_DEFAULT_STORAGE)()

        sources_to_delete = []
        time_start = time.time()

        query = Source.objects.all()
        if last_n_days > 0:
            today = date.today()
            query = query.filter(
                modified__range=(today - timedelta(days=last_n_days), today))
        if cleanup_path:
            query = query.filter(name__startswith=cleanup_path)

        for source in queryset_iterator(query):
            self.sources += 1
            abs_source_path = self._get_absolute_path(source.name)

            if not self._check_if_exists(storage, abs_source_path):
                if verbosity > 0:
                    print ("Source not present:", abs_source_path)
                self.source_refs_deleted += 1
                sources_to_delete.append(source.id)

                for thumb in source.thumbnails.all():
                    self.thumbnails_deleted += 1
                    abs_thumbnail_path = self._get_absolute_path(thumb.name)

                    if self._check_if_exists(storage, abs_thumbnail_path):
                        if not dry_run:
                            storage.delete(abs_thumbnail_path)
                        if verbosity > 0:
                            print ("Deleting thumbnail:", abs_thumbnail_path)

            if len(sources_to_delete) >= 1000 and not dry_run:
                self._delete_sources_by_id(sources_to_delete)
                sources_to_delete = []

        if not dry_run:
            self._delete_sources_by_id(sources_to_delete)
        self.execution_time = round(time.time() - time_start)

    def print_stats(self):
        

        print(
            "{0:-<48}".format(str(datetime.now().strftime('%Y-%m-%d %H:%M '))))
        print("{0:<40} {1:>7}".format("Sources checked:", self.sources))
        print("{0:<40} {1:>7}".format(
            "Source references deleted from DB:", self.source_refs_deleted))
        print("{0:<40} {1:>7}".format("Thumbnails deleted from disk:",
                                    self.thumbnails_deleted))
        print("(Completed in %s seconds)\n" % self.execution_time)


def queryset_iterator(queryset, chunksize=1000):
    


    primary_key = 0
    last_pk = queryset.order_by('-pk')[0].pk
    queryset = queryset.order_by('pk')
    while primary_key < last_pk:
        for row in queryset.filter(pk__gt=primary_key)[:chunksize]:
            primary_key = row.pk
            yield row
        gc.collect()


class Command(BaseCommand):
    help = 


    option_list = BaseCommand.option_list + (
        make_option(
            '--dry-run',
            action='store_true',
            dest='dry_run',
            default=False,
            help='Dry run the execution.'),
        make_option(
            '--last-n-days',
            action='store',
            dest='last_n_days',
            default=0,
            type='int',
            help='The number of days back in time to clean thumbnails for.'),
        make_option(
            '--path',
            action='store',
            dest='cleanup_path',
            type='string',
            help='Specify a path to clean up.'),
    )

    def handle(self, *args, **options):
        tcc = ThumbnailCollectionCleaner()
        tcc.clean_up(
            dry_run=options.get('dry_run', False),
            verbosity=int(options.get('verbosity', 1)),
            last_n_days=int(options.get('last_n_days', 0)),
            cleanup_path=options.get('cleanup_path'))
        tcc.print_stats()

import ConfigParser
import os
import sys

import install_venv_common as install_venv


def print_help(project, venv, root):
    help = 

    print (help % dict(project=project, venv=venv, root=root))


def main(argv):
    root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

    if os.environ.get('tools_path'):
        root = os.environ['tools_path']
    venv = os.path.join(root, '.venv')
    if os.environ.get('venv'):
        venv = os.environ['venv']

    pip_requires = os.path.join(root, 'requirements.txt')
    test_requires = os.path.join(root, 'test-requirements.txt')
    py_version = "python%s.%s" % (sys.version_info[0], sys.version_info[1])
    setup_cfg = ConfigParser.ConfigParser()
    setup_cfg.read('setup.cfg')
    project = setup_cfg.get('metadata', 'name')

    install = install_venv.InstallVenv(
        root, venv, pip_requires, test_requires, py_version, project)
    options = install.parse_args(argv)
    install.check_python_version()
    install.check_dependencies()
    install.create_virtualenv(no_site_packages=options.no_site_packages)
    install.install_dependencies()
    print_help(project, venv, root)

if __name__ == '__main__':
    main(sys.argv)

import sys, os




extensions = ['sphinx.ext.autodoc']

templates_path = ['_templates']

source_suffix = '.rst'


master_doc = 'index'

project = u'Grit'
copyright = u'2011-2012, Ryan Galloway <ryan@rsgalloway.com>'

version = '0.1'
release = '0.1'



exclude_patterns = ['build']





pygments_style = 'sphinx'




html_theme = 'default'







html_static_path = ['static']













htmlhelp_basename = 'Gritdoc'





latex_documents = [
  ('index', 'Grit.tex', u'Grit Documentation',
   u'Ryan Galloway', 'manual'),
]










man_pages = [
    ('index', 'grit', u'Grit Documentation',
     [u'Ryan Galloway'], 1)
]


import os
import arcpy
import traceback
import sys

inputTDSFeatureDataset = arcpy.GetParameterAsText(0)
inputMAOTWorkspace = arcpy.GetParameterAsText(1)

FEATURECLASSES_TO_MERGE = ["UtilityInfrastructureCrv","StructureCrv",
                           "MilitaryCrv","PhysiographyCrv","RecreationCrv",
                           "VegetationCrv", "HydrographyCrv",
                           "TransportationWaterCrv","TransportationGroundCrv"]
DEBUG = True


def GetQualifierName(inputFD):
    

    qfList = os.path.basename(inputFD).split(".")
    arcpy.AddMessage("qfList: " + str(qfList))
    if len(qfList) != 1:
        qualifierString = str(qfList[0] + "." + qfList[1] + ".")
    else:
        qualifierString = ""
    arcpy.AddMessage("qualifierString: " + str(qualifierString))
    return qualifierString


def FeatureClassesFromWorkspace(inputTDSFeatureDataset):
    

    arcpy.env.workspace = os.path.dirname(inputTDSFeatureDataset)
    tdsFeatureClasses = arcpy.ListFeatureClasses("*", "Line",
                                                 os.path.basename(inputTDSFeatureDataset))
    return tdsFeatureClasses


def main():
    

    try:
        arcpy.AddMessage("Getting database qualifier string ...")
        qualifierString = GetQualifierName(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("qualifier string: " + qualifierString)

        fqClassesToMerge = [str(qualifierString + i) for i in FEATURECLASSES_TO_MERGE]
        if DEBUG is True: arcpy.AddMessage("fqClassesToMerge: " + str(fqClassesToMerge))

        workspace = os.path.dirname(inputTDSFeatureDataset)
        tdsFeatureClasses = FeatureClassesFromWorkspace(inputTDSFeatureDataset)
        if DEBUG is True: arcpy.AddMessage("tdsFeatureClasses: " + str(tdsFeatureClasses))

        arcpy.AddMessage("Building list of input features ...")
        newList = [str(os.path.join(workspace, os.path.basename(inputTDSFeatureDataset), fc))\
                   for fc in tdsFeatureClasses if fc in fqClassesToMerge]
        if DEBUG is True: arcpy.AddMessage("newList: " + str(newList))

        target = os.path.join(inputMAOTWorkspace, "HLZLinearObstacleOutput")
        if DEBUG is True: arcpy.AddMessage("target: " + str(target))

        arcpy.AddMessage("Merging features to output (this may take some time)...")
        arcpy.Merge_management(newList, target)

        arcpy.AddMessage("Setting output ...")
        arcpy.SetParameter(2, target)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)
        print(msgs)

    except:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        arcpy.AddError(msgs)

        print(pymsg + "\n")
        print(msgs)


if __name__ == "__main__":
    main()

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/chassis/shared_blacksun_light_s04_chassis_token.iff"
	result.attribute_template_id = 8
	result.stfName("space_crafting_n","blacksun_light_s04_chassis_token")		
	
	


import arcpy
import os
import sys
import traceback

inputTrackLines = arcpy.GetParameterAsText(0)
inputTrackIDFieldName = arcpy.GetParameterAsText(1)
inputPatrolReportXML = arcpy.GetParameterAsText(2)
inputPatrolReportTable = arcpy.GetParameterAsText(3)
inputEnemySightingsTable = arcpy.GetParameterAsText(4)

DEBUG = False

def getUniqueID(featlayer, idfield):
    uniqueid, rows, row = None, None, None
    try:
        count = int(arcpy.GetCount_management(featlayer).getOutput(0))
        rows = arcpy.da.SearchCursor(featlayer,["OID@",idfield])
        uniqueid = row[1]
        oid = row[0]
        if count > 1:
            arcpy.AddWarning("Found more than one selected row. Using row with Object ID = " + str(oid) + ", and with value: " + str(uniqueid))
        del row
        del rows
        return uniqueid

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]
        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"
        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)


def getToolboxName():
    versionDesktop = ["10.1.0","10.2.0","10.2.1","10.2.2","10.3.0","10.3.1","10.4.0"]
    versionPro = ["1.0","1.1"]
    version = arcpy.GetInstallInfo()["Version"]
    if version in versionDesktop:
        tbxName = "Patrol Data Capture Tools_10.3.tbx"
        
    elif version in versionPro:
        tbxName = "Patrol Data Capture Tools.tbx"
    else:
        raise Exception("Unable to determine ArcGIS version")
    if DEBUG == True:
        arcpy.AddMessage("Version " + str(version) + " using " + tbxName)
    return tbxName

def getToolboxPath():
    tbxPath = os.path.dirname(os.path.dirname(sys.argv[0]))
    if DEBUG == True:
        arcpy.AddMessage("Using toolbox path: " + str(tbxPath))
    return tbxPath

def main():
    try:
        tbx = os.path.join(getToolboxPath(),getToolboxName())
        if not (os.path.exists(tbx)):
            raise Exception("Cannot find toolbox: " + str(tbx))

        arcpy.ImportToolbox(tbx,"pdc")

        arcpy.env.overwriteOutput = True

        uniqueID = getUniqueID(inputTrackLines, inputTrackIDFieldName)
        arcpy.AddMessage("Using unique ID = " + str(uniqueID))

        arcpy.AddMessage("Adding Enemy Sightnings...")
        arcpy.ImportEnemySightingsXML_pdc(inputPatrolReportXML, uniqueID, inputEnemySightingsTable)

        arcpy.AddMessage("Adding Patrol Report...")
        arcpy.ImportPatrolRptXML_pdc(inputPatrolReportXML, uniqueID, inputPatrolReportTable)

        arcpy.AddMessage("Done!")
        arcpy.SetParameter(5,inputEnemySightingsTable)
        arcpy.SetParameter(6,inputPatrolReportTable)

    except arcpy.ExecuteError:
        msgs = arcpy.GetMessages()
        arcpy.AddError(msgs)

        sys.exit(-1)

    except Exception as e:
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        pymsg = "PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1])
        msgs = "ArcPy ERRORS:\n" + arcpy.GetMessages() + "\n"

        arcpy.AddError(pymsg)
        print(pymsg)
        arcpy.AddError(msgs)
        print(msgs)

if __name__ == "__main__":
    main()

import re

from pygments.lexer import Lexer
from pygments.token import Token


HEADING = Token.Generic.Heading
SETTING = Token.Keyword.Namespace
IMPORT = Token.Name.Namespace
TC_KW_NAME = Token.Generic.Subheading
KEYWORD = Token.Name.Function
ARGUMENT = Token.String
VARIABLE = Token.Name.Variable
COMMENT = Token.Comment
SEPARATOR = Token.Punctuation
SYNTAX = Token.Punctuation
GHERKIN = Token.Generic.Emph
ERROR = Token.Error


def normalize(string, remove=''):
    string = string.lower()
    for char in remove + ' ':
        if char in string:
            string = string.replace(char, '')
    return string


class RobotFrameworkLexer(Lexer):
    

    name = 'RobotFramework'
    aliases = ['RobotFramework', 'robotframework']
    filenames = ['*.txt']
    mimetypes = ['text/x-robotframework']

    def __init__(self):
        Lexer.__init__(self, tabsize=2, encoding='UTF-8')

    def get_tokens_unprocessed(self, text):
        row_tokenizer = RowTokenizer()
        var_tokenizer = VariableTokenizer()
        index = 0
        for row in text.splitlines():
            for value, token in row_tokenizer.tokenize(row):
                for value, token in var_tokenizer.tokenize(value, token):
                    if value:
                        yield index, token, unicode(value)
                        index += len(value)


class VariableTokenizer(object):

    def tokenize(self, string, token):
        var = VariableSplitter(string, identifiers='$@%')
        if var.start < 0 or token in (COMMENT, ERROR):
            yield string, token
            return
        for value, token in self._tokenize(var, string, token):
            if value:
                yield value, token

    def _tokenize(self, var, string, orig_token):
        before = string[:var.start]
        yield before, orig_token
        yield var.identifier + '{', SYNTAX
        for value, token in self.tokenize(var.base, VARIABLE):
            yield value, token
        yield '}', SYNTAX
        if var.index:
            yield '[', SYNTAX
            for value, token in self.tokenize(var.index, VARIABLE):
                yield value, token
            yield ']', SYNTAX
        for value, token in self.tokenize(string[var.end:], orig_token):
            yield value, token


class RowTokenizer(object):

    def __init__(self):
        self._table = UnknownTable()
        self._splitter = RowSplitter()
        testcases = TestCaseTable()
        settings = SettingTable(testcases.set_default_template)
        variables = VariableTable()
        keywords = KeywordTable()
        self._tables = {'settings': settings, 'setting': settings,
                        'metadata': settings,
                        'variables': variables, 'variable': variables,
                        'testcases': testcases, 'testcase': testcases,
                        'keywords': keywords, 'keyword': keywords,
                        'userkeywords': keywords, 'userkeyword': keywords}

    def tokenize(self, row):
        commented = False
        heading = False
        for index, value in enumerate(self._splitter.split(row)):
            index, separator = divmod(index-1, 2)
                commented = True
            elif index == 0 and value.startswith('*'):
                self._table = self._start_table(value)
                heading = True
            for value, token in self._tokenize(value, index, commented,
                                               separator, heading):
                yield value, token
        self._table.end_row()

    def _start_table(self, header):
        name = normalize(header, remove='*')
        return self._tables.get(name, UnknownTable())

    def _tokenize(self, value, index, commented, separator, heading):
        if commented:
            yield value, COMMENT
        elif separator:
            yield value, SEPARATOR
        elif heading:
            yield value, HEADING
        else:
            for value, token in self._table.tokenize(value, index):
                yield value, token


class RowSplitter(object):
    _space_splitter = re.compile('( {2,})')
    _pipe_splitter = re.compile('((?:^| +)\|(?: +|$))')

    def split(self, row):
        splitter = self._split_from_spaces \
            if not row.startswith('| ') else self._split_from_pipes
        for value in splitter(row):
            yield value
        yield '\n'

    def _split_from_spaces(self, row):
        for value in self._space_splitter.split(row):
            yield value

    def _split_from_pipes(self, row):
        _, separator, rest = self._pipe_splitter.split(row, 1)
        yield separator
        while self._pipe_splitter.search(rest):
            cell, separator, rest = self._pipe_splitter.split(rest, 1)
            yield cell
            yield separator
        yield rest


class Tokenizer(object):
    _tokens = None

    def __init__(self):
        self._index = 0

    def tokenize(self, value):
        values_and_tokens = self._tokenize(value, self._index)
        self._index += 1
        if isinstance(values_and_tokens, type(Token)):
            values_and_tokens = [(value, values_and_tokens)]
        return values_and_tokens

    def _tokenize(self, value, index):
        index = min(index, len(self._tokens) - 1)
        return self._tokens[index]

    def _is_assign(self, value):
        if value.endswith('='):
            value = value[:-1].strip()
        var = VariableSplitter(value, identifiers='$@')
        return var.start == 0 and var.end == len(value)


class Comment(Tokenizer):
    _tokens = (COMMENT,)


class Setting(Tokenizer):
    _tokens = (SETTING, ARGUMENT)
    _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                         'suitepostcondition', 'testsetup', 'testprecondition',
                         'testteardown', 'testpostcondition', 'testtemplate')
    _import_settings = ('library', 'resource', 'variables')
    _other_settings = ('documentation', 'metadata', 'forcetags', 'defaulttags',
                       'testtimeout')
    _custom_tokenizer = None

    def __init__(self, template_setter=None):
        Tokenizer.__init__(self)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 1 and self._template_setter:
            self._template_setter(value)
        if index == 0:
            normalized = normalize(value)
            if normalized in self._keyword_settings:
                self._custom_tokenizer = KeywordCall(support_assign=False)
            elif normalized in self._import_settings:
                self._custom_tokenizer = ImportSetting()
            elif normalized not in self._other_settings:
                return ERROR
        elif self._custom_tokenizer:
            return self._custom_tokenizer.tokenize(value)
        return Tokenizer._tokenize(self, value, index)


class ImportSetting(Tokenizer):
    _tokens = (IMPORT, ARGUMENT)


class TestCaseSetting(Setting):
    _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                         'template')
    _import_settings = ()
    _other_settings = ('documentation', 'tags', 'timeout')

    def _tokenize(self, value, index):
        if index == 0:
            type = Setting._tokenize(self, value[1:-1], index)
            return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
        return Setting._tokenize(self, value, index)


class KeywordSetting(TestCaseSetting):
    _keyword_settings = ('teardown',)
    _other_settings = ('documentation', 'arguments', 'return', 'timeout')


class Variable(Tokenizer):
    _tokens = (SYNTAX, ARGUMENT)

    def _tokenize(self, value, index):
        if index == 0 and not self._is_assign(value):
            return ERROR
        return Tokenizer._tokenize(self, value, index)


class KeywordCall(Tokenizer):
    _tokens = (KEYWORD, ARGUMENT)

    def __init__(self, support_assign=True):
        Tokenizer.__init__(self)
        self._keyword_found = not support_assign
        self._assigns = 0

    def _tokenize(self, value, index):
        if not self._keyword_found and self._is_assign(value):
            self._assigns += 1
        if self._keyword_found:
            return Tokenizer._tokenize(self, value, index - self._assigns)
        self._keyword_found = True
        return GherkinTokenizer().tokenize(value, KEYWORD)


class GherkinTokenizer(object):
    _gherkin_prefix = re.compile('^(Given|When|Then|And|But) ', re.IGNORECASE)

    def tokenize(self, value, token):
        match = self._gherkin_prefix.match(value)
        if not match:
            return [(value, token)]
        end = match.end()
        return [(value[:end], GHERKIN), (value[end:], token)]


class TemplatedKeywordCall(Tokenizer):
    _tokens = (ARGUMENT,)


class ForLoop(Tokenizer):

    def __init__(self):
        Tokenizer.__init__(self)
        self._in_arguments = False

    def _tokenize(self, value, index):
        token = ARGUMENT if self._in_arguments else SYNTAX
        if value.upper() in ('IN', 'IN RANGE'):
            self._in_arguments = True
        return token


class _Table(object):
    _tokenizer_class = None

    def __init__(self, prev_tokenizer=None):
        self._tokenizer = self._tokenizer_class()
        self._prev_tokenizer = prev_tokenizer
        self._prev_values_on_row = []

    def tokenize(self, value, index):
        if self._continues(value, index):
            self._tokenizer = self._prev_tokenizer
            yield value, SYNTAX
        else:
            for value_and_token in self._tokenize(value, index):
                yield value_and_token
        self._prev_values_on_row.append(value)

    def _continues(self, value, index):
        return value == '...' and all(self._is_empty(t)
                                      for t in self._prev_values_on_row)

    def _is_empty(self, value):
        return value in ('', '\\')

    def _tokenize(self, value, index):
        return self._tokenizer.tokenize(value)

    def end_row(self):
        self.__init__(prev_tokenizer=self._tokenizer)


class UnknownTable(_Table):
    _tokenizer_class = Comment

    def _continues(self, value, index):
        return False


class VariableTable(_Table):
    _tokenizer_class = Variable


class SettingTable(_Table):
    _tokenizer_class = Setting

    def __init__(self, template_setter, prev_tokenizer=None):
        _Table.__init__(self, prev_tokenizer)
        self._template_setter = template_setter

    def _tokenize(self, value, index):
        if index == 0 and normalize(value) == 'testtemplate':
            self._tokenizer = Setting(self._template_setter)
        return _Table._tokenize(self, value, index)

    def end_row(self):
        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)


class TestCaseTable(_Table):
    _setting_class = TestCaseSetting
    _test_template = None
    _default_template = None

    @property
    def _tokenizer_class(self):
        if self._test_template or (self._default_template
                                   and self._test_template is not False):
            return TemplatedKeywordCall
        return KeywordCall

    def _continues(self, value, index):
        return index > 0 and _Table._continues(self, value, index)

    def _tokenize(self, value, index):
        if index == 0:
            if value:
                self._test_template = None
            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
        if index == 1 and self._is_setting(value):
            if self._is_template(value):
                self._test_template = False
                self._tokenizer = self._setting_class(self.set_test_template)
            else:
                self._tokenizer = self._setting_class()
        if index == 1 and self._is_for_loop(value):
            self._tokenizer = ForLoop()
        if index == 1 and self._is_empty(value):
            return [(value, SYNTAX)]
        return _Table._tokenize(self, value, index)

    def _is_setting(self, value):
        return value.startswith('[') and value.endswith(']')

    def _is_template(self, value):
        return normalize(value) == '[template]'

    def _is_for_loop(self, value):
        return value.startswith(':') and normalize(value, remove=':') == 'for'

    def set_test_template(self, template):
        self._test_template = self._is_template_set(template)

    def set_default_template(self, template):
        self._default_template = self._is_template_set(template)

    def _is_template_set(self, template):
        return normalize(template) not in ('', '\\', 'none', '${empty}')


class KeywordTable(TestCaseTable):
    _tokenizer_class = KeywordCall
    _setting_class = KeywordSetting

    def _is_template(self, value):
        return False



class VariableSplitter:

    def __init__(self, string, identifiers):
        self.identifier = None
        self.base = None
        self.index = None
        self.start = -1
        self.end = -1
        self._identifiers = identifiers
        self._may_have_internal_variables = False
        try:
            self._split(string)
        except ValueError:
            pass
        else:
            self._finalize()

    def get_replaced_base(self, variables):
        if self._may_have_internal_variables:
            return variables.replace_string(self.base)
        return self.base

    def _finalize(self):
        self.identifier = self._variable_chars[0]
        self.base = ''.join(self._variable_chars[2:-1])
        self.end = self.start + len(self._variable_chars)
        if self._has_list_variable_index():
            self.index = ''.join(self._list_variable_index_chars[1:-1])
            self.end += len(self._list_variable_index_chars)

    def _has_list_variable_index(self):
        return self._list_variable_index_chars \
            and self._list_variable_index_chars[-1] == ']'

    def _split(self, string):
        start_index, max_index = self._find_variable(string)
        self.start = start_index
        self._open_curly = 1
        self._state = self._variable_state
        self._variable_chars = [string[start_index], '{']
        self._list_variable_index_chars = []
        self._string = string
        start_index += 2
        for index, char in enumerate(string[start_index:]):
            try:
                self._state(char, index)
            except StopIteration:
                return
            if index == max_index and not self._scanning_list_variable_index():
                return

    def _scanning_list_variable_index(self):
        return self._state in [self._waiting_list_variable_index_state,
                               self._list_variable_index_state]

    def _find_variable(self, string):
        max_end_index = string.rfind('}')
        if max_end_index == -1:
            return ValueError('No variable end found')
        if self._is_escaped(string, max_end_index):
            return self._find_variable(string[:max_end_index])
        start_index = self._find_start_index(string, 1, max_end_index)
        if start_index == -1:
            return ValueError('No variable start found')
        return start_index, max_end_index

    def _find_start_index(self, string, start, end):
        index = string.find('{', start, end) - 1
        if index < 0:
            return -1
        if self._start_index_is_ok(string, index):
            return index
        return self._find_start_index(string, index+2, end)

    def _start_index_is_ok(self, string, index):
        return string[index] in self._identifiers \
            and not self._is_escaped(string, index)

    def _is_escaped(self, string, index):
        escaped = False
        while index > 0 and string[index-1] == '\\':
            index -= 1
            escaped = not escaped
        return escaped

    def _variable_state(self, char, index):
        self._variable_chars.append(char)
        if char == '}' and not self._is_escaped(self._string, index):
            self._open_curly -= 1
            if self._open_curly == 0:
                if not self._is_list_variable():
                    raise StopIteration
                self._state = self._waiting_list_variable_index_state
        elif char in self._identifiers:
            self._state = self._internal_variable_start_state

    def _is_list_variable(self):
        return self._variable_chars[0] == '@'

    def _internal_variable_start_state(self, char, index):
        self._state = self._variable_state
        if char == '{':
            self._variable_chars.append(char)
            self._open_curly += 1
            self._may_have_internal_variables = True
        else:
            self._variable_state(char, index)

    def _waiting_list_variable_index_state(self, char, index):
        if char != '[':
            raise StopIteration
        self._list_variable_index_chars.append(char)
        self._state = self._list_variable_index_state

    def _list_variable_index_state(self, char, index):
        self._list_variable_index_chars.append(char)
        if char == ']':
            raise StopIteration
import datetime

import mock
import pytest

from .. import util
from okcupyd import User
from okcupyd.db import model, txn
from okcupyd.db.mailbox import Sync
from okcupyd.util import Fetchable


@pytest.fixture
def mock_user(T):
    mock_user = mock.MagicMock(
        profile=T.build_mock.profile(),
        inbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0)))),
        outbox=Fetchable(mock.Mock(fetch=lambda: (i for i in range(0))))
    )
    return mock_user


@pytest.fixture
def mailbox_sync(mock_user):
    return Sync(mock_user)


def set_mailbox(mailbox, value):
    def fetch():
        for i in value:
            yield i
    mailbox._fetcher.fetch = fetch
    mailbox()

@pytest.mark.skipif(True, reason='Test needs work')
def test_mailbox_sync_creates_message_rows(T, mailbox_sync, mock_user):
    T.factory.okcupyd_user(mock_user)
    initiator_one = 'first_initiator'
    respondent_one = 'respondent_one'
    respondent_two = 'respondent_two'
    initiator_two = 'other'
    inbox_items = [
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_one,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_one, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5)),
        T.build_mock.thread(initiator=initiator_two, respondent=respondent_two,
                            datetime=datetime.datetime(year=2014, day=2, month=5),
                            message_count=1)
    ]
    set_mailbox(mock_user.inbox, inbox_items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox}

    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.id]
            )

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    mock_user.inbox[0].messages.append(T.build_mock.message(
        sender=respondent_one, recipient=initiator_one, content='final'
    ))

    mock_user.inbox[2].messages.append(T.build_mock.message(
        sender=initiator_two, recipient=respondent_two, content='last'
    ))

    mailbox_sync.all()
    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        assert len(message_threads) == len(id_to_mock_thread)
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )

    set_mailbox(mock_user.inbox, [
        T.build_mock.thread(initiator=initiator_one,
                            respondent=initiator_two)
    ] + mock_user.inbox.items)

    id_to_mock_thread = {t.id: t for t in mock_user.inbox.items}
    mailbox_sync.all()

    with txn() as session:
        message_threads = session.query(model.MessageThread).all()
        for message_thread in message_threads:
            T.ensure.thread_model_resembles_okcupyd_thread(
                message_thread,
                id_to_mock_thread[message_thread.okc_id]
            )


@util.use_cassette
def test_mailbox_sync_integration(T):
    user = User()
    T.factory.okcupyd_user(user)
    user.quickmatch().message('test... sorry.')

    Sync(user).all()

    user_model = model.User.find(user.profile.id, id_key='okc_id')
    messages = model.Message.query(
        model.Message.sender_id == user_model.id
    )

    Sync(user).all()

    assert len(messages) == len(model.Message.query(
        model.Message.sender_id == user_model.id
    ))

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/shared_dressed_blood_razor_pirate_strong_tran_m.iff"
	result.attribute_template_id = 9
	result.stfName("npc_name","trandoshan_base_male")		
	
	
	return result
import logging, time, base64
import urllib2
from urllib2 import Request, urlopen, HTTPError, HTTPBasicAuthHandler, HTTPCookieProcessor
from urllib import urlencode
from urlparse import urlunparse, urlparse

try:
    from xml.etree import ElementTree
    from xml.etree.ElementTree import Element
except ImportError:
    from elementtree import ElementTree
    from elementtree.ElementTree import Element

AUTH_HANDLERS = [HTTPBasicAuthHandler]

try:
    from ntlm.HTTPNtlmAuthHandler import HTTPNtlmAuthHandler
except ImportError:
    logging.warn("Windows integrated authentication module (ntlm) not found.")
else:
    class CustomHTTPNtlmAuthHandler(HTTPNtlmAuthHandler):
        

        def http_error_401(self, req, fp, code, msg, hdrs):
            response = HTTPNtlmAuthHandler.http_error_401(self, req, fp, code, msg, hdrs)
            if not (200 <= response.code < 300):
                response = self.parent.error(
                        'http', req, response, response.code, response.msg,
                        response.info)
            return response

    AUTH_HANDLERS.append(CustomHTTPNtlmAuthHandler)




class V1Error(Exception): pass

class V1AssetNotFoundError(V1Error): pass

class V1Server(object):
  "Accesses a V1 HTTP server as a client of the XML API protocol"

  def __init__(self, address="localhost", instance="VersionOne.Web", username='', password='', scheme="http", instance_url=None, logparent=None, loglevel=logging.ERROR):
    if instance_url:
      self.instance_url = instance_url
      parsed = urlparse(instance_url)
      self.address = parsed.netloc
      self.instance = parsed.path.strip('/')
      self.scheme = parsed.scheme
    else:
      self.address = address
      self.instance = instance.strip('/')
      self.scheme = scheme
      self.instance_url = self.build_url('')

    modulelogname='v1pysdk.client'
    logname = "%s.%s" % (logparent, modulelogname) if logparent else None
    self.logger = logging.getLogger(logname)
    self.logger.setLevel(loglevel)
    self.username = username
    self.password = password
    self._install_opener()
        
  def _install_opener(self):
    base_url = self.build_url('')
    password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
    password_manager.add_password(None, base_url, self.username, self.password)
    handlers = [HandlerClass(password_manager) for HandlerClass in AUTH_HANDLERS]
    self.opener = urllib2.build_opener(*handlers)
    self.opener.add_handler(HTTPCookieProcessor())

  def http_get(self, url):
    request = Request(url)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
  
  def http_post(self, url, data=''):
    request = Request(url, data)
    request.add_header("Content-Type", "text/xml;charset=UTF-8")
    response = self.opener.open(request)
    return response
    
  def build_url(self, path, query='', fragment='', params=''):
    "So we dont have to interpolate urls ad-hoc"
    path = self.instance + '/' + path.strip('/')
    if isinstance(query, dict):
      query = urlencode(query)
    url = urlunparse( (self.scheme, self.address, path, params, query, fragment) )
    return url

  def _debug_headers(self, headers):
    self.logger.debug("Headers:")
    for hdr in str(headers).split('\n'):
      self.logger.debug("  %s" % hdr)

  def _debug_body(self, body, headers):
    try:
      ctype = headers['content-type']
    except AttributeError:
      ctype = None
    if ctype is not None and ctype[:5] == 'text/':
      self.logger.debug("Body:")
      for line in str(body).split('\n'):
        self.logger.debug("  %s" % line)
    else:
      self.logger.debug("Body: non-textual content (Content-Type: %s). Not logged." % ctype)

  def fetch(self, path, query='', postdata=None):
    "Perform an HTTP GET or POST depending on whether postdata is present"
    url = self.build_url(path, query=query)
    self.logger.debug("URL: %s" % url)
    try:
      if postdata is not None:
          if isinstance(postdata, dict):
              postdata = urlencode(postdata)
              self.logger.debug("postdata: %s" % postdata)
          response = self.http_post(url, postdata)
      else:
        response = self.http_get(url)
      body = response.read()
      self._debug_headers(response.headers)
      self._debug_body(body, response.headers)
      return (None, body)
    except HTTPError, e:
      if e.code == 401:
          raise
      body = e.fp.read()
      self._debug_headers(e.headers)
      self._debug_body(body, e.headers)
      return (e, body)

  def handle_non_xml_response(self, body, exception, msg, postdata):
      if exception.code >= 500:
        self.logger.error("{0} during {1}".format(exception, msg))
        if postdata is not None:
          self.logger.error(postdata)
        raise exception

  def get_xml(self, path, query='', postdata=None):
    verb = "HTTP POST to " if postdata else "HTTP GET from "
    msg = verb + path
    self.logger.info(msg)
    exception, body = self.fetch(path, query=query, postdata=postdata)
    if exception:
      self.handle_non_xml_response(body, exception, msg, postdata)

      self.logger.warn("{0} during {1}".format(exception, msg))
      if postdata is not None:
         self.logger.warn(postdata)
    document = ElementTree.fromstring(body)
    if exception:
      exception.xmldoc = document
      if exception.code == 404:
        raise V1AssetNotFoundError(exception)
      elif exception.code == 400:
        raise V1Error('\n'+body)
      else:
        raise V1Error(exception)
    return document
   
  def get_asset_xml(self, asset_type_name, oid, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, moment) if moment else '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path)
    
  def get_query_xml(self, asset_type_name, where=None, sel=None):
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    query = {}
    if where is not None:
        query['Where'] = where
    if sel is not None:
        query['sel'] = sel        
    return self.get_xml(path, query=query)
    
  def get_meta_xml(self, asset_type_name):
    path = '/meta.v1/{0}'.format(asset_type_name)
    return self.get_xml(path)
    
  def execute_operation(self, asset_type_name, oid, opname):
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    query = {'op': opname}
    return self.get_xml(path, query=query, postdata={})
    
  def get_attr(self, asset_type_name, oid, attrname, moment=None):
    path = '/rest-1.v1/Data/{0}/{1}/{3}/{2}'.format(asset_type_name, oid, attrname, moment) if moment else '/rest-1.v1/Data/{0}/{1}/{2}'.format(asset_type_name, oid, attrname)
    return self.get_xml(path)
  
  def create_asset(self, asset_type_name, xmldata, context_oid=''):
    body = ElementTree.tostring(xmldata, encoding="utf-8")
    query = {}
    if context_oid:
      query = {'ctx': context_oid}
    path = '/rest-1.v1/Data/{0}'.format(asset_type_name)
    return self.get_xml(path, query=query, postdata=body)
    
  def update_asset(self, asset_type_name, oid, update_doc):
    newdata = ElementTree.tostring(update_doc, encoding='utf-8')
    path = '/rest-1.v1/Data/{0}/{1}'.format(asset_type_name, oid)
    return self.get_xml(path, postdata=newdata)


  def get_attachment_blob(self, attachment_id, blobdata=None):
    path = '/attachment.v1/{0}'.format(attachment_id)
    exception, body = self.fetch(path, postdata=blobdata)
    if exception:
        raise exception
    return body
    
  set_attachment_blob = get_attachment_blob
  
    
    
  
    

from __future__ import absolute_import

import logging
import sys
import types
import threading
import inspect
from functools import wraps
from itertools import chain
from . import config

tls = threading.local()
tls.tracing = False
tls.indent = 0

def find_function_info(func, spec, args):
    


    module = getattr(func, '__module__', None)
    name = getattr(func, '__name__', None)
    self = getattr(func, '__self__', None)
    cname = None
    if self:
        cname = self.__name__
    elif len(spec.args) and spec.args[0] == 'self':
        cname = args[0].__class__.__name__
    elif len(spec.args) and spec.args[0] == 'cls':
        cname = args[0].__name__
    if name:
        qname = []
        if module and module != '__main__':
            qname.append(module)
            qname.append('.')
        if cname:
            qname.append(cname)
            qname.append('.')
        qname.append(name)
        name = ''.join(qname)
    return name, None

def chop(value):
    MAX_SIZE = 320
    s = repr(value)
    if len(s) > MAX_SIZE:
        return s[:MAX_SIZE] + '...' + s[-1]
    else:
        return s

def create_events(fname, spec, args, kwds):

    values = dict()
    if spec.defaults:
        values = dict(zip(spec.args[-len(spec.defaults):],spec.defaults))
    values.update(kwds)
    values.update(list(zip(spec.args[:len(args)], args)))
    positional = ['%s=%r'%(a, values.pop(a)) for a in spec.args]
    anonymous = [str(a) for a in args[len(positional):]]
    keywords = ['%s=%r'%(k, values[k]) for k in sorted(values.keys())]
    params = ', '.join([f for f in chain(positional, anonymous, keywords) if f])

    enter = ['>> ', tls.indent * ' ', fname, '(', params, ')']
    leave = ['<< ', tls.indent * ' ', fname]
    return enter, leave

    
def dotrace(*args, **kwds):
    


    recursive = kwds.get('recursive', False)
    def decorator(func):

        spec = None
        logger = logging.getLogger('trace')
        def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)
            
            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result

        result = None
        rewrap = lambda x: x
        if type(func) == classmethod:
            rewrap = type(func)
            func = func.__get__(True).__func__
        elif type(func) == staticmethod:
            rewrap = type(func)
            func = func.__get__(True)
        elif type(func) == property:
            raise NotImplementedError

        spec = inspect.getargspec(func)
        return rewrap(wraps(func)(wrapper))
    
    arg0 = len(args) and args[0] or None
    if recursive:
        raise NotImplementedError
        if inspect.ismodule(arg0):
            for n, f in inspect.getmembers(arg0, inspect.isfunction):
                setattr(arg0, n, decorator(f))
            for n, c in inspect.getmembers(arg0, inspect.isclass):
                dotrace(c, *args, recursive=recursive)
        elif inspect.isclass(arg0):
            for n, f in inspect.getmembers(arg0, lambda x: (inspect.isfunction(x) or
                                                            inspect.ismethod(x))):
                setattr(arg0, n, decorator(f))
                

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    elif type(arg0) == property:
        pget, pset, pdel = None, None, None
        if arg0.fget:
            pget = decorator(arg0.fget)
        if arg0.fset:
            pset = decorator(arg0.fset)
        if arg0.fdel:
            pdel = decorator(arg0.fdel)
        return property(pget, pset, pdel)
        
    else:
        return decorator

def notrace(*args, **kwds):
    

    def decorator(func):
        return func
    arg0 = len(args) and args[0] or None

    if callable(arg0) or type(arg0) in (classmethod, staticmethod):
        return decorator(arg0)
    else:
        return decorator

def doevent(msg):
    msg = ['== ', tls.indent * ' ', msg]
    logger = logging.getLogger('trace')
    logger.info(''.join(msg))

def noevent(msg):
    pass

if config.TRACE:
    logger = logging.getLogger('trace')
    logger.setLevel(logging.INFO)
    logger.handlers = [logging.StreamHandler()]
    trace = dotrace
    event = doevent
else:
    trace = notrace
    event = noevent
r

import sys
import cPickle as pickle
from hmac import new as hmac
from datetime import datetime
from time import time, mktime, gmtime
from werkzeug import url_quote_plus, url_unquote_plus
from werkzeug._internal import _date_to_unix
from werkzeug.contrib.sessions import ModificationTrackingDict


_default_hash = None
if sys.version_info >= (2, 5):
    try:
        from hashlib import sha1 as _default_hash
    except ImportError:
        pass
if _default_hash is None:
    import sha as _default_hash


class UnquoteError(Exception):
    



class SecureCookie(ModificationTrackingDict):
    


    hash_method = _default_hash

    serialization_method = pickle

    quote_base64 = True

    def __init__(self, data=None, secret_key=None, new=True):
        ModificationTrackingDict.__init__(self, data or ())
        if secret_key is not None:
            secret_key = str(secret_key)
        self.secret_key = secret_key
        self.new = new

    def __repr__(self):
        return '<%s %s%s>' % (
            self.__class__.__name__,
            dict.__repr__(self),
            self.should_save and '*' or ''
        )

    @property
    def should_save(self):
        

        return self.modified

    @classmethod
    def quote(cls, value):
        

        if cls.serialization_method is not None:
            value = cls.serialization_method.dumps(value)
        if cls.quote_base64:
            value = ''.join(value.encode('base64').splitlines()).strip()
        return value

    @classmethod
    def unquote(cls, value):
        

        try:
            if cls.quote_base64:
                value = value.decode('base64')
            if cls.serialization_method is not None:
                value = cls.serialization_method.loads(value)
            return value
        except:
            raise UnquoteError()

    def serialize(self, expires=None):
        

        if self.secret_key is None:
            raise RuntimeError('no secret key defined')
        if expires:
            self['_expires'] = _date_to_unix(expires)
        result = []
        mac = hmac(self.secret_key, None, self.hash_method)
        for key, value in sorted(self.items()):
            result.append('%s=%s' % (
                url_quote_plus(key),
                self.quote(value)
            ))
            mac.update('|' + result[-1])
        return '%s?%s' % (
            mac.digest().encode('base64').strip(),
            '&'.join(result)
        )

    @classmethod
    def unserialize(cls, string, secret_key):
        

        if isinstance(string, unicode):
            string = string.encode('utf-8', 'ignore')
        try:
            base64_hash, data = string.split('?', 1)
        except (ValueError, IndexError):
            items = ()
        else:
            items = {}
            mac = hmac(secret_key, None, cls.hash_method)
            for item in data.split('&'):
                mac.update('|' + item)
                if not '=' in item:
                    items = None
                    break
                key, value = item.split('=', 1)
                key = url_unquote_plus(key)
                try:
                    key = str(key)
                except UnicodeError:
                    pass
                items[key] = value

            try:
                client_hash = base64_hash.decode('base64')
            except Exception:
                items = client_hash = None
            if items is not None and client_hash == mac.digest():
                try:
                    for key, value in items.iteritems():
                        items[key] = cls.unquote(value)
                except UnquoteError:
                    items = ()
                else:
                    if '_expires' in items:
                        if time() > items['_expires']:
                            items = ()
                        else:
                            del items['_expires']
            else:
                items = ()
        return cls(items, secret_key, False)

    @classmethod
    def load_cookie(cls, request, key='session', secret_key=None):
        

        data = request.cookies.get(key)
        if not data:
            return cls(secret_key=secret_key)
        return cls.unserialize(data, secret_key)

    def save_cookie(self, response, key='session', expires=None,
                    session_expires=None, max_age=None, path='/', domain=None,
                    secure=None, httponly=False, force=False):
        

        if force or self.should_save:
            data = self.serialize(session_expires or expires)
            response.set_cookie(key, data, expires=expires, max_age=max_age,
                                path=path, domain=domain, secure=secure,
                                httponly=httponly)
from django import template

from sorl.thumbnail.conf import settings
from sorl.thumbnail import default
from sorl.thumbnail.images import ImageFile
from sorl.thumbnail.parsers import parse_geometry

register = template.Library()


@register.tag
def sorl_thumbnail(*args, **kwargs):
    from sorl.thumbnail.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.tag
def easy_thumbnail(*args, **kwargs):
    from easy_thumbnails.templatetags.thumbnail import thumbnail
    return thumbnail(*args, **kwargs)


@register.filter
def sorl_margin(file_, geometry_string):
    

    if not file_ or settings.THUMBNAIL_DUMMY:
        return 'auto'
    margin = [0, 0, 0, 0]
    image_file = default.kvstore.get_or_set(ImageFile(file_))
    x, y = parse_geometry(geometry_string, image_file.ratio)
    ex = x - image_file.x
    margin[3] = ex / 2
    margin[1] = ex / 2
    if ex % 2:
        margin[1] += 1
    ey = y - image_file.y
    margin[0] = ey / 2
    margin[2] = ey / 2
    if ey % 2:
        margin[2] += 1
    return ' '.join([ '%spx' % n for n in margin ])



import warnings

warnings.warn("paver.command is deprecated. Please re-run the generate_setup task.",
    stacklevel=2)
import paver.tasks

def main():
    paver.tasks.main()
import sys
from remoteserver import DirectResultRemoteServer


class SpecialErrors(object):

    def continuable(self, message, traceback):
        return self._special_error(message, traceback, continuable=True)

    def fatal(self, message, traceback):
        return self._special_error(message, traceback,
                                   fatal='this wins', continuable=42)

    def _special_error(self, message, traceback, continuable=False, fatal=False):
        return {'status': 'FAIL', 'error': message, 'traceback': traceback,
                'continuable': continuable, 'fatal': fatal}


if __name__ == '__main__':
    DirectResultRemoteServer(SpecialErrors(), *sys.argv[1:])

from django.http import Http404

from zookeeper.conf import CLUSTERS


def get_cluster_or_404(id):
  try:
    name = id
    cluster = CLUSTERS.get()[name]
  except (TypeError, ValueError):
    raise Http404()

  cluster = {
    'id': id,
    'nice_name': id,
    'host_ports': cluster.HOST_PORTS.get(),
    'rest_url': cluster.REST_URL.get(),
  }

  return cluster

from oslotest import base as test_base

from ironic_python_agent import encoding


class SerializableTesting(encoding.Serializable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class SerializableComparableTesting(encoding.SerializableComparable):
    serializable_fields = ('jack', 'jill')

    def __init__(self, jack, jill):
        self.jack = jack
        self.jill = jill


class TestSerializable(test_base.BaseTestCase):
    def test_baseclass_serialize(self):
        obj = encoding.Serializable()
        self.assertEqual({}, obj.serialize())

    def test_childclass_serialize(self):
        expected = {'jack': 'hello', 'jill': 'world'}
        obj = SerializableTesting('hello', 'world')
        self.assertEqual(expected, obj.serialize())


class TestSerializableComparable(test_base.BaseTestCase):

    def test_childclass_equal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world')
        self.assertEqual(obj1, obj2)

    def test_childclass_notequal(self):
        obj1 = SerializableComparableTesting('hello', 'world')
        obj2 = SerializableComparableTesting('hello', 'world2')
        self.assertNotEqual(obj1, obj2)

    def test_childclass_hash(self):
        obj = SerializableComparableTesting('hello', 'world')
        self.assertIsNone(obj.__hash__)

__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']

from emitter import *
from serializer import *
from representer import *
from resolver import *

class BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        SafeRepresenter.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

class Dumper(Emitter, Serializer, Representer, Resolver):

    def __init__(self, stream,
            default_style=None, default_flow_style=None,
            canonical=None, indent=None, width=None,
            allow_unicode=None, line_break=None,
            encoding=None, explicit_start=None, explicit_end=None,
            version=None, tags=None):
        Emitter.__init__(self, stream, canonical=canonical,
                indent=indent, width=width,
                allow_unicode=allow_unicode, line_break=line_break)
        Serializer.__init__(self, encoding=encoding,
                explicit_start=explicit_start, explicit_end=explicit_end,
                version=version, tags=tags)
        Representer.__init__(self, default_style=default_style,
                default_flow_style=default_flow_style)
        Resolver.__init__(self)

import datetime

from django.core.exceptions import ObjectDoesNotExist

from app_metrics.models import Metric, MetricItem, MetricDay, MetricWeek, MetricMonth, MetricYear
from app_metrics.utils import week_for_date, month_for_date, year_for_date, get_previous_month, get_previous_year

class InvalidMetric(Exception): pass

def trending_for_metric(metric=None, date=None):
    


    if not isinstance(metric, Metric):
        raise InvalidMetric('No Metric instance passed to trending_for_metric()')
    if not date:
        date = datetime.date.today()

    data = {}

    if date == datetime.date.today():
        data['current_day'] = _trending_for_current_day(metric)

    data['yesterday']   = _trending_for_yesterday(metric)
    data['week']        = _trending_for_week(metric)
    data['month']       = _trending_for_month(metric)
    data['year']        = _trending_for_year(metric)

    return data

def _trending_for_current_day(metric=None):
    date = datetime.date.today()
    unaggregated_values = MetricItem.objects.filter(metric=metric)
    aggregated_values = MetricDay.objects.filter(metric=metric, created=date)
    count = 0

    for u in unaggregated_values:
        count = count + u.num

    for a in aggregated_values:
        count = count + a.num

    return count

def _trending_for_yesterday(metric=None):
    today = datetime.date.today()
    yesterday_date = today - datetime.timedelta(days=1)
    previous_week_date = today - datetime.timedelta(weeks=1)
    previous_month_date = get_previous_month(today)

    data = {
            'yesterday': 0,
            'previous_week': 0,
            'previous_month': 0,
    }

    try:
        yesterday = MetricDay.objects.get(metric=metric, created=yesterday_date)
        data['yesterday'] = yesterday.num
    except ObjectDoesNotExist:
        pass

    try: 
        previous_week = MetricDay.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricDay.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_week(metric=None):
    this_week_date = week_for_date(datetime.date.today())
    previous_week_date = this_week_date - datetime.timedelta(weeks=1)
    previous_month_week_date = get_previous_month(this_week_date)
    previous_year_week_date = get_previous_year(this_week_date)

    data = {
            'week': 0,
            'previous_week': 0,
            'previous_month_week': 0,
            'previous_year_week': 0,
    }

    try:
        week = MetricWeek.objects.get(metric=metric, created=this_week_date)
        data['week'] = week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_week = MetricWeek.objects.get(metric=metric, created=previous_week_date)
        data['previous_week'] = previous_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_week = MetricWeek.objects.get(metric=metric, created=previous_month_week_date)
        data['previous_month_week'] = previous_month_week.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year_week = MetricWeek.objects.get(metric=metric, created=previous_year_week_date)
        data['previous_year_week'] = previous_year_week.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_month(metric=None):
    this_month_date = month_for_date(datetime.date.today())
    previous_month_date = get_previous_month(this_month_date)
    previous_month_year_date = get_previous_year(this_month_date)

    data = {
            'month': 0,
            'previous_month': 0,
            'previous_month_year': 0
    }

    try:
        month = MetricMonth.objects.get(metric=metric, created=this_month_date)
        data['month'] = month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month = MetricMonth.objects.get(metric=metric, created=previous_month_date)
        data['previous_month'] = previous_month.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_month_year = MetricMonth.objects.get(metric=metric, created=previous_month_year_date)
        data['previous_month_year'] = previous_month_year.num
    except ObjectDoesNotExist:
        pass

    return data

def _trending_for_year(metric=None):
    this_year_date = year_for_date(datetime.date.today())
    previous_year_date = get_previous_year(this_year_date)

    data = {
            'year': 0,
            'previous_year': 0,
    }

    try:
        year = MetricYear.objects.get(metric=metric, created=this_year_date)
        data['year'] = year.num
    except ObjectDoesNotExist:
        pass

    try:
        previous_year = MetricYear.objects.get(metric=metric, created=previous_year_date)
        data['previous_year'] = previous_year.num
    except ObjectDoesNotExist:
        pass

    return data
from __future__ import unicode_literals

import os
import random
import string
import sys
from optparse import make_option

from django import db
from django.contrib.auth.models import User
from django.core.files import File
from django.core.management.base import (BaseCommand, CommandError,
                                         NoArgsCommand)
from django.db import transaction
from django.utils import six

from reviewboard.accounts.models import Profile
from reviewboard.reviews.forms import UploadDiffForm
from reviewboard.diffviewer.models import DiffSetHistory
from reviewboard.reviews.models import ReviewRequest, Review, Comment
from reviewboard.scmtools.models import Repository, Tool

NORMAL = 1
DESCRIPTION_SIZE = 100
SUMMARY_SIZE = 6
LOREM_VOCAB = [
    'Lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur',
    'Nullam', 'quis', 'erat', 'libero.', 'Ut', 'vel', 'velit', 'augue, ',
    'risus.', 'Curabitur', 'dignissim', 'luctus', 'dui, ', 'et',
    'tristique', 'id.', 'Etiam', 'blandit', 'adipiscing', 'molestie.',
    'libero', 'eget', 'lacus', 'adipiscing', 'aliquet', 'ut', 'eget',
    'urna', 'dui', 'auctor', 'id', 'varius', 'eget', 'consectetur',
    'Sed', 'ornare', 'fermentum', 'erat', 'ut', 'consectetur', 'diam',
    'in.', 'Aliquam', 'eleifend', 'egestas', 'erat', 'nec', 'semper.',
    'a', 'mi', 'hendrerit', 'vestibulum', 'ut', 'vehicula', 'turpis.',
    'habitant', 'morbi', 'tristique', 'senectus', 'et', 'netus', 'et',
    'fames', 'ac', 'turpis', 'egestas.', 'Vestibulum', 'purus', 'odio',
    'quis', 'consequat', 'non, ', 'vehicula', 'nec', 'ligula.', 'In',
    'ipsum', 'in', 'volutpat', 'ipsum.', 'Morbi', 'aliquam', 'velit',
    'molestie', 'suscipit.', 'Morbi', 'dapibus', 'nibh', 'vel',
    'justo', 'nibh', 'facilisis', 'tortor, ', 'sit', 'amet', 'dictum',
    'amet', 'arcu.', 'Quisque', 'ultricies', 'justo', 'non', 'neque',
    'nibh', 'tincidunt.', 'Curabitur', 'sit', 'amet', 'sem', 'quis',
    'vulputate.', 'Mauris', 'a', 'lorem', 'mi.', 'Donec', 'dolor',
    'interdum', 'eu', 'scelerisque', 'vel', 'massa.', 'Vestibulum',
    'risus', 'vel', 'ipsum', 'suscipit', 'laoreet.', 'Proin', 'congue',
    'blandit.', 'Aenean', 'aliquet', 'auctor', 'nibh', 'sit', 'amet',
    'Vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci',
    'posuere', 'cubilia', 'Curae;', 'Donec', 'lacinia', 'tincidunt',
    'facilisis', 'nisl', 'eu', 'fermentum.', 'Ut', 'nec', 'laoreet',
    'magna', 'egestas', 'nulla', 'pharetra', 'vel', 'egestas', 'tellus',
    'Pellentesque', 'sed', 'pharetra', 'orci.', 'Morbi', 'eleifend, ',
    'interdum', 'placerat,', 'mi', 'dolor', 'mollis', 'libero',
    'quam', 'posuere', 'nisl.', 'Vivamus', 'facilisis', 'aliquam',
    'condimentum', 'pulvinar', 'egestas.', 'Lorem', 'ipsum', 'dolor',
    'consectetur', 'adipiscing', 'elit.', 'In', 'hac', 'habitasse',
    'Aenean', 'blandit', 'lectus', 'et', 'dui', 'tincidunt', 'cursus',
    'Suspendisse', 'ipsum', 'dui, ', 'accumsan', 'eget', 'imperdiet',
    'est.', 'Integer', 'porta, ', 'ante', 'ac', 'commodo', 'faucibus',
    'molestie', 'risus, ', 'a', 'imperdiet', 'eros', 'neque', 'ac',
    'nisi', 'leo', 'pretium', 'congue', 'eget', 'quis', 'arcu.', 'Cras'
]

NAMES = [
    'Aaron', 'Abbey', 'Adan', 'Adelle', 'Agustin', 'Alan', 'Aleshia',
    'Alexia', 'Anderson', 'Ashely', 'Barbara', 'Belen', 'Bernardo',
    'Bernie', 'Bethanie', 'Bev', 'Boyd', 'Brad', 'Bret', 'Caleb',
    'Cammy', 'Candace', 'Carrol', 'Charlette', 'Charlie', 'Chelsea',
    'Chester', 'Claude', 'Daisy', 'David', 'Delila', 'Devorah',
    'Edwin', 'Elbert', 'Elisha', 'Elvis', 'Emmaline', 'Erin',
    'Eugene', 'Fausto', 'Felix', 'Foster', 'Garrett', 'Garry',
    'Garth', 'Gracie', 'Henry', 'Hertha', 'Holly', 'Homer',
    'Ileana', 'Isabella', 'Jacalyn', 'Jaime', 'Jeff', 'Jefferey',
    'Jefferson', 'Joie', 'Kanesha', 'Kassandra', 'Kirsten', 'Kymberly',
    'Lashanda', 'Lean', 'Lonnie', 'Luis', 'Malena', 'Marci', 'Margarett',
    'Marvel', 'Marvin', 'Mel', 'Melissia', 'Morton', 'Nickole', 'Nicky',
    'Odette', 'Paige', 'Patricia', 'Porsche', 'Rashida', 'Raul',
    'Renaldo', 'Rickie', 'Robbin', 'Russel', 'Sabine', 'Sabrina',
    'Sacha', 'Sam', 'Sasha', 'Shandi', 'Sherly', 'Stacey', 'Stephania',
    'Stuart', 'Talitha', 'Tanesha', 'Tena', 'Tobi', 'Tula', 'Valene',
    'Veda', 'Vikki', 'Wanda', 'Wendie', 'Wendolyn', 'Wilda', 'Wiley',
    'Willow', 'Yajaira', 'Yasmin', 'Yoshie', 'Zachariah', 'Zenia',
    'Allbert', 'Amisano', 'Ammerman', 'Androsky', 'Arrowsmith',
    'Bankowski', 'Bleakley', 'Boehringer', 'Brandstetter',
    'Capehart', 'Charlesworth', 'Danforth', 'Debernardi',
    'Delasancha', 'Denkins', 'Edmunson', 'Ernsberger', 'Faupel',
    'Florence', 'Frisino', 'Gardner', 'Ghormley', 'Harrold',
    'Hilty', 'Hopperstad', 'Hydrick', 'Jennelle', 'Massari',
    'Solinski', 'Swisher', 'Talladino', 'Tatham', 'Thornhill',
    'Ulabarro', 'Welander', 'Xander', 'Xavier', 'Xayas', 'Yagecic',
    'Yagerita', 'Yamat', 'Ying', 'Yurek', 'Zaborski', 'Zeccardi',
    'Zecchini', 'Zimerman', 'Zitzow', 'Zoroiwchak', 'Zullinger', 'Zyskowski'
]


class Command(NoArgsCommand):
    help = 'Populates the database with the specified fields'

    option_list = BaseCommand.option_list + (
        make_option('-u', '--users', type="int", default=None, dest='users',
                    help='The number of users to add'),
        make_option('--review-requests', default=None, dest='review_requests',
                    help='The number of review requests per user [min:max]'),
        make_option('--diffs', default=None, dest='diffs',
                    help='The number of diff per review request [min:max]'),
        make_option('--reviews', default=None, dest='reviews',
                    help='The number of reviews per diff [min:max]'),
        make_option('--diff-comments', default=None, dest='diff_comments',
                    help='The number of comments per diff [min:max]'),
        make_option('-p', '--password', type="string", default=None,
                    dest='password',
                    help='The login password for users created')
    )

    @transaction.atomic
    def handle_noargs(self, users=None, review_requests=None, diffs=None,
                      reviews=None, diff_comments=None, password=None,
                      verbosity=NORMAL, **options):
        num_of_requests = None
        num_of_diffs = None
        num_of_reviews = None
        num_of_diff_comments = None
        random.seed()

        if review_requests:
            num_of_requests = self.parse_command("review_requests",
                                                 review_requests)

            repo_dir = os.path.abspath(
                os.path.join(sys.argv[0], "..", "scmtools", "testdata",
                             "git_repo"))

            if not os.path.exists(repo_dir):
                raise CommandError("No path to the repository")

            self.repository = Repository.objects.create(
                name="Test Repository", path=repo_dir,
                tool=Tool.objects.get(name="Git"))

        if diffs:
            num_of_diffs = self.parse_command("diffs", diffs)

            diff_dir_tmp = os.path.abspath(
                os.path.join(sys.argv[0], "..", "reviews", "management",
                             "commands", "diffs"))

            if not os.path.exists(diff_dir_tmp):
                    raise CommandError("Diff dir does not exist")


            files = [f for f in os.listdir(diff_dir)
                     if f.endswith('.diff')]

            if len(files) == 0:
                raise CommandError("No diff files in this directory")

        if reviews:
            num_of_reviews = self.parse_command("reviews", reviews)

        if diff_comments:
            num_of_diff_comments = self.parse_command("diff-comments",
                                                      diff_comments)

        if not users:
            raise CommandError("At least one user must be added")

        for i in range(1, users + 1):
            new_user = User.objects.create(
                first_name=random.choice(NAMES),
                last_name=random.choice(NAMES),
                email="test@example.com",
                is_staff=False,
                is_active=True,
                is_superuser=False)

            if password:
                new_user.set_password(password)
                new_user.save()
            else:
                new_user.set_password("test1")
                new_user.save()

            Profile.objects.create(
                user=new_user,
                first_time_setup_done=True,
                collapsed_diffs=True,
                wordwrapped_diffs=True,
                syntax_highlighting=True,
                show_closed=True)

            req_val = self.pick_random_value(num_of_requests)

            if int(verbosity) > NORMAL:
                self.stdout.write("For user %s:%s" % (i, new_user.username))
                self.stdout.write("=============================")

            for j in range(0, req_val):
                if int(verbosity) > NORMAL:

                review_request = ReviewRequest.objects.create(new_user, None)
                review_request.public = True
                review_request.summary = self.lorem_ipsum("summary")
                review_request.description = self.lorem_ipsum("description")
                review_request.shipit_count = 0
                review_request.repository = self.repository
                if j == 0:
                    review_request.target_people.add(User.objects.get(pk=1))
                review_request.save()

                diff_val = self.pick_random_value(num_of_diffs)

                if diff_val > 0:
                    diffset_history = DiffSetHistory.objects.create(
                        name='testDiffFile' + six.text_type(i))
                    diffset_history.save()

                for k in range(0, diff_val):
                    if int(verbosity) > NORMAL:

                    random_number = random.randint(0, len(files) - 1)
                    file_to_open = diff_dir + files[random_number]
                    f = open(file_to_open, 'r')
                    form = UploadDiffForm(review_request=review_request,
                                          files={"path": File(f)})

                    if form.is_valid():
                        cur_diff = form.create(f, None, diffset_history)

                    review_request.diffset_history = diffset_history
                    review_request.save()
                    review_request.publish(new_user)
                    f.close()

                    review_val = self.pick_random_value(num_of_reviews)

                    for l in range(0, review_val):
                        if int(verbosity) > NORMAL:
                                              (i, j, l))

                        reviews = Review.objects.create(
                            review_request=review_request,
                            user=new_user)

                        reviews.publish(new_user)

                        comment_val = self.pick_random_value(
                            num_of_diff_comments)

                        for m in range(0, comment_val):
                            if int(verbosity) > NORMAL:
                                                  (i, j, m))

                            if m == 0:
                                file_diff = cur_diff.files.order_by('id')[0]

                            max_lines = 220
                            first_line = random.randrange(1, max_lines - 1)
                            remain_lines = max_lines - first_line
                            num_lines = random.randrange(1, remain_lines)

                            diff_comment = Comment.objects.create(
                                filediff=file_diff,
                                text="comment number %s" % (m + 1),
                                first_line=first_line,
                                num_lines=num_lines)

                            review_request.publish(new_user)

                            reviews.comments.add(diff_comment)
                            reviews.save()
                            reviews.publish(new_user)

                            db.reset_queries()

                        if comment_val == 0:
                            db.reset_queries()

                    if review_val == 0:
                        db.reset_queries()

                if diff_val == 0:
                    db.reset_queries()

            if req_val == 0:
                db.reset_queries()

            if req_val != 0:
                self.stdout.write("user %s created with %s requests"
                                  % (new_user.username, req_val))
            else:
                self.stdout.write("user %s created successfully"
                                  % new_user.username)

    def parse_command(self, com_arg, com_string):
        

        try:
            return tuple((int(item.strip()) for item in com_string.split(':')))
        except ValueError:
            raise CommandError('You failed to provide "%s" with one or two '
                               'values of type int.\nExample: --%s=2:5'
                               % (com_arg, com_arg))

    def rand_username(self):
        

        return ''.join(random.choice(string.ascii_lowercase)
                       for x in range(0, random.randrange(5, 9)))

    def pick_random_value(self, value):
        

        if not value:
            return 0

        if len(value) == 1:
            return value[0]

        return random.randrange(value[0], value[1])

    def lorem_ipsum(self, ipsum_type):
        

        if ipsum_type == "description":
            max_size = DESCRIPTION_SIZE
        else:
            max_size = SUMMARY_SIZE

        return ' '.join(random.choice(LOREM_VOCAB)
                        for x in range(0, max_size))


import logging
import copy
import mc_states.api
from salt.utils.pycrypto import secure_password

__name = 'nginx'

log = logging.getLogger(__name__)


def is_reverse_proxied():
    is_vm = False
    try:
        with open('/etc/mastersalt/makina-states/cloud.yaml') as fic:
            is_vm = 'is.vm' in fic.read()
    except Exception:
        pass
    return __salt__['mc_cloud.is_vm']() or is_vm


def settings():
    

    @mc_states.api.lazy_subregistry_get(__salt__, __name)
    def _settings():
        grains = __grains__
        pillar = __pillar__
        local_conf = __salt__['mc_macros.get_local_registry'](
            'nginx', registry_format='pack')
        naxsi_ui_pass = local_conf.setdefault('naxsi_ui_pass',
                                              secure_password(32))
        locations = __salt__['mc_locations.settings']()
        nbcpus = __grains__.get('num_cpus', '4')
        epoll = False
        if 'linux' in __grains__.get('kernel', '').lower():
            epoll = True
        ulimit = "65536"
        is_rp = is_reverse_proxied()
        reverse_proxy_addresses = []
        if is_rp:
            gw = grains.get('makina.default_route', {}).get('gateway', '').strip()
            if gw and gw not in reverse_proxy_addresses:
                reverse_proxy_addresses.append(gw)

        logformat = '$remote_addr - $remote_user [$time_local]  '
        logformat += '"$request" $status $bytes_sent "$http_referer" '
        logformat += '"$http_user_agent" "$gzip_ratio"'
        logformats = {
            'custom_combined': logformat
        }

        www_reg = __salt__['mc_www.settings']()
        nginxData = __salt__['mc_utils.defaults'](
            'makina-states.services.http.nginx', {
                'rotate': '365',
                'is_reverse_proxied': is_rp,
                'reverse_proxy_addresses': reverse_proxy_addresses,
                'default_vhost': True,
                'use_real_ip': True,
                'use_naxsi': False,
                'use_naxsi_secrules': True,
                'naxsi_ui_user': 'naxsi_web',
                'proxy_headers_hash_max_size': '1024',
                'proxy_headers_hash_bucket_size': '128',
                'naxsi_ui_pass': naxsi_ui_pass,
                'naxsi_ui_host': '127.0.01',
                'naxsi_ui_intercept_port': '18080',
                'naxsi_ui_extract_port': '18081',
                'use_naxsi_learning': True,
                'naxsi_denied_url': "/RequestDenied",
                'real_ip_header': 'X-Forwarded-For',
                'logformat': 'custom_combined',
                'logformats': logformats,
                'v6': False,
                'allowed_hosts': [],
                'ulimit': ulimit,
                'client_max_body_size': www_reg[
                    'upload_max_filesize'],
                'open_file_cache': 'max=200000 inactive=5m',
                'open_file_cache_valid': '6m',
                'open_file_cache_min_uses': '2',
                'open_file_cache_errors': 'off',
                'epoll': epoll,
                'default_type': 'application/octet-stream',
                'worker_processes': nbcpus,
                'worker_connections': '1024',
                'multi_accept': True,
                'user': 'www-data',
                'server_names_hash_bucket_size': '64',
                'loglevel': 'crit',
                'ldap_cache': True,
                'logdir': '/var/log/nginx',
                'access_log': '{logdir}/access.log',
                'sendfile': True,
                'tcp_nodelay': True,
                'tcp_nopush': True,
                'reset_timedout_connection': 'on',
                'client_body_timeout': 4 * 60 * 60,
                'send_timeout': '60s',
                'keepalive_requests': '100000',
                'keepalive_timeout': '30',
                'types_hash_max_size': '2048',
                'server_tokens': False,
                'server_name_in_redirect': False,
                'error_log':  '{logdir}/error.log',
                'virtualhosts': {},
                'gzip': True,
                'redirect_aliases': True,
                'port': '80',
                'default_domains': ['localhost'],
                'ssl_port': '443',
                'ssl_protocols': 'SSLv3 TLSv1 TLSv1.1 TLSv1.2',
                'ssl_redirect': False,
                'ssl_cacert_first': False,
                'ssl_session_cache': 'shared:SSL:10m',
                'ssl_session_timeout': '10m',
                'ssl_ciphers': 'HIGH:!aNULL:!MD5',
                'default_activation': True,
                'package': 'nginx',
                'docdir': '/usr/share/doc/nginx',
                'doc_root': www_reg['doc_root'],
                'service': 'nginx',
                'basedir': locations['conf_dir'] + '/nginx',
                'confdir': locations['conf_dir'] + '/nginx/conf.d',
                'logdir': locations['var_log_dir'] + '/nginx',
                'wwwdir': locations['srv_dir'] + '/www',
                'vhost_default_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_wrapper_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.conf'),
                'vhost_default_content': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/default.conf'),
                'vhost_top_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.top.conf'),
                'vhost_content_template': (
                    'salt://makina-states/files/'
                    'etc/nginx/sites-available/vhost.content.conf'),
            }
        )
        __salt__['mc_macros.update_local_registry'](
            'nginx', local_conf, registry_format='pack')
        return nginxData
    return _settings()


def vhost_settings(domain, doc_root, **kwargs):
    

    nginxSettings = copy.deepcopy(__salt__['mc_nginx.settings']())
    extra = kwargs.pop('extra', {})
    kwargs.update(extra)
    kwargs.setdefault('small_name',
                      domain.replace('.', '_').replace('-', '_').replace('*', 'star'))
    vhost_basename = kwargs.setdefault('vhost_basename', domain)
    kwargs.setdefault(
        'vhost_available_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".conf")
    kwargs.setdefault(
        'vhost_content_file',
        (nginxSettings['basedir'] + "/sites-available/" +
         vhost_basename + ".content.conf"))
    kwargs.setdefault(
        'vhost_top_file',
        nginxSettings['basedir'] + "/sites-available/" + vhost_basename + ".top.conf")
    kwargs.setdefault('redirect_aliases', True)
    kwargs.setdefault('domain', domain)
    kwargs.setdefault('active', nginxSettings['default_activation'])
    kwargs.setdefault('server_name', kwargs['domain'])
    kwargs.setdefault('default_server', False)
    kwargs.setdefault('ssl_ciphers', nginxSettings['ssl_ciphers'])
    kwargs.setdefault('ssl_port', nginxSettings['ssl_port'])
    kwargs.setdefault('ssl_protocols', nginxSettings['ssl_protocols'])
    kwargs.setdefault('ssl_redirect', nginxSettings['ssl_redirect'])
    kwargs.setdefault('ssl_cacert_first', nginxSettings['ssl_cacert_first'])
    kwargs.setdefault('ssl_session_cache', nginxSettings['ssl_session_cache'])
    kwargs.setdefault('ssl_session_timeout',
                      nginxSettings['ssl_session_timeout'])
    kwargs.setdefault('server_aliases', None)
    kwargs.setdefault('doc_root', doc_root)
    kwargs.setdefault('vh_top_source', nginxSettings['vhost_top_template'])
    kwargs.setdefault('vh_template_source',
                      nginxSettings['vhost_wrapper_template'])
    kwargs.setdefault('vh_content_source',
                      nginxSettings['vhost_content_template'])
    nginxSettings = __salt__['mc_utils.dictupdate'](nginxSettings, kwargs)
    nginxSettings['data'] = copy.deepcopy(nginxSettings)
    nginxSettings['data']['extra'] = copy.deepcopy(nginxSettings)
    nginxSettings['extra'] = copy.deepcopy(nginxSettings)

    if nginxSettings.get('ssl_cert', None) != '':
        ssldomain = domain
        if ssldomain in ['default']:
            ssldomain = __grains__['fqdn']
        lcert, lkey, lchain = __salt__[
            'mc_ssl.get_configured_cert'](ssldomain, gen=True)
        if not nginxSettings.get('ssl_cert'):
            nginxSettings['ssl_cert'] = lcert + lchain
        if not nginxSettings.get('ssl_key'):
            nginxSettings['ssl_key'] = lcert + lchain + lkey
        if not nginxSettings.get('ssl_bundle'):
            nginxSettings['ssl_bundle'] = ''
        certs = ['ssl_cert']
        if nginxSettings.get('ssl_cacert', ''):
            if nginxSettings['ssl_cacert_first']:
                certs.insert(0, 'ssl_cacert')
            else:
                certs.append('ssl_cacert')
        for cert in certs:
            nginxSettings['ssl_bundle'] += nginxSettings[cert]
            if not nginxSettings['ssl_bundle'].endswith('\n'):
                nginxSettings['ssl_bundle'] += '\n'
        for k in ['ssl_bundle', 'ssl_key', 'ssl_cert', 'ssl_cacert']:
            kpath = k + '_path'
            if not nginxSettings.get(kpath):
                nginxSettings.setdefault(
                    kpath,
                    "/etc/ssl/nginx/{0}_{1}.pem".format(ssldomain, k))
    return nginxSettings

from swgpy.object import *	

def create(kernel):
	result = Creature()

	result.template = "object/mobile/skeleton/shared_droid_2.iff"
	result.attribute_template_id = 9
	result.stfName("obj_n","unknown_creature")		
	
	

import itertools
import unittest

import numpy as np
import numpy.testing as npt

from skbio import Sequence, DNA
from skbio.sequence.distance import hamming, kmer_distance


class TestHamming(unittest.TestCase):
    def test_non_sequence(self):
        seq1 = Sequence('abc')
        seq2 = 'abc'

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq1, seq2)

        with self.assertRaisesRegex(TypeError, 'seq1.*seq2.*Sequence.*str'):
            hamming(seq2, seq1)

    def test_type_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ACG')

        with self.assertRaisesRegex(TypeError,
                                    'Sequence.*does not match.*DNA'):
            hamming(seq1, seq2)

    def test_length_mismatch(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABCD')

        with self.assertRaisesRegex(ValueError, 'equal length.*3 != 4'):
            hamming(seq1, seq2)

    def test_return_type(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertIsInstance(distance, float)
        self.assertEqual(distance, 0.0)

    def test_minimum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('ABC')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 0.0)

    def test_mid_range_distance(self):
        seq1 = Sequence("abcdefgh")
        seq2 = Sequence("1b23ef45")

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 5.0/8.0)

    def test_maximum_distance(self):
        seq1 = Sequence('ABC')
        seq2 = Sequence('CAB')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 1.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')

        distance = hamming(seq1, seq2)

        npt.assert_equal(distance, np.nan)

    def test_single_character_sequences(self):
        seq1 = Sequence('a')
        seq2 = Sequence('b')

        self.assertEqual(hamming(seq1, seq1), 0.0)
        self.assertEqual(hamming(seq1, seq2), 1.0)

    def test_sequence_subclass(self):
        seq1 = DNA('ACG-T')
        seq2 = DNA('ACCTT')

        distance = hamming(seq1, seq2)

        self.assertEqual(distance, 2.0/5.0)

    def test_sequences_with_metadata(self):
        seqs1 = [
            Sequence("ACGT"),
            Sequence("ACGT", metadata={'id': 'abc'}),
            Sequence("ACGT", positional_metadata={'qual': range(4)})
        ]
        seqs2 = [
            Sequence("AAAA"),
            Sequence("AAAA", metadata={'id': 'def'}),
            Sequence("AAAA", positional_metadata={'qual': range(4, 8)})
        ]

        for seqs in seqs1, seqs2:
            for seq1, seq2 in itertools.product(seqs, repeat=2):
                distance = hamming(seq1, seq2)
                self.assertEqual(distance, 0.0)

        for seq1, seq2 in itertools.product(seqs1, seqs2):
            distance = hamming(seq1, seq2)
            self.assertEqual(distance, 0.75)


class TestKmerDistance(unittest.TestCase):
    def test_default_kwargs(self):
        seq1 = Sequence('AACCTAGCAATGGAT')
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_nondefault_k(self):
        seq1 = Sequence('GCTTATGGAGAGAGA')
        seq2 = Sequence('CTCGAACTCCAGCCA')
        obs = kmer_distance(seq1, seq2, 2)
        exp = 0.7333333333333333
        self.assertAlmostEqual(obs, exp)
        seq1 = Sequence('EADDECAEECDEACD')
        seq2 = Sequence('DCBCBADADABCCDA')
        obs = kmer_distance(seq1, seq2, 1)
        exp = 0.4
        self.assertAlmostEqual(obs, exp)

    def test_overlap_false(self):
        seq1 = Sequence('CGTTATGTCTGTGAT')
        seq2 = Sequence('CTGAATCGGTAGTGT')
        obs = kmer_distance(seq1, seq2, 3, overlap=False)
        exp = 0.8888888888888888
        self.assertAlmostEqual(obs, exp)

    def test_entirely_different_sequences(self):
        seq1 = Sequence('CCGTGGTCGTATAAG')
        seq2 = Sequence('CGCCTTCCACATCAG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertEqual(obs, exp)

    def test_same_sequence(self):
        seq1 = Sequence('CTGCGACAGTTGGTA')
        seq2 = Sequence('CTGCGACAGTTGGTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.0
        self.assertEqual(obs, exp)

    def test_differing_length_seqs(self):
        seq1 = Sequence('AGAAATCTGAGCAAGGATCA')
        seq2 = Sequence('TTAGTGCGTAATCCG')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9285714285714286
        self.assertAlmostEqual(obs, exp)

    def test_with_sequence_subclass(self):
        seq1 = DNA('GATGGTACTGTAGGT')
        seq2 = DNA('AGGGTGAAGGTATCA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.8421052631578947
        self.assertAlmostEqual(obs, exp)

    def test_with_metadata_sanity(self):
        seq1 = Sequence('AACCTAGCAATGGAT',
                        metadata={'Name': 'Kestrel Gorlick'},
                        positional_metadata={'seq': list('ACTCAAGCTACGAAG')})
        seq2 = Sequence('CAGGCAGTTCTCACC')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 0.9130434782608695
        self.assertAlmostEqual(obs, exp)

    def test_return_type(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ATCG')
        obs = kmer_distance(seq1, seq2, 3)
        self.assertIsInstance(obs, float)
        self.assertEqual(obs, 0.0)

    def test_empty_sequences(self):
        seq1 = Sequence('')
        seq2 = Sequence('')
        obs = kmer_distance(seq1, seq2, 3)
        npt.assert_equal(obs, np.nan)

    def test_one_empty_sequence(self):
        seq1 = Sequence('')
        seq2 = Sequence('CGGGCAGCTCCTACCTGCTA')
        obs = kmer_distance(seq1, seq2, 3)
        exp = 1.0
        self.assertAlmostEqual(obs, exp)

    def test_no_kmers_found(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACGT')
        obs = kmer_distance(seq1, seq2, 5)
        npt.assert_equal(obs, np.nan)

    def test_k_less_than_one_error(self):
        seq1 = Sequence('ATCG')
        seq2 = Sequence('ACTG')
        with self.assertRaisesRegex(ValueError, 'k must be greater than 0.'):
            kmer_distance(seq1, seq2, 0)

    def test_type_mismatch_error(self):
        seq1 = Sequence('ABC')
        seq2 = DNA('ATC')
        with self.assertRaisesRegex(TypeError, "Type 'Sequence'.*type 'DNA'"):
            kmer_distance(seq1, seq2, 3)

    def test_non_sequence_error(self):
        seq1 = Sequence('ATCG')
        seq2 = 'ATCG'
        with self.assertRaisesRegex(TypeError, "not 'str'"):
            kmer_distance(seq1, seq2, 3)


if __name__ == "__main__":
    unittest.main()
from test.unit_tests.providers import common
from test.unit_tests.providers.common import ProviderTestCase
from totalimpact.providers.provider import Provider, ProviderContentMalformedError
from test.utils import http

import os
import collections
import json
from nose.tools import assert_equals, assert_items_equal, raises, nottest

class TestBlog_post(ProviderTestCase):

    provider_name = "blog_post"

    testitem_aliases = ('blog_post', json.dumps({"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com"}))
    testitem_aliases_not_wordpress_com = ('blog_post', json.dumps({"post_url": "http://jasonpriem.org/2012/05/toward-a-second-revolution-altmetrics-total-impact-and-the-decoupled-journal-video/", "blog_url": "http://jasonpriem.org/blog"}))
    api_key = os.environ["WORDPRESS_OUR_BLOG_API_KEY"]

    def setUp(self):
        ProviderTestCase.setUp(self) 

    def test_is_relevant_alias(self):
        assert_equals(self.provider.is_relevant_alias(self.testitem_aliases), True)

        assert_equals(self.provider.is_relevant_alias(("url", "https://twitter.com/researchremix/status/400821465828061184")), False)


    @http
    def test_aliases_wordpress_com(self):
        response = self.provider.aliases([self.testitem_aliases])
        print response
        expected = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "http://researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        assert_equals(response, expected)

    @http
    def test_aliases_not_wordpress(self):
        response = self.provider.aliases([('blog_post', json.dumps({"post_url": "http://jasonpriem.com/cv", "blog_url": "jasonpriem.com"}))])
        print response
        expected = [('url', u'http://jasonpriem.com/cv')]
        assert_equals(response, expected)


    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_wordpress(self):
        response = self.provider.biblio([self.testitem_aliases])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Elsevier agrees UBC researchers can text-mine for citizen science, research tools'}
        assert_equals(response, expected)

    @http
    def test_biblio_not_wordpress2(self):
        test_alias = ('blog_post', "{\"post_url\": \"http://researchremix.wordpress.com/2011/08/10/personal\", \"blog_url\": \"http://researchremix.wordpress.com\"}")
        response = self.provider.biblio([test_alias])
        print response
        expected = {'url': u'http://researchremix.wordpress.com/2011/08/10/personal', 'account': u'researchremix.wordpress.com', 'hosting_platform': 'wordpress.com', 'blog_title': 'Research Remix', 'title': 'Cancer data: it just got personal'}
        assert_equals(response, expected)

    @http
    @nottest
    def test_metrics(self):
        wordpress_aliases = [('url', u'http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/'), ('wordpress_blog_post', '{"post_url": "http://researchremix.wordpress.com/2012/04/17/elsevier-agrees/", "blog_url": "researchremix.wordpress.com", "wordpress_post_id": 1119}')]
        response = self.provider.metrics(wordpress_aliases)
        print response
        expected = {}
        assert_equals(response, expected)


    def test_provider_aliases_400(self):
        pass
    def test_provider_aliases_500(self):
        pass

    def test_provider_biblio_400(self):
        pass
    def test_provider_biblio_500(self):
        pass

    def test_provider_metrics_400(self):
        pass
    def test_provider_metrics_500(self):
        pass
    def test_provider_metrics_empty(self):
        pass
    def test_provider_metrics_nonsense_txt(self):
        pass
    def test_provider_metrics_nonsense_xml(self):
        passfrom betamax import Betamax
from tests.integration.helper import IntegrationHelper


class TestUnicode(IntegrationHelper):
    def test_unicode_is_saved_properly(self):
        s = self.session
        url = 'http://www.amazon.com/review/RAYTXRF3122TO'

        with Betamax(s).use_cassette('test_unicode') as beta:
            self.cassette_path = beta.current_cassette.cassette_path
            s.get(url)
__author__ = 'cooper'
from awacs.aws import Statement, Principal, Allow, Policy
from awacs import sts


def make_simple_assume_statement(principal):
    return Statement(
        Principal=Principal('Service', [principal]),
        Effect=Allow,
        Action=[sts.AssumeRole])


def get_default_assumerole_policy(region=''):
    


    service = 'ec2.amazonaws.com'
    if region == 'cn-north-1':
        service = 'ec2.amazonaws.com.cn'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_ecs_assumerole_policy(region=''):
    


    service = 'ecs.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy


def get_lambda_assumerole_policy(region=''):
    


    service = 'lambda.amazonaws.com'
    policy = Policy(
        Statement=[make_simple_assume_statement(service)]
    )
    return policy
from django.core.exceptions import ImproperlyConfigured


class SelectRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the select_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.select_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s select_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(SelectRelatedMixin, self).get_queryset()

        if not self.select_related:
            return queryset
        return queryset.select_related(*self.select_related)


class PrefetchRelatedMixin(object):
    


    def get_queryset(self):
            raise ImproperlyConfigured(
                '{0} is missing the prefetch_related property. This must be '
                'a tuple or list.'.format(self.__class__.__name__))

        if not isinstance(self.prefetch_related, (tuple, list)):
            raise ImproperlyConfigured(
                "{0}'s prefetch_related property must be a tuple or "
                "list.".format(self.__class__.__name__))

        queryset = super(PrefetchRelatedMixin, self).get_queryset()

        if not self.prefetch_related:
            return queryset
        return queryset.prefetch_related(*self.prefetch_related)


class OrderableListMixin(object):
    


    orderable_columns = None
    orderable_columns_default = None
    order_by = None
    ordering = None

    def get_context_data(self, **kwargs):
        

        context = super(OrderableListMixin, self).get_context_data(**kwargs)
        context["order_by"] = self.order_by
        context["ordering"] = self.ordering
        return context

    def get_orderable_columns(self):
        if not self.orderable_columns:
            raise ImproperlyConfigured(
                '{0} needs the ordering columns defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns

    def get_orderable_columns_default(self):
        if not self.orderable_columns_default:
            raise ImproperlyConfigured(
                '{0} needs the default ordering column defined.'.format(
                    self.__class__.__name__))
        return self.orderable_columns_default

    def get_ordered_queryset(self, queryset=None):
        

        get_order_by = self.request.GET.get("order_by")

        if get_order_by in self.get_orderable_columns():
            order_by = get_order_by
        else:
            order_by = self.get_orderable_columns_default()

        self.order_by = order_by
        self.ordering = "asc"

        if order_by and self.request.GET.get("ordering", "asc") == "desc":
            order_by = "-" + order_by
            self.ordering = "desc"

        return queryset.order_by(order_by)

    def get_queryset(self):
        

        unordered_queryset = super(OrderableListMixin, self).get_queryset()
        return self.get_ordered_queryset(unordered_queryset)




import networkx as nx

try:
    import pygraphviz
    from networkx.drawing.nx_agraph import write_dot
    print("using package pygraphviz")
except ImportError:
    try:
        import pydotplus
        from networkx.drawing.nx_pydot import write_dot
        print("using package pydotplus")
    except ImportError:
        print()
        print("Both pygraphviz and pydotplus were not found ")
        print("see http://networkx.github.io/documentation"
              "/latest/reference/drawing.html for info")
        print()
        raise

write_dot(G,"grid.dot")
print("Now run: neato -Tps grid.dot >grid.ps")
from unittest import TestCase
from stashy.client import StashClient


class TestStashClient(TestCase):
    def test_url_without_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("api/1.0/admin/groups"))

    def test_url_with_slash_prefix(self):
        client = StashClient("http://example.com/stash")
        self.assertEqual("http://example.com/stash/rest/api/1.0/admin/groups", client.url("/api/1.0/admin/groups"))




from pyjamas.ui.VerticalPanel import VerticalPanel
from pyjamas.ui.HTML import HTML
from pyjamas.ui.ClickListener import ClickHandler
from pyjamas.ui.RootPanel import RootPanel
from pyjamas.ui import Event
from pyjamas import DOM
from pyjamas import logging
log = logging.getAppendLogger(__name__, logging.DEBUG, logging.PLAIN_FORMAT)

class Board(VerticalPanel, ClickHandler):
    def __init__(self):
        

        global Text
        VerticalPanel.__init__(self)
        ClickHandler.__init__(self)
        self.addClickListener(self)
        self.title=Text('Board')
        self.title.setzIndex(100)
        self.add(self.title)
        self.setSize("100%", "50%")
        self.setBorderWidth(1)

    def onClick(self, sender):
        log.debug('Text'+str(sender))

    def _event_targets_title(self, event):
        target = DOM.eventGetTarget(event)
        return target and DOM.isOrHasChild(self.title.getElement(), target)

    def onBrowserEvent(self, event):
        etype = DOM.eventGetType(event)
        if etype == "click":
            if self._event_targets_title(event):
                return
        ClickHandler.onBrowserEvent(self, event)


class Text(HTML, ClickHandler):
    def __init__(self, text):
        HTML.__init__(self, text)
        ClickHandler.__init__(self, preventDefault=True)
        self.addClickListener(self)

    def onClick(self, sender):
        log.debug('Text'+str(sender))


if __name__ == "__main__":
    pyjd.setup("./Override.html")
    board = Board()
    RootPanel().add(board)
    pyjd.run()





from googleads import dfp


def main(client):
  network_service = client.GetService('NetworkService', version='v201505')

  networks = network_service.getAllNetworks()

  for network in networks:
    print ('Network with network code \'%s\' and display name \'%s\' was found.'
           % (network['networkCode'], network['displayName']))

  print '\nNumber of results found: %s' % len(networks)

if __name__ == '__main__':
  dfp_client = dfp.DfpClient.LoadFromStorage()
  main(dfp_client)
import py
from py.process import cmdexec

def exvalue():
    return py.std.sys.exc_info()[1]

class Test_exec_cmd:
    def test_simple(self):
        out = cmdexec('echo hallo')
        assert out.strip() == 'hallo'
        assert py.builtin._istext(out)

    def test_simple_newline(self):
        import sys
        out = cmdexec(r
 % sys.executable)
        assert out == 'hello\n'
        assert py.builtin._istext(out)

    def test_simple_error(self):
        py.test.raises (cmdexec.Error, cmdexec, 'exit 1')

    def test_simple_error_exact_status(self):
        try:
            cmdexec('exit 1')
        except cmdexec.Error:
            e = exvalue()
            assert e.status == 1
            assert py.builtin._istext(e.out)
            assert py.builtin._istext(e.err)

    def test_err(self):
        try:
            cmdexec('echoqweqwe123 hallo')
            raise AssertionError("command succeeded but shouldn't")
        except cmdexec.Error:
            e = exvalue()
            assert hasattr(e, 'err')
            assert hasattr(e, 'out')
            assert e.err or e.out



import collections
import decimal
import sys
from io import StringIO

import yaml

from django.core.serializers.base import DeserializationError
from django.core.serializers.python import (
    Deserializer as PythonDeserializer, Serializer as PythonSerializer,
)
from django.db import models
from django.utils import six

try:
    from yaml import CSafeLoader as SafeLoader
    from yaml import CSafeDumper as SafeDumper
except ImportError:
    from yaml import SafeLoader, SafeDumper


class DjangoSafeDumper(SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

    def represent_ordered_dict(self, data):
        return self.represent_mapping('tag:yaml.org,2002:map', data.items())

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)


class Serializer(PythonSerializer):
    


    internal_use_only = False

    def handle_field(self, obj, field):
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    

    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode('utf-8')
    if isinstance(stream_or_string, six.string_types):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    try:
        for obj in PythonDeserializer(yaml.load(stream, Loader=SafeLoader), **options):
            yield obj
    except GeneratorExit:
        raise
    except Exception as e:
        six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
from __future__ import absolute_import, unicode_literals

import unittest

from mopidy.local import json
from mopidy.models import Ref, Track

from tests import path_to_data_dir


class BrowseCacheTest(unittest.TestCase):
    maxDiff = None

        self.uris = ['local:track:foo/bar/song1',
                     'local:track:foo/bar/song2',
                     'local:track:foo/baz/song3',
                     'local:track:foo/song4',
                     'local:track:song5']
        self.cache = json._BrowseCache(self.uris)

    def test_lookup_root(self):
        expected = [Ref.directory(uri='local:directory:foo', name='foo'),
                    Ref.track(uri='local:track:song5', name='song5')]
        self.assertEqual(expected, self.cache.lookup('local:directory'))

    def test_lookup_foo(self):
        expected = [Ref.directory(uri='local:directory:foo/bar', name='bar'),
                    Ref.directory(uri='local:directory:foo/baz', name='baz'),
                    Ref.track(uri=self.uris[3], name='song4')]
        result = self.cache.lookup('local:directory:foo')
        self.assertEqual(expected, result)

    def test_lookup_foo_bar(self):
        expected = [Ref.track(uri=self.uris[0], name='song1'),
                    Ref.track(uri=self.uris[1], name='song2')]
        self.assertEqual(
            expected, self.cache.lookup('local:directory:foo/bar'))

    def test_lookup_foo_baz(self):
        result = self.cache.lookup('local:directory:foo/unknown')
        self.assertEqual([], result)


class JsonLibraryTest(unittest.TestCase):

    config = {
        'core': {
            'data_dir': path_to_data_dir(''),
        },
        'local': {
            'media_dir': path_to_data_dir(''),
            'library': 'json',
        },
    }

        self.library = json.JsonLibrary(self.config)

    def _create_tracks(self, count):
        for i in range(count):
            self.library.add(Track(uri='local:track:%d' % i))

    def test_search_should_default_limit_results(self):
        self._create_tracks(101)

        result = self.library.search()
        result_exact = self.library.search(exact=True)

        self.assertEqual(len(result.tracks), 100)
        self.assertEqual(len(result_exact.tracks), 100)

    def test_search_should_limit_results(self):
        self._create_tracks(100)

        result = self.library.search(limit=35)
        result_exact = self.library.search(exact=True, limit=35)

        self.assertEqual(len(result.tracks), 35)
        self.assertEqual(len(result_exact.tracks), 35)

    def test_search_should_offset_results(self):
        self._create_tracks(200)

        expected = self.library.search(limit=110).tracks[10:]
        expected_exact = self.library.search(exact=True, limit=110).tracks[10:]

        result = self.library.search(offset=10).tracks
        result_exact = self.library.search(offset=10, exact=True).tracks

        self.assertEqual(expected, result)
        self.assertEqual(expected_exact, result_exact)



from flask import Blueprint
from flask import render_template
from flask import json
from flask import session
from flask import url_for
from flask import redirect
from flask import request
from flask import abort
from flask import Response

from app import db

base_view = Blueprint('base_view', __name__)

@base_view.route('/')
def index():
	return "Backyard API Home"
from .case import UnauthenticatedViewTestCase
from . import setUpModule, tearDownModule


class RootFactoryTestCase(UnauthenticatedViewTestCase):

    def test_get_acl_admin(self):
        from pyshop.security import RootFactory
        root = RootFactory(self.create_request())
        self.assertEqual(set(root.__acl__),
                              (u'Allow', u'admin', u'admin_view'),
                              (u'Allow', u'admin', u'download_releasefile'),
                              (u'Allow', u'admin', u'upload_releasefile'),
                              (u'Allow', u'admin', u'user_view'),

                              (u'Allow', u'installer', u'download_releasefile'),

                              (u'Allow', u'developer', u'download_releasefile'),
                              (u'Allow', u'developer', u'upload_releasefile'),
                              (u'Allow', u'developer', u'user_view'),
                              ]))


class GroupFinderTestCase(UnauthenticatedViewTestCase):

    def test_admin_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'admin', self.create_request())),
                         set([u'admin']))

    def test_installer_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'pip', self.create_request())),
                         set([u'installer']))

    def test_dev_groups(self):
        from pyshop.security import groupfinder
        self.assertEqual(set(groupfinder(u'local_user', self.create_request())),
                         set([u'developer']))

import sys
from datetime import datetime
import logging

from modularodm import Q

from website.archiver import (
    ARCHIVER_UNCAUGHT_ERROR,
    ARCHIVER_FAILURE,
    ARCHIVER_INITIATED
)
from website.settings import ARCHIVE_TIMEOUT_TIMEDELTA
from website.archiver.utils import handle_archive_fail
from website.archiver.model import ArchiveJob

from website.app import init_app

from scripts import utils as script_utils


logger = logging.getLogger(__name__)

def find_failed_registrations():
    expired_if_before = datetime.utcnow() - ARCHIVE_TIMEOUT_TIMEDELTA
    jobs = ArchiveJob.find(
        Q('sent', 'eq', False) &
        Q('datetime_initiated', 'lt', expired_if_before) &
        Q('status', 'eq', ARCHIVER_INITIATED)
    )
    return {node.root for node in [job.dst_node for job in jobs] if node}

def remove_failed_registrations(dry_run=True):
    init_app(set_backends=True, routes=False)
    count = 0
    failed = find_failed_registrations()
    if not dry_run:
        for f in failed:
            logging.info('Cleaning {}'.format(f))
            if not f.registered_from:
                logging.info('Node {0} had registered_from == None'.format(f._id))
                continue
                continue
            f.archive_job.status = ARCHIVER_FAILURE
            f.archive_job.sent = True
            f.archive_job.save()
            handle_archive_fail(
                ARCHIVER_UNCAUGHT_ERROR,
                f.registered_from,
                f,
                f.creator,
                f.archive_job.target_info()
            )
            count += 1
    logging.info('Cleaned {} registrations'.format(count))

def main():
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    remove_failed_registrations(dry_run=dry)

if __name__ == '__main__':
    main()
from __future__ import unicode_literals

from .common import InfoExtractor
from ..utils import ExtractorError


class FreeVideoIE(InfoExtractor):

    _TEST = {
        'url': 'http://www.freevideo.cz/vase-videa/vysukany-zadecek-22033.html',
        'info_dict': {
            'id': 'vysukany-zadecek-22033',
            'ext': 'mp4',
            "title": "vysukany-zadecek-22033",
            "age_limit": 18,
        },
        'skip': 'Blocked outside .cz',
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage, handle = self._download_webpage_handle(url, video_id)
        if '//www.czechav.com/' in handle.geturl():
            raise ExtractorError(
                'Access to freevideo is blocked from your location',
                expected=True)

        video_url = self._search_regex(
            r'\s+url: "(http://[a-z0-9-]+.cdn.freevideo.cz/stream/.*?/video.mp4)"',
            webpage, 'video URL')

        return {
            'id': video_id,
            'url': video_url,
            'title': video_id,
            'age_limit': 18,
        }
import unittest
from test.test_support import run_unittest, TESTFN
import glob
import os
import shutil

class GlobTests(unittest.TestCase):

    def norm(self, *parts):
        return os.path.normpath(os.path.join(self.tempdir, *parts))

    def mktemp(self, *parts):
        filename = self.norm(*parts)
        base, file = os.path.split(filename)
        if not os.path.exists(base):
            os.makedirs(base)
        f = open(filename, 'w')
        f.close()

    def setUp(self):
        self.tempdir = TESTFN+"_dir"
        self.mktemp('a', 'D')
        self.mktemp('aab', 'F')
        self.mktemp('aaa', 'zzzF')
        self.mktemp('ZZZ')
        self.mktemp('a', 'bcd', 'EF')
        self.mktemp('a', 'bcd', 'efg', 'ha')
        if hasattr(os, 'symlink'):
            os.symlink(self.norm('broken'), self.norm('sym1'))
            os.symlink(self.norm('broken'), self.norm('sym2'))

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def glob(self, *parts):
        if len(parts) == 1:
            pattern = parts[0]
        else:
            pattern = os.path.join(*parts)
        p = os.path.join(self.tempdir, pattern)
        res = glob.glob(p)
        self.assertEqual(list(glob.iglob(p)), res)
        return res

    def assertSequencesEqual_noorder(self, l1, l2):
        self.assertEqual(set(l1), set(l2))

    def test_glob_literal(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a'), [self.norm('a')])
        eq(self.glob('a', 'D'), [self.norm('a', 'D')])
        eq(self.glob('aab'), [self.norm('aab')])
        eq(self.glob('zymurgy'), [])

    def test_glob_one_directory(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('a*'), map(self.norm, ['a', 'aab', 'aaa']))
        eq(self.glob('*a'), map(self.norm, ['a', 'aaa']))
        eq(self.glob('aa?'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('aa[ab]'), map(self.norm, ['aaa', 'aab']))
        eq(self.glob('*q'), [])

    def test_glob_nested_directory(self):
        eq = self.assertSequencesEqual_noorder
        if os.path.normcase("abCD") == "abCD":
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF')])
        else:
            eq(self.glob('a', 'bcd', 'E*'), [self.norm('a', 'bcd', 'EF'),
                                             self.norm('a', 'bcd', 'efg')])
        eq(self.glob('a', 'bcd', '*g'), [self.norm('a', 'bcd', 'efg')])

    def test_glob_directory_names(self):
        eq = self.assertSequencesEqual_noorder
        eq(self.glob('*', 'D'), [self.norm('a', 'D')])
        eq(self.glob('*', '*a'), [])
        eq(self.glob('a', '*', '*', '*a'),
           [self.norm('a', 'bcd', 'efg', 'ha')])
        eq(self.glob('?a?', '*F'), map(self.norm, [os.path.join('aaa', 'zzzF'),
                                                   os.path.join('aab', 'F')]))

    def test_glob_directory_with_trailing_slash(self):
        res = glob.glob(self.tempdir + '*' + os.sep)
        self.assertEqual(len(res), 1)
        self.assertTrue(res[0] in [self.tempdir, self.tempdir + os.sep])

    def test_glob_broken_symlinks(self):
        if hasattr(os, 'symlink'):
            eq = self.assertSequencesEqual_noorder
            eq(self.glob('sym*'), [self.norm('sym1'), self.norm('sym2')])
            eq(self.glob('sym1'), [self.norm('sym1')])
            eq(self.glob('sym2'), [self.norm('sym2')])


def test_main():
    run_unittest(GlobTests)


if __name__ == "__main__":
    test_main()
import os
import errno

from . import FSQEnvError
from .internal import coerce_unicode

FSQ_CHARSET = os.environ.get("FSQ_CHARSET", u'utf8')
FSQ_CHARSET = coerce_unicode(FSQ_CHARSET, FSQ_CHARSET)
FSQ_DELIMITER = coerce_unicode(os.environ.get("FSQ_DELIMITER", u'_'),
                               FSQ_CHARSET)
FSQ_ENCODE = coerce_unicode(os.environ.get("FSQ_ENCODE", u'%'), FSQ_CHARSET)
FSQ_TIMEFMT = coerce_unicode(os.environ.get("FSQ_TIMEFMT", u'%Y%m%d%H%M%S'),
                             FSQ_CHARSET)
FSQ_QUEUE = coerce_unicode(os.environ.get("FSQ_QUEUE", u'queue'), FSQ_CHARSET)
FSQ_DONE = coerce_unicode(os.environ.get("FSQ_DONE", u'done'), FSQ_CHARSET)
FSQ_FAIL = coerce_unicode(os.environ.get("FSQ_FAIL", u'fail'), FSQ_CHARSET)
FSQ_TMP = coerce_unicode(os.environ.get("FSQ_TMP", u'tmp'), FSQ_CHARSET)
FSQ_DOWN = coerce_unicode(os.environ.get("FSQ_DOWN", u'down'), FSQ_CHARSET)
FSQ_TRIGGER = coerce_unicode(os.environ.get("FSQ_TRIGGER", u'trigger-s'),
                             FSQ_CHARSET)
FSQ_ROOT = coerce_unicode(os.environ.get("FSQ_ROOT", u'/var/fsq'),
                          FSQ_CHARSET)
FSQ_HOSTS = coerce_unicode(os.environ.get("FSQ_HOSTS", u'hosts'),
                          FSQ_CHARSET)
FSQ_HOSTS_TRIGGER = coerce_unicode(os.environ.get("FSQ_HOSTS_TRIGGER",
                                   u'hosts-trigger-s'), FSQ_CHARSET)

FSQ_ITEM_GROUP = coerce_unicode(os.environ.get("FSQ_ITEM_GROUP", u''),
                                FSQ_CHARSET) or None
FSQ_ITEM_USER = coerce_unicode(os.environ.get("FSQ_ITEM_USER", u''),
                               FSQ_CHARSET) or None
FSQ_QUEUE_GROUP = coerce_unicode(os.environ.get("FSQ_QUEUE_GROUP", u''),
                                 FSQ_CHARSET) or None
FSQ_QUEUE_USER = coerce_unicode(os.environ.get("FSQ_QUEUE_USER", u''),
                                FSQ_CHARSET) or None

FSQ_EXEC_DIR = coerce_unicode(os.environ.get("FSQ_EXEC_DIR", u''),
                              FSQ_CHARSET) or None

try:
    FSQ_ITEM_MODE = int(coerce_unicode(os.environ.get("FSQ_ITEM_MODE",
                        u'00640'), FSQ_CHARSET), 8)
    FSQ_QUEUE_MODE = int(coerce_unicode(os.environ.get("FSQ_QUEUE_MODE",
                         u'02770'), FSQ_CHARSET), 8)
    FSQ_FAIL_TMP = int(os.environ.get("FSQ_FAIL_TMP", 111))
    FSQ_FAIL_PERM = int(os.environ.get("FSQ_FAIL_PERM", 100))
    FSQ_SUCCESS = int(os.environ.get("FSQ_SUCCESS", 0))
    FSQ_USE_TRIGGER = int(os.environ.get("FSQ_USE_TRIGGER", 0))
    FSQ_LOCK = int(os.environ.get("FSQ_LOCK", 1))
    FSQ_MAX_TRIES = int(os.environ.get("FSQ_MAX_TRIES", 1))
    FSQ_TTL = int(os.environ.get("FSQ_TTL", 0))
except ValueError, e:
    raise FSQEnvError(errno.EINVAL, e.message)
from test import CollectorTestCase
from test import get_collector_config

from mock import call, Mock, patch
from unittest import TestCase

from diamond.collector import Collector

from portstat import get_port_stats, PortStatCollector


class PortStatCollectorTestCase(CollectorTestCase):

    TEST_CONFIG = {
        'port': {
            'something1': {
                'number': 5222,
            },
            'something2': {
                'number': 8888,
            }
        }
    }

    def setUp(self):
        config = get_collector_config('PortStatCollector',
                                      self.TEST_CONFIG)

        self.collector = PortStatCollector(config, None)

    def test_import(self):
        self.assertTrue(PortStatCollector)

    @patch('portstat.get_port_stats')
    @patch.object(Collector, 'publish')
    def test_collect(self, publish_mock, get_port_stats_mock):

        get_port_stats_mock.return_value = {'foo': 1}

        self.collector.collect()

        get_port_stats_mock.assert_has_calls([call(5222), call(8888)],
                                             any_order=True)
        self.assertPublished(publish_mock, 'something1.foo', 1)
        self.assertPublished(publish_mock, 'something2.foo', 1)


class GetPortStatsTestCase(TestCase):

    @patch('portstat.psutil.net_connections')
    def test_get_port_stats(self, net_connections_mock):

        ports = [Mock() for _ in range(5)]

        ports[0].laddr = (None, 5222)
        ports[0].status = 'ok'
        ports[1].laddr = ports[2].laddr = ports[3].laddr = (None, 8888)
        ports[1].status = 'ok'
        ports[2].status = 'OK'
        ports[3].status = 'bad'
        ports[4].laddr = (None, 9999)

        net_connections_mock.return_value = ports

        cnts = get_port_stats(5222)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 1})

        cnts = get_port_stats(8888)

        net_connections_mock.assert_called_once()
        self.assertEqual(cnts, {'ok': 2, 'bad': 1})

from __future__ import unicode_literals


class InvalidBBCodeTag(Exception):
    

    pass


class InvalidBBCodePlaholder(Exception):
    

    pass

import numpy as np
import tensorflow as tf
import accum_trainer

class AccumTrainerTest(tf.test.TestCase):
  def testAccum(self):
    with self.test_session():
      var0 = tf.Variable([1.0, 2.0])
      trainer = accum_trainer.AccumTrainer()
      
      cost = tf.square(var0)
      
      trainer.prepare_minimize(cost, [var0])
      
      accmulate_grad = trainer.accumulate_gradients()
      reset = trainer.reset_gradients()
      
      tf.initialize_all_variables().run()

      accmulate_grad.run()
      
      self.assertAllClose([1.0, 2.0], var0.eval())
      
      accum_grads = trainer._accum_grad_list
      accum_grad0 = accum_grads[0]

      self.assertAllClose([2.0, 4.0], accum_grad0.eval())

      accmulate_grad.run()

      self.assertAllClose([4.0, 8.0], accum_grad0.eval())

      reset.run()

      self.assertAllClose([0.0, 0.0], accum_grad0.eval())
      

if __name__ == "__main__":
  tf.test.main()





from __future__ import print_function
from __future__ import absolute_import

import argparse
import operator

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
import six
from six.moves import range

options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))



def dump_group(pgroup, sort=False):
    node_kinds = pgroup._v_file._node_kinds[1:]
    what = pgroup._f_walk_groups()
    if sort:
        what = sorted(what, key=operator.attrgetter('_v_pathname'))
    for group in what:
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))




def _get_parser():
    parser = argparse.ArgumentParser(
        description=
)

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-s', '--sort', action='store_true',
        help='sort output by node name',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help=
,
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help=
,
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    if isinstance(args.rng, six.string_types):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            nodename = "/"

    try:
        h5file = open_file(filename, 'r')
    except Exception as e:
        return 'Cannot open input file: ' + str(e)

    with h5file:
        nodeobject = h5file.get_node(nodename)
        if isinstance(nodeobject, Group):
            dump_group(nodeobject, args.sort)
        elif isinstance(nodeobject, Leaf):
            dump_leaf(nodeobject)
        else:
            print("Unrecognized object:", nodeobject)

    return default


    return 'howdy all'
from setuptools import setup
from pip.req import parse_requirements

import os.path

install_reqs = parse_requirements("REQUIREMENTS.txt")
reqs = [str(ir.req) for ir in install_reqs]

files = [os.path.join(r,item).replace("flask_dashboards/", "")
         for r, d, f in os.walk("flask_dashboards/static") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/templates") for item in f]
files += [os.path.join(r,item).replace("flask_dashboards/", "")
          for r, d, f in os.walk("flask_dashboards/widgets") for item in f]

setup(
    name = "flask-dashboards",
    version = "0.1.0",
    author = "Atomic Labs, LLC",
    author_email = "ben@atomicmgmt.com",
    description = ("Adds easy dashboard endpoints to Flask"),
    license = "MIT",
    url = "http://www.atomicmgmt.com",
    packages=["flask_dashboards"],
    package_data={"flask_dashboards": files},
    install_requires=reqs,
    zip_safe=False
)
import django.template
import pretend


def test_compile_filter(monkeypatch):

    compile_static = pretend.call_recorder(lambda source_path: "compiled")
    monkeypatch.setattr("static_precompiler.utils.compile_static", compile_static)
    template = django.template.Template(
)
    assert template.render(django.template.Context({})) == "compiled"

    monkeypatch.setattr("static_precompiler.settings.PREPEND_STATIC_URL", True)
    assert template.render(django.template.Context({})) == "/static/compiled"

    assert compile_static.calls == [pretend.call("source"), pretend.call("source")]


def test_inlinecompile_tag(monkeypatch):
    compiler = pretend.stub(compile_source=pretend.call_recorder(lambda *args: "compiled"))
    get_compiler_by_name = pretend.call_recorder(lambda *args: compiler)

    monkeypatch.setattr("static_precompiler.utils.get_compiler_by_name", get_compiler_by_name)

    template = django.template.Template(
        "{% load compile_static %}{% inlinecompile compiler='sass' %}source{% endinlinecompile %}"
    )
    assert template.render(django.template.Context({})) == "compiled"

    assert get_compiler_by_name.calls == [pretend.call("sass")]
    assert compiler.compile_source.calls == [pretend.call("source")]
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    

    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = 

        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = 

        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

import optparse, sys

from twisted.internet import defer
from twisted.internet.protocol import Protocol, ClientFactory
from twisted.protocols.basic import NetstringReceiver


def parse_args():
    usage = 


    parser = optparse.OptionParser(usage)

    _, addresses = parser.parse_args()

    if len(addresses) < 2:
        print parser.format_help()
        parser.exit()

    def parse_address(addr):
        if ':' not in addr:
            host = '127.0.0.1'
            port = addr
        else:
            host, port = addr.split(':', 1)

        if not port.isdigit():
            parser.error('Ports must be integers.')

        return host, int(port)

    return map(parse_address, addresses)


class PoetryProtocol(Protocol):

    poem = ''

    def dataReceived(self, data):
        self.poem += data

    def connectionLost(self, reason):
        self.poemReceived(self.poem)

    def poemReceived(self, poem):
        self.factory.poem_finished(poem)


class PoetryClientFactory(ClientFactory):

    protocol = PoetryProtocol

    def __init__(self, deferred):
        self.deferred = deferred

    def poem_finished(self, poem):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.callback(poem)

    def clientConnectionFailed(self, connector, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)


class TransformClientProtocol(NetstringReceiver):

    def connectionMade(self):
        self.sendRequest(self.factory.xform_name, self.factory.poem)

    def sendRequest(self, xform_name, poem):
        self.sendString(xform_name + '.' + poem)

    def stringReceived(self, s):
        self.transport.loseConnection()
        self.poemReceived(s)

    def poemReceived(self, poem):
        self.factory.handlePoem(poem)


class TransformClientFactory(ClientFactory):

    protocol = TransformClientProtocol

    def __init__(self, xform_name, poem):
        self.xform_name = xform_name
        self.poem = poem
        self.deferred = defer.Deferred()

    def handlePoem(self, poem):
        d, self.deferred = self.deferred, None
        d.callback(poem)

    def clientConnectionLost(self, _, reason):
        if self.deferred is not None:
            d, self.deferred = self.deferred, None
            d.errback(reason)

    clientConnectionFailed = clientConnectionLost


class TransformProxy(object):
    


    def __init__(self, host, port):
        self.host = host
        self.port = port

    def xform(self, xform_name, poem):
        factory = TransformClientFactory(xform_name, poem)
        from twisted.internet import reactor
        reactor.connectTCP(self.host, self.port, factory)
        return factory.deferred


def get_poetry(host, port):
    

    d = defer.Deferred()
    from twisted.internet import reactor
    factory = PoetryClientFactory(d)
    reactor.connectTCP(host, port, factory)
    return d


def poetry_main():
    addresses = parse_args()

    xform_addr = addresses.pop(0)

    proxy = TransformProxy(*xform_addr)

    from twisted.internet import reactor

    @defer.inlineCallbacks
    def get_transformed_poem(host, port):
        try:
            poem = yield get_poetry(host, port)
        except Exception, e:
            print >>sys.stderr, 'The poem download failed:', e
            raise

        try:
            poem = yield proxy.xform('cummingsify', poem)
        except Exception:
            print >>sys.stderr, 'Cummingsify failed!'

        defer.returnValue(poem)

    def got_poem(poem):
        print poem

    ds = []

    for (host, port) in addresses:
        d = get_transformed_poem(host, port)
        d.addCallbacks(got_poem)
        ds.append(d)

    dlist = defer.DeferredList(ds, consumeErrors=True)
    dlist.addCallback(lambda res : reactor.stop())

    reactor.run()


if __name__ == '__main__':
    poetry_main()
from datetime import datetime, timedelta
import unittest
from hamcrest import assert_that, has_entry, is_
import pytz
from performanceplatform.collector.pingdom import \
    convert_from_pingdom_to_performanceplatform, truncate_hour_fraction, \
    parse_time_range

from freezegun import freeze_time


class TestCollect(unittest.TestCase):
    def test_converting_from_pingdom_to_performanceplatform_records(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }

        name_of_check = 'testCheck'
        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc,
                    has_entry('_id', 'testCheck.2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('_timestamp', '2013-06-15T22:00:00+00:00'))
        assert_that(doc, has_entry('check', 'testCheck'))
        assert_that(doc, has_entry('avgresponse', 721))
        assert_that(doc, has_entry('uptime', 3599))
        assert_that(doc, has_entry('downtime', 523))
        assert_that(doc, has_entry('unmonitored', 12))

    def test_converting_to_pp_record_removes_whitespace_from_id(self):
        hourly_stats = {
            u'avgresponse': 721,
            u'downtime': 523,
            u'starttime': datetime(2013, 6, 15, 22, 0, tzinfo=pytz.UTC),
            u'unmonitored': 12,
            u'uptime': 3599
        }
        name_of_check = "name with whitespace"

        doc = convert_from_pingdom_to_performanceplatform(
            hourly_stats, name_of_check)

        assert_that(doc, has_entry('_id', 'name_with_whitespace.'
                                          '2013-06-15T22:00:00+00:00'))

    def test_truncate_hour_fraction(self):
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 0, 0, 0)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )
        assert_that(
            truncate_hour_fraction(datetime(2013, 6, 15, 22, 1, 2, 3)),
            is_(datetime(2013, 6, 15, 22, 0, 0, 0))
        )

    def test_start_date_no_end_date_results_in_period_until_now(self):
        start_dt = datetime(2013, 6, 15, 13, 0)
        end_dt = None

        now = datetime(2014, 2, 15, 11, 0)

        with freeze_time(now):
            assert_that(
                parse_time_range(start_dt, end_dt),
                is_((start_dt, now)))

    def test_end_date_no_start_date_results_in_2005_until_start_date(self):
        start_dt = None
        end_dt = datetime(2013, 6, 15, 13, 0)

        earliest = datetime(2005, 1, 1)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((earliest, end_dt)))

    def test_start_and_end_results_in_correct_period(self):
        start_dt = datetime(2013, 6, 10, 10, 0)
        end_dt = datetime(2013, 6, 15, 13, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((start_dt, end_dt)))

    def test_parse_time_range_no_start_or_end_uses_last_24_hours(self):
        now = datetime(2013, 12, 27, 13, 0, 0)
        one_day_ago = now - timedelta(days=1)

        with freeze_time(now):
            assert_that(
                parse_time_range(None, None),
                is_((one_day_ago, now)))

    def test_parse_time_range_truncates_the_hour(self):
        start_dt = datetime(2013, 6, 27, 10, 15, 34)
        end_dt = datetime(2013, 6, 30, 13, 45, 30)

        expected_start_dt = datetime(2013, 6, 27, 10, 0, 0)
        expected_end_dt = datetime(2013, 6, 30, 13, 0, 0)

        assert_that(
            parse_time_range(start_dt, end_dt),
            is_((expected_start_dt, expected_end_dt)))



import py
from pypy.rpython.lltypesystem import lltype
from pypy.rpython.ootypesystem import ootype
from pypy.rpython.lltypesystem.lloperation import llop
from pypy.rlib.rarithmetic import ovfcheck, r_uint, intmask
from pypy.jit.metainterp.history import BoxInt, ConstInt, check_descr
from pypy.jit.metainterp.history import INT, REF, ConstFloat
from pypy.jit.metainterp import resoperation
from pypy.jit.metainterp.resoperation import rop




def do_int_add(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() + box2.getint()))

def do_int_sub(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() - box2.getint()))

def do_int_mul(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() * box2.getint()))

def do_int_floordiv(cpu, box1, box2):
    z = llop.int_floordiv(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_mod(cpu, box1, box2):
    z = llop.int_mod(lltype.Signed, box1.getint(), box2.getint())
    return ConstInt(z)

def do_int_and(cpu, box1, box2):
    return ConstInt(box1.getint() & box2.getint())

def do_int_or(cpu, box1, box2):
    return ConstInt(box1.getint() | box2.getint())

def do_int_xor(cpu, box1, box2):
    return ConstInt(box1.getint() ^ box2.getint())

def do_int_rshift(cpu, box1, box2):
    return ConstInt(box1.getint() >> box2.getint())

def do_int_lshift(cpu, box1, box2):
    return ConstInt(intmask(box1.getint() << box2.getint()))

def do_uint_rshift(cpu, box1, box2):
    v = r_uint(box1.getint()) >> r_uint(box2.getint())
    return ConstInt(intmask(v))


def do_int_lt(cpu, box1, box2):
    return ConstInt(box1.getint() < box2.getint())

def do_int_le(cpu, box1, box2):
    return ConstInt(box1.getint() <= box2.getint())

def do_int_eq(cpu, box1, box2):
    return ConstInt(box1.getint() == box2.getint())

def do_int_ne(cpu, box1, box2):
    return ConstInt(box1.getint() != box2.getint())

def do_int_gt(cpu, box1, box2):
    return ConstInt(box1.getint() > box2.getint())

def do_int_ge(cpu, box1, box2):
    return ConstInt(box1.getint() >= box2.getint())

def do_uint_lt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) < r_uint(box2.getint()))

def do_uint_le(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) <= r_uint(box2.getint()))

def do_uint_gt(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) > r_uint(box2.getint()))

def do_uint_ge(cpu, box1, box2):
    return ConstInt(r_uint(box1.getint()) >= r_uint(box2.getint()))


def do_int_is_true(cpu, box1):
    return ConstInt(bool(box1.getint()))

def do_int_neg(cpu, box1):
    return ConstInt(intmask(-box1.getint()))

def do_int_invert(cpu, box1):
    return ConstInt(~box1.getint())

def do_bool_not(cpu, box1):
    return ConstInt(not box1.getint())

def do_same_as(cpu, box1):
    return box1

def do_oononnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(x)

def do_ooisnull(cpu, box1):
    tp = box1.type
    if tp == INT:
        x = bool(box1.getint())
    elif tp == REF:
        x = bool(box1.getref_base())
    else:
        assert False
    return ConstInt(not x)

def do_oois(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() == box2.getint()
    elif tp == REF:
        x = box1.getref_base() == box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_ooisnot(cpu, box1, box2):
    tp = box1.type
    assert tp == box2.type
    if tp == INT:
        x = box1.getint() != box2.getint()
    elif tp == REF:
        x = box1.getref_base() != box2.getref_base()
    else:
        assert False
    return ConstInt(x)

def do_subclassof(cpu, box1, box2):
    return ConstInt(cpu.ts.subclassOf(cpu, box1, box2))


def do_int_add_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x + y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_sub_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x - y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)

def do_int_mul_ovf(cpu, box1, box2):
    x = box1.getint()
    y = box2.getint()
    try:
        z = ovfcheck(x * y)
    except OverflowError:
        ovf = True
        z = 0
    else:
        ovf = False
    cpu._overflow_flag = ovf
    return BoxInt(z)


def do_float_neg(cpu, box1):
    return ConstFloat(-box1.getfloat())

def do_float_abs(cpu, box1):
    return ConstFloat(abs(box1.getfloat()))

def do_float_is_true(cpu, box1):
    return ConstInt(bool(box1.getfloat()))

def do_float_add(cpu, box1, box2):
    return ConstFloat(box1.getfloat() + box2.getfloat())

def do_float_sub(cpu, box1, box2):
    return ConstFloat(box1.getfloat() - box2.getfloat())

def do_float_mul(cpu, box1, box2):
    return ConstFloat(box1.getfloat() * box2.getfloat())

def do_float_truediv(cpu, box1, box2):
    return ConstFloat(box1.getfloat() / box2.getfloat())

def do_float_lt(cpu, box1, box2):
    return ConstInt(box1.getfloat() < box2.getfloat())

def do_float_le(cpu, box1, box2):
    return ConstInt(box1.getfloat() <= box2.getfloat())

def do_float_eq(cpu, box1, box2):
    return ConstInt(box1.getfloat() == box2.getfloat())

def do_float_ne(cpu, box1, box2):
    return ConstInt(box1.getfloat() != box2.getfloat())

def do_float_gt(cpu, box1, box2):
    return ConstInt(box1.getfloat() > box2.getfloat())

def do_float_ge(cpu, box1, box2):
    return ConstInt(box1.getfloat() >= box2.getfloat())

def do_cast_float_to_int(cpu, box1):
    return ConstInt(int(int(box1.getfloat())))

def do_cast_int_to_float(cpu, box1):
    return ConstFloat(float(box1.getint()))


def do_debug_merge_point(cpu, box1):
    from pypy.jit.metainterp.warmspot import get_stats
    loc = box1._get_str()
    get_stats().add_merge_point_location(loc)



def make_execute_list(cpuclass):
    from pypy.jit.backend.model import AbstractCPU
        def wrap(fn):
            def myfn(*args):
                print '<<<', fn.__name__
                try:
                    return fn(*args)
                finally:
                    print fn.__name__, '>>>'
            return myfn
    else:
        def wrap(fn):
            return fn
    execute_by_num_args = {}
    for key, value in rop.__dict__.items():
        if not key.startswith('_'):
            if (rop._FINAL_FIRST <= value <= rop._FINAL_LAST or
                rop._GUARD_FIRST <= value <= rop._GUARD_LAST):
                continue
            num_args = resoperation.oparity[value]
            withdescr = resoperation.opwithdescr[value]
            if withdescr and num_args >= 0:
                num_args += 1
            if num_args not in execute_by_num_args:
                execute_by_num_args[num_args] = [None] * (rop._LAST+1)
            execute = execute_by_num_args[num_args]
            if execute[value] is not None:
                raise Exception("duplicate entry for op number %d" % value)
            if key.endswith('_PURE'):
                key = key[:-5]
            name = 'do_' + key.lower()
            if hasattr(cpuclass, name):
                execute[value] = wrap(getattr(cpuclass, name))
            elif name in globals():
                execute[value] = wrap(globals()[name])
            else:
                assert hasattr(AbstractCPU, name), name
    cpuclass._execute_by_num_args = execute_by_num_args


def get_execute_funclist(cpu, num_args):
    return cpu._execute_by_num_args[num_args]
get_execute_funclist._annspecialcase_ = 'specialize:memo'

def get_execute_function(cpu, opnum, num_args):
    return cpu._execute_by_num_args[num_args][opnum]
get_execute_function._annspecialcase_ = 'specialize:memo'

def has_descr(opnum):
    return resoperation.opwithdescr[opnum]
has_descr._annspecialcase_ = 'specialize:memo'


def execute(cpu, opnum, descr, *argboxes):
    if has_descr(opnum):
        check_descr(descr)
        argboxes = argboxes + (descr,)
    else:
        assert descr is None
    func = get_execute_function(cpu, opnum, len(argboxes))
    assert func is not None
    return func(cpu, *argboxes)
execute._annspecialcase_ = 'specialize:arg(1)'

def execute_varargs(cpu, opnum, argboxes, descr):
    check_descr(descr)
    func = get_execute_function(cpu, opnum, -1)
    assert func is not None
    return func(cpu, argboxes, descr)
execute_varargs._annspecialcase_ = 'specialize:arg(1)'


def execute_nonspec(cpu, opnum, argboxes, descr=None):
    arity = resoperation.oparity[opnum]
    assert arity == -1 or len(argboxes) == arity
    if resoperation.opwithdescr[opnum]:
        check_descr(descr)
        if arity == -1:
            func = get_execute_funclist(cpu, -1)[opnum]
            return func(cpu, argboxes, descr)
        if arity == 0:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, descr)
        if arity == 1:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], descr)
        if arity == 2:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], descr)
        if arity == 3:
            func = get_execute_funclist(cpu, 4)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2], descr)
    else:
        assert descr is None
        if arity == 1:
            func = get_execute_funclist(cpu, 1)[opnum]
            return func(cpu, argboxes[0])
        if arity == 2:
            func = get_execute_funclist(cpu, 2)[opnum]
            return func(cpu, argboxes[0], argboxes[1])
        if arity == 3:
            func = get_execute_funclist(cpu, 3)[opnum]
            return func(cpu, argboxes[0], argboxes[1], argboxes[2])
    raise NotImplementedError




import os
import time

from threading import Thread

from acrylamid.utils import force_unicode as u
from acrylamid.compat import PY2K
from acrylamid.helpers import joinurl

if PY2K:
    from SocketServer import TCPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
else:
    from socketserver import TCPServer
    from http.server import SimpleHTTPRequestHandler


class ReuseAddressServer(TCPServer):
    

    allow_reuse_address = True

    def serve_forever(self):
        

        while not self.kill_received:
            if not self.wait:
                self.handle_request()
            else:
                time.sleep(0.1)


class RequestHandler(SimpleHTTPRequestHandler):
    


    www_root = '.'
    log_error = lambda x, *y: None

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        return joinurl(u(os.getcwd()), self.www_root, path[len(u(os.getcwd())):])

    def end_headers(self):
        self.send_header("Cache-Control", "max-age=0, must-revalidate")
        SimpleHTTPRequestHandler.end_headers(self)


class Webserver(Thread):
    


    def __init__(self, port=8000, root='.', log_message=lambda x, *y: None):
        Thread.__init__(self)
        Handler = RequestHandler
        Handler.www_root = root
        Handler.log_message = log_message

        self.httpd = ReuseAddressServer(("", port), Handler)
        self.httpd.wait = False
        self.httpd.kill_received = False

    def setwait(self, value):
        self.httpd.wait = value
    wait = property(lambda self: self.httpd.wait, setwait)

    def run(self):
        self.httpd.serve_forever()
        self.join(1)

    def shutdown(self):
        

        self.httpd.kill_received = True
        self.httpd.socket.close()

import os, sys, pickle
from SimpleCV import *
from nose.tools import with_setup, nottest


black = Color.BLACK
white = Color.WHITE
red = Color.RED
green = Color.GREEN
blue = Color.BLUE


pair1 = ("../sampleimages/stereo1_left.png" , "../sampleimages/stereo1_right.png")
pair2 = ("../sampleimages/stereo2_left.png" , "../sampleimages/stereo2_right.png")
pair3 = ("../sampleimages/stereo1_real_left.png" , "../sampleimages/stereo1_real_right.png")
pair4 = ("../sampleimages/stereo2_real_left.png" , "../sampleimages/stereo2_real_right.png")
pair5 = ("../sampleimages/stereo3_real_left.png" , "../sampleimages/stereo3_real_right.png")

correct_pairs = [pair1,pair2,pair3,pair4,pair5]

standard_path = "./standard/"


def imgDiffs(test_imgs,name_stem,tolerance,path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"
        rhs = Image(fname)
        if( lhs.width == rhs.width and lhs.height == rhs.height ):
            diff = (lhs-rhs)
            val = np.average(diff.getNumpy())
            if( val > tolerance ):
                print val
                return True
    return False

def imgSaves(test_imgs, name_stem, path=standard_path):
    count = len(test_imgs)
    for idx in range(0,count):
        fname = standard_path+name_stem+str(idx)+".jpg"

def perform_diff(result,name_stem,tolerance=2.0,path=standard_path):
        imgSaves(result,name_stem,path)
        if( imgDiffs(result,name_stem,tolerance,path) ):
            assert False
        else:
            pass

def setup_context():
    img = Image(pair1[0])

def destroy_context():
    img = ""

@with_setup(setup_context, destroy_context)
def test_findFundamentalMat():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if ( not StereoImg.findFundamentalMat()):
            assert False

def test_findHomography():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        if (not StereoImg.findHomography()):
            assert False

def test_findDisparityMap():
    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="BM"))
    name_stem = "test_disparitymapBM"
    perform_diff(dips,name_stem)

    dips = []
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        dips.append(StereoImg.findDisparityMap(method="SGBM"))
    name_stem = "test_disparitymapSGBM"
    perform_diff(dips,name_stem)

def test_eline():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        F,ptsLeft,ptsRight = StereoImg.findFundamentalMat()
        for pts in ptsLeft :
            line = StereoImg.Eline(pts,F,2)
            if (line == None):
                assert False


def test_projectPoint():
    for pairs in correct_pairs :
        img1 = Image(pairs[0])
        img2 = Image(pairs[1])
        StereoImg = StereoImage(img1,img2)
        H,ptsLeft,ptsRight = StereoImg.findHomography()
        for pts in ptsLeft :
            line = StereoImg.projectPoint(pts,H,2)
            if (line == None):
                assert False


def test_StereoCalibration():
    cam = StereoCamera()
    try :
        cam1 = Camera(0)
        cam2 = Camera(1)
        cam1.getImage()
        cam2.getImage()
        try :
            cam = StereoCamera()
            calib = cam.StereoCalibration(0,1,nboards=1)
            if (calib):
                assert True
            else :
                assert False
        except:
            assert False
    except :
        assert True

def test_loadCalibration():
    cam = StereoCamera()
    calbib =  cam.loadCalibration("Stereo","./StereoVision/")
    if (calbib) :
        assert True
    else :
        assert False

def test_StereoRectify():
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    if rectify :
        assert True
    else :
        assert False

def test_getImagesUndistort():
    img1 = Image(correct_pairs[0][0]).resize(352,288)
    img2 = Image(correct_pairs[0][1]).resize(352,288)
    cam = StereoCamera()
    calib = cam.loadCalibration("Stereo","./StereoVision/")
    rectify = cam.stereoRectify(calib)
    rectLeft,rectRight = cam.getImagesUndistort(img1,img2,calib,rectify)
    if rectLeft and rectRight :
        assert True
    else :
        assert False

try:
    from collections import OrderedDict
except:
    from gluon.contrib.simplejson.ordered_dict import OrderedDict

from gluon import current
from gluon.storage import Storage

def config(settings):
    


    T = current.T


    settings.base.prepopulate += ("PIANGO", "default/users")



    settings.mail.approver = "ADMIN"

    settings.gis.countries = ("FJ",)
    settings.gis.legend = "float"

    settings.L10n.utc_offset = "+1200"
    settings.L10n.decimal_separator = "."
    settings.L10n.thousands_separator = ","
    settings.fin.currencies = {
        "EUR" : "Euros",
        "FJD" : "Fiji Dollars",
        "GBP" : "Great British Pounds",
        "USD" : "United States Dollars",
    }
    settings.fin.currency_default = "USD"


    settings.org.sector = True
    settings.ui.cluster = True
    settings.org.resources_tab = True
    settings.org.groups = "Organization Group"

    settings.project.mode_3w = True
    settings.project.mode_drr = True
    settings.project.activities = True
    settings.project.activity_types = True
    settings.project.codes = True
    settings.project.demographics = True
    settings.project.hazards = True
    settings.project.programmes = True
    settings.project.projects = True
    settings.project.themes = True
    settings.project.multiple_organisations = True

    settings.req.req_type = ("Stock", "Other")

    def customise_project_activity_resource(r, tablename):

        s3db = current.s3db

        if r.tablename == "project_project":

            from s3 import S3SQLCustomForm, S3SQLInlineComponent, S3SQLInlineLink

            table = s3db.project_sector_project
            query = (table.project_id == r.id) & \
                    (table.deleted == False)
            rows = current.db(query).select(table.sector_id)
            sector_ids = [row.sector_id for row in rows]

            crud_form = S3SQLCustomForm("name",
                                        "status_id",
                                        S3SQLInlineLink("sector",
                                                        field = "sector_id",
                                                        label = T("Sectors"),
                                                        filterby = "id",
                                                        options = sector_ids,
                                                        widget = "groupedopts",
                                                        ),
                                        S3SQLInlineLink("activity_type",
                                                        field = "activity_type_id",
                                                        label = T("Activity Types"),
                                                        widget = "groupedopts",
                                                        ),
                                        "location_id",
                                        "date",
                                        "end_date",
                                        S3SQLInlineComponent("distribution",
                                                             fields = ["parameter_id",
                                                                       "value",
                                                                       (T("Intended Impact"), "comments"),
                                                                       ],
                                                             label = T("Distributed Supplies"),
                                                             ),
                                        "person_id",
                                        "comments",
                                        )

            s3db.configure(tablename,
                           crud_form = crud_form,
                           )

            list_fields = s3db.get_config(tablename, "list_fields")
            list_fields.insert(3, (T("Distributions"), "distribution.parameter_id"))

        elif r.tablename == "project_activity":
            list_fields = [("CSO", "project_id$organisation_id"),
                           (T("Activity"), "name"),
                           (T("Intended Impact"), "distribution.comments"),
                           (T("Location"), "location_id"),
                           ]

            s3db.configure(tablename,
                           deletable = False,
                           editable = False,
                           insertable = False,
                           list_fields = list_fields,
                           )

    settings.customise_project_activity_resource = customise_project_activity_resource

    settings.modules = OrderedDict([
        ("default", Storage(
            name_nice = T("Home"),
        )),
        ("admin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("appadmin", Storage(
            name_nice = T("Administration"),
            restricted = True,
        )),
        ("errors", Storage(
            name_nice = T("Ticket Viewer"),
            restricted = False,
        )),
        ("gis", Storage(
            name_nice = T("Map"),
            restricted = True,
        )),
        ("pr", Storage(
            name_nice = T("Person Registry"),
            restricted = True,
            module_type = 10
        )),
        ("org", Storage(
            name_nice = T("Organizations"),
            restricted = True,
            module_type = 1
        )),
        ("hrm", Storage(
            name_nice = T("Staff"),
            restricted = True,
            module_type = 2,
        )),
        ("vol", Storage(
            name_nice = T("Volunteers"),
            restricted = True,
            module_type = 2,
        )),
        ("cms", Storage(
          name_nice = T("Content Management"),
          restricted = True,
          module_type = 10,
        )),
        ("doc", Storage(
            name_nice = T("Documents"),
            restricted = True,
            module_type = 10,
        )),
        ("supply", Storage(
            name_nice = T("Supply Chain Management"),
            restricted = True,
        )),
        ("inv", Storage(
            name_nice = T("Warehouses"),
            restricted = True,
            module_type = 4
        )),
        ("req", Storage(
            name_nice = T("Requests"),
            restricted = True,
            module_type = 10,
        )),
        ("project", Storage(
            name_nice = T("Projects"),
            restricted = True,
            module_type = 2
        )),
        ("event", Storage(
            name_nice = T("Events"),
            restricted = True,
            module_type = 10,
        )),
        ("stats", Storage(
            name_nice = T("Statistics"),
            restricted = True,
            module_type = None,
        )),
        ("survey", Storage(
            name_nice = T("Surveys"),
            restricted = True,
            module_type = None,
        )),
    ])


from injector import GetInjector, PostInjector, CookieInjector, \
    UserAgentInjector, CmdInjector
from context import Context
from exceptions import OutboundException, PluginMustOverride, Unavailable
from async import AsyncPool
from wrappers import DatabaseWrapper, TableWrapper, FieldWrapper

DBS_ENUM = 0x01
TABLES_ENUM = 0x02
COLS_ENUM = 0x04
FIELDS_ENUM = 0x08
STR = 0x10
COMMENT = 0x20

class DBMSFactory:
    


    def __init__(self, plugin_class, name, desc):
        self._clazz = plugin_class
        self._name = name
        self._desc = desc

    def get(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(GetInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def post(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(PostInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def user_agent(self, method, context=Context(), limit_max_count=500):
        

        inst = self._clazz(UserAgentInjector(method, context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cookie(self, method='GET', context=Context(), limit_max_count=500, data=None, content_type=None):
        

        inst = self._clazz(CookieInjector(method, context, data, content_type), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def cmd(self, context=Context(), limit_max_count=500):
        

        inst = self._clazz(CmdInjector(context), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst

    def custom(self, injector, *args, **kwargs):
        

        if 'limit_max_count' in kwargs:
            limit_max_count = kwargs['limit_max_count']
            del kwargs['limit_max_count']
        else:
            limit_max_count = 500
        inst = self._clazz(injector(*args, **kwargs), limit_max_count)
        inst.name = self._name
        inst.desc = self._desc
        return inst


class dbms:
    


    def __init__(self, name, desc):
        

        self.name = name
        self.desc = desc

    def __call__(self, inst):
        return DBMSFactory(inst, self.name, self.desc)


class allow:
    



    def __init__(self, flags):
        self.flags = flags

    def __call__(self, inst):
        def wrapped_(*args):
            a = inst(*args)
            if hasattr(a, 'capabilities'):
                a.capabilities |= self.flags
            else:
                a.capabilities = self.flags
            return a

        return wrapped_


class DBMS:
    


    def __init__(self, forge, injector=None, limit_count_max=500):
        

        self.forge_class = forge
        self.context = injector.get_context()
        self.forge = forge(self.context)
        self.injector = injector
        self.limit_count_max = limit_count_max
        self.current_db = None
        self.current_user = None


    def has_cap(self, cap):
        

        return (self.capabilities & cap) == cap

    def determine(self):
        

        raise PluginMustOverride

    def get_forge(self):
        

        return self.forge


    def set_injector(self, injector):
        

        method = self.injector.getMethod()
        self.injector = injector(self.context, method)

    def  get_injector(self):
        

        return self.injector

    def set_forge(self, forge):
        

        self.forge_class = forge
        self.forge = forge(self.context)


    def set_trigger(self, trigger):
        

        self.injector.set_trigger(trigger)


    def use(self, db):
        

        self.current_db = db
        return DatabaseWrapper(self, self.current_db)

    def apply_bisec(self, cdt, min, max):
        

        while (max - min) > 1:
            mid = (max - min) / 2 + min
            if self.injector.inject(self.forge.wrap_bisec(self.forge.forge_cdt(cdt, mid))):
                max = mid
            else:
                min = mid
        return min


    def get_inband_str(self, sql):
        

        return self.injector.inject(self.forge.wrap_sql(self.forge.forge_second_query(sql)))


    def get_inband_int(self, sql):
        

        return int(self.get_inband_str(sql))

    def get_blind_int(self, sql):
        

        pool = AsyncPool(self)
        if self.context.is_multithread():
            pool.add_bisec_task(sql, 0, self.limit_count_max)
        else:
            pool.add_classic_bisec_task(sql, 0, self.limit_count_max)
        pool.solve_tasks()
        return pool.result[0]


    def get_char(self, sql, pos):
        

        return self.forge.get_char(sql, pos)

    def get_blind_str(self, sql):
        

        size = self.get_blind_int(self.forge.string_len(sql))
        if size == (self.limit_count_max - 1):
            raise OutboundException()
        if self.context.is_multithread():
            pool = AsyncPool(self)
            for p in range(size):
                pool.add_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
            pool.solve_tasks()
            return pool.get_str_result()
        else:
            result = ''
            for p in range(size):
                pool = AsyncPool(self)
                pool.add_classic_bisec_task(self.forge.ascii(self.forge.get_char(sql, p + 1)), 0, 255)
                pool.solve_tasks()
                result += pool.get_str_result()
            return result

    def get_int(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_int(sql)
        else:
            return self.get_inband_int(sql)

    def get_str(self, sql):
        

        if self.context.is_blind():
            return self.get_blind_str(sql)
        else:
            return self.get_inband_str(sql)

    def version(self):
        

        return self.get_str(self.forge.get_version())

    def database(self, db=None):
        

        if db is not None:
            return DatabaseWrapper(self, db)
        else:
            self.current_db = self.get_str(self.forge.get_current_database())
            return DatabaseWrapper(self, self.current_db)

    def get_nb_databases(self):
        

        return self.get_int(self.forge.get_nb_databases())

    def get_database_name(self, id):
        

        return self.get_str(self.forge.get_database_name(id))

    def databases(self):
        

        if self.has_cap(DBS_ENUM):
            n = self.get_nb_databases()
            for i in range(n):
                yield DatabaseWrapper(self, self.get_database_name(i))
        else:
            raise Unavailable()


    def get_nb_tables(self, db=None):
        

        if db:
            return self.get_int(self.forge.get_nb_tables(db=db))
        else:
            db = self.database()
            return self.get_int(self.forge.get_nb_tables(db=db))

    def get_table_name(self, id, db=None):
        

        return self.get_str(self.forge.get_table_name(id, db=db))

    def tables(self, db=None):
        

        if self.has_cap(TABLES_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db

            n = self.get_nb_tables(db)
            for i in range(n):
                yield TableWrapper(self, self.get_table_name(i, db), db)
        else:
            raise Unavailable()

    def get_nb_fields(self, table, db):
        

        return self.get_int(self.forge.get_nb_fields(table, db=db))

    def get_field_name(self, table, id, db):
        

        return self.get_str(self.forge.get_field_name(table, id, db))

    def fields(self, table, db=None):
        

        if self.has_cap(FIELDS_ENUM):
            if db is None:
                if self.current_db is None:
                    self.database()
                db = self.current_db
            n = self.get_nb_fields(table, db)
            for i in range(n):
                yield FieldWrapper(self, table, db, self.get_field_name(table, i, db))
        else:
            raise Unavailable()


    def user(self):
        

        return self.get_str(self.forge.get_user())

    def count_table_records(self, table, db=None, max=1000000):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_int(self.forge.count(self.forge.select_all(table, db)))


    def get_record_field_value(self, field, table, pos, db=None):
        

        if db is None:
            if self.currrent_db is None:
                self.database()
            db = self.current_db
        return self.get_str(self.forge.get_table_field_record(field, table, db, pos))

    def __getitem__(self, i):
        

        if isinstance(i, (int, long)):
            d = self.getDatabaseName(i)
            if d is None:
                raise IndexError
            else:
                return DatabaseWrapper(self, d)
        elif isinstance(i, basestring):
            return DatabaseWrapper(self, i)


    def __len__(self):
        

        return self.get_nb_databases()

import pytest

xfail = pytest.mark.xfail

import framework.routes as routes
import re
from framework.routes import get, post, put, delete, abort, \
    build_path_pattern, find_path


def test_get():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['GET']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['GET'].remove(found)
    assert len(routes.routes['GET']) == start_ln


def test_post():
    


    start_ln = len(routes.routes['POST'])

    @post('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['POST']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['POST'].remove(found)
    assert len(routes.routes['POST']) == start_ln


def test_put():
    


    start_ln = len(routes.routes['PUT'])

    @put('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['PUT']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['PUT'].remove(found)
    assert len(routes.routes['PUT']) == start_ln


def test_delete():
    


    start_ln = len(routes.routes['DELETE'])

    @delete('/s/foo')
    def foo_route(request):
        return 200, ''

    for path, fn in routes.routes['DELETE']:
        if fn == foo_route:
            found = (path, fn)
    assert found
    routes.routes['DELETE'].remove(found)
    assert len(routes.routes['DELETE']) == start_ln


def test_build_path_pattern():
    


    assert (build_path_pattern('/foo') ==
            re.compile('^/foo/?$'))
    assert (build_path_pattern('/foo/{u_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/?$'))
    assert (build_path_pattern('/foo/{u_id}/aaa/{n_id}') ==
            re.compile('^/foo/(?P<u_id>[\w\-]+)/aaa/(?P<n_id>[\w\-]+)/?$'))


def test_find_path():
    


    start_ln = len(routes.routes['GET'])

    @get('/s/foo/{u_id}')
    def foo_route(request):
        return 200, ''

    fn, params = find_path('GET', '/s/foo/a1')

    assert fn == foo_route
    assert params == {'u_id': 'a1'}

    path = re.compile('^/s/foo/(?P<u_id>[\w\-]+)/?$')
    routes.routes['GET'].remove((path, fn))
    assert len(routes.routes['GET']) == start_ln


def test_abort():
    


    code, response = abort(404)
    assert code == 404
    assert 'errors' in response
    assert response['errors'][0]['message'] == '404 Not Found'

import os
import os.path
import sys
import socket
import sublime
import sublime_plugin
import subprocess
import threading
import json
import time
import re
from functools import reduce

if int(sublime.version()) < 3000:
    import symbols
    from sublime_haskell_common import *
else:
    import SublimeHaskell.symbols as symbols
    from SublimeHaskell.sublime_haskell_common import *

def concat_args(args):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        return (px or py, (ex if px else []) + (ey if py else []))
    return reduce(cat, args, (True, []))[1]

def concat_opts(opts):
    def cat(x, y):
        (px, ex) = x
        (py, ey) = y
        v = (ex if px else {}).copy()
        v.update((ey if py else {}).copy())
        return (px or py, v)
    return reduce(cat, opts, (True, {}))[1]

def flatten_opts(opts):
    r = []

    def to_opt(x):
        return '--{0}'.format(x)

    for k, v in opts.items():
        if v is None:
            r.append(to_opt(k))
        elif type(v) is list:
            for n in v:
                r.extend([to_opt(k), str(n)])
        else:
            r.extend([to_opt(k), str(v)])

    return r

def hsdev_enabled():
    return get_setting_async('enable_hsdev') == True

def hsdev_enable(enable = True):
    set_setting_async('enable_hsdev', enable)

def hsdev_version():
    try:
        (exit_code, out, err) = call_and_wait(['hsdev', 'version'])
        if exit_code == 0:
            m = re.match('(?P<major>\d+)\.(?P<minor>\d+)\.(?P<revision>\d+)\.(?P<build>\d+)', out)
            if m:
                major = int(m.group('major'))
                minor = int(m.group('minor'))
                revision = int(m.group('revision'))
                build = int(m.group('build'))
                return [major, minor, revision, build]
    except FileNotFoundError:
        pass
    return None

def show_version(ver):
    return '.'.join(map(lambda i: str(i), ver))

def check_version(ver, minimal = [0, 0, 0, 0], maximal = None):
    if ver is None:
        return False
    if ver < minimal:
        return False
    if maximal and ver >= maximal:
        return False
    return True

def if_some(x, lst):
    return lst if x is not None else []

def cabal_path(cabal):
    if not cabal:
        return []
    return ["--cabal"] if cabal == 'cabal' else ["--sandbox={0}".format(cabal)]

def hsinspect(module = None, file = None, cabal = None, ghc_opts = []):
    cmd = ['hsinspect']
    on_result = lambda s: s
    if module:
        cmd.extend([module])
        on_result = parse_module
    elif file:
        cmd.extend([file])
        on_result = parse_module
    elif cabal:
        cmd.extend([cabal])
    else:
        log('hsinspect must specify module, file or cabal', log_debug)
        return None

    for opt in ghc_opts:
        cmd.extend(['-g', opt])

    r = call_and_wait_tool(cmd, 'hsinspect', lambda s: json.loads(s), file, None)
    if r:
        if 'error' in r:
            log('hsinspect returns error: {0}'.format(r['error']), log_error)
        else:
            return on_result(r)
    return None

def print_status(s):
    print(s['status'])

def parse_database(s):
    if not s:
        return None
    if s and 'projects' in s and 'modules' in s:
        return (s['projects'], [parse_module(m) for m in s['modules']])
    return None

def parse_decls(s):
    if s is None:
        return None
    return [parse_module_declaration(decl) for decl in s]

def parse_modules_brief(s):
    if s is None:
        return None
    return [parse_module_id(m) for m in s]

def get_value(dc, ks, defval = None):
    if dc is None:
        return defval
    if type(ks) == list:
        cur = dc
        for k in ks:
            cur = cur.get(k)
            if cur is None:
                return defval
        return cur
    else:
        return dc.get(ks, defval)

def parse_package_db(d, defval = None):
    if type(d) == dict:
        pdb = get_value(d, 'package-db')
        return symbols.PackageDb(package_db = pdb) if pdb else defval
    if d == 'global-db':
        return symbols.PackageDb(global_db = True)
    if d == 'user-db':
        return symbols.PackageDb(user_db = True)
    return defval

def parse_position(d):
    if not d:
        return None
    line = get_value(d, 'line')
    column = get_value(d, 'column')
    if line is not None and column is not None:
        return symbols.Position(line, column)
    return None

def parse_location(d):
    loc = symbols.Location(
        get_value(d, 'file'),
        get_value(d, 'project'))
    if not loc.is_null():
        return loc
    loc = symbols.InstalledLocation(
        symbols.parse_package(get_value(d, 'package')),
        parse_package_db(get_value(d, 'db')))
    if not loc.is_null():
        return loc
    loc = symbols.OtherLocation(
        get_value(d, 'source'))
    if not loc.is_null():
        return loc
    return None

def parse_import(d):
    if not d:
        return None
    return symbols.Import(d['name'], d['qualified'], d.get('as'), parse_position(d.get('pos')))

def parse_module_id(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        [], [], {},
        parse_location(d.get('location')))

def parse_declaration(decl):
    try:
        what = decl['decl']['what']
        docs = crlf2lf(decl.get('docs'))
        name = decl['name']
        pos = parse_position(decl.get('pos'))
        imported = []
        if 'imported' in decl and decl['imported']:
            imported = [parse_import(d) for d in decl['imported']]
        defined = None
        if 'defined' in decl and decl['defined']:
            defined = parse_module_id(decl['defined'])

        if what == 'function':
            return symbols.Function(name, decl['decl'].get('type'), docs, imported, defined, pos)
        elif what == 'type':
            return symbols.Type(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'newtype':
            return symbols.Newtype(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'data':
            return symbols.Data(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        elif what == 'class':
            return symbols.Class(name, decl['decl']['info'].get('ctx'), decl['decl']['info'].get('args', []), decl['decl']['info'].get('def'), docs, imported, defined, pos)
        else:
            return None
    except Exception as e:
        log('Error pasring declaration: {0}'.format(e), log_error)
        return None

def parse_declarations(decls):
    if decls is None:
        return None
    return [parse_declaration(d) for d in decls]

def parse_module_declaration(d, parse_module_info = True):
    try:
        m = None
        if 'module-id' in d and parse_module_info:
            m = parse_module_id(d['module-id'])

        loc = parse_location(d['module-id'].get('location'))
        decl = parse_declaration(d['declaration'])

        if not decl:
            return None

        decl.module = m

        return decl
    except:
        return None

def parse_module(d):
    if d is None:
        return None
    return symbols.Module(
        d['name'],
        d.get('exports'),
        [parse_import(i) for i in d['imports']] if 'imports' in d else [],
        dict((decl['name'],parse_declaration(decl)) for decl in d['declarations']) if 'declarations' in d else {},
        parse_location(d.get('location')))

def parse_modules(ds):
    if ds is None:
        return None
    return [parse_module(d) for d in ds]

def parse_cabal_package(d):
    if d is None:
        return None
    return symbols.CabalPackage(
        d['name'],
        d.get('synopsis'),
        d.get('default-version'),
        d.get('installed-versions'),
        d.get('homepage'),
        d.get('license'))

def parse_corrections(d):
    if d is None:
        return None
    return [parse_correction(c) for c in d]

def parse_correction(d):
    return symbols.Correction(
        d['source']['file'],
        d['level'],
        d['note']['message'],
        parse_corrector(d['note']['corrector']))

def parse_corrector(d):
    return symbols.Corrector(
        parse_position(d['region']['from']),
        parse_position(d['region']['to']),
        d['contents'])

def encode_corrections(cs):
    return [encode_correction(c) for c in cs]

def encode_correction(c):
    return {
        'source': {
            'project': None,
            'file': c.file },
        'level': c.level,
        'note': {
            'corrector': encode_corrector(c.corrector),
            'message': c.message },
        'region': {
            'from': encode_position(c.corrector.start.from_zero_based()),
            'to': encode_position(c.corrector.end.from_zero_based()) }
        }

def encode_corrector(c):
    return {
        'region': {
            'from': encode_position(c.start),
            'to': encode_position(c.end) },
        'contents': c.contents }

def encode_position(p):
    return {
        'line': p.line,
        'column': p.column }

def encode_package_db(db):
    if db.user_db:
        return 'user-db'
    if db.global_db:
        return 'global-db'
    if db.package_db:
        return {'package-db': db.package_db}
    return None

def reconnect_function(fn):
    def wrapped(self, *args, **kwargs):
        autoconnect_ = kwargs.pop('autoconnect', False)
        on_reconnect_ = kwargs.pop('on_reconnect', None)
        just_connect_ = kwargs.pop('just_connect', False)
        def run_fn():
            if not just_connect_:
                self.autoconnect = autoconnect_
                self.on_reconnect = on_reconnect_
            return fn(self, *args, **kwargs)
        if not just_connect_:
            self.set_reconnect_function(run_fn)
        return run_fn()
    return wrapped

class begin_connecting(object):
    def __init__(self, agent):
        self.agent = agent

    def __enter__(self):
        self.agent.set_connecting()
        return self

    def __exit__(self, type, value, traceback):
        if type:
            self.agent.set_unconnected()
        else:
            if self.agent.is_connecting():
                self.agent.set_unconnected()

def connect_function(fn):
    def wrapped(self, *args, **kwargs):
        if self.is_unconnected():
            with begin_connecting(self):
                return fn(self, *args, **kwargs)
        else:
            log('hsdev already connected', log_warning)
    return wrapped

def hsdev_command(async = False, timeout = None, is_list = False):
    def wrap_function(fn):
        def wrapped(self, *args, **kwargs):
            wait_flag = kwargs.pop('wait', not async)
            timeout_arg = kwargs.pop('timeout', timeout)
            on_resp = kwargs.pop('on_response', None)
            on_not = kwargs.pop('on_notify', None)
            on_err = kwargs.pop('on_error', None)
            on_res_part = kwargs.pop('on_result_part', None)
            split_res = kwargs.pop('split_result', on_res_part is not None)

            (name_, opts_, on_result_) = fn(self, *args, **kwargs)

            if is_list and split_res:
                result = []
                def on_notify(n):
                    if 'result-part' in n:
                        rp = on_result_([n['result-part']])[0]
                        call_callback(on_res_part, rp)
                        result.append(rp)
                    else:
                        call_callback(on_not, n)
                def on_response(r):
                    on_resp(result)

                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_notify,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return result
                return r

            else:
                def on_response(r):
                    on_resp(on_result_(r))
                r = self.call(
                    name_,
                    opts_,
                    on_response = on_response if on_resp else None,
                    on_notify = on_not,
                    on_error = on_err,
                    wait = wait_flag,
                    timeout = timeout_arg)
                if wait_flag:
                    return on_result_(r)
                return r
        return wrapped
    return wrap_function

def command(fn):
    return hsdev_command(async = False, timeout = 1)(fn)

def async_command(fn):
    return hsdev_command(async = True)(fn)

def list_command(fn):
    return hsdev_command(async = False, timeout = 1, is_list = True)(fn)

def async_list_command(fn):
    return hsdev_command(async = True, is_list = True)(fn)

def cmd(name_, opts_ = {}, on_result = lambda r: r):
    return (name_, opts_, on_result)

def call_callback(fn, *args, **kwargs):
    name = kwargs.get('name')
    if name:
        del kwargs['name']
    try:
        if fn is not None:
            fn(*args, **kwargs)
    except Exception as e:
        log("callback '{0}' throws exception: {1}".format(name or '<unnamed>', e))

def format_error_details(ds):
    return ', '.join(['{}: {}'.format(k, v) for k, v in ds.items()])

class HsDevCallbacks(object):
    def __init__(self, id, command, on_response = None, on_notify = None, on_error = None):
        self.id = id
        self.command = command
        self.start_time = time.clock()
        self.on_response = on_response
        self.on_notify = on_notify
        self.on_error = on_error

    def time(self):
        return time.clock() - self.start_time if self.start_time is not None else None

    def log_time(self):
        log('{0}: {1} seconds'.format(self.command, self.time()), log_trace)

    def call_response(self, r):
        self.log_time()
        call_callback(self.on_response, r)

    def call_notify(self, n):
        call_callback(self.on_notify, n)

    def call_error(self, e, ds):
        self.log_time()
        log('{0} returns error: {1}, {2}'.format(self.command, e, format_error_details(ds)), log_error)
        call_callback(self.on_error, e, ds)


class HsDev(object):
    def __init__(self, port = 4567):
        self.port = port
        self.connecting = threading.Event()
        self.connected = threading.Event()
        self.socket = None
        self.listener = None
        self.hsdev_address = None
        self.autoconnect = True
        self.map = LockedObject({})
        self.id = 1

        self.connect_fun = None

        self.part = ''

        self.on_connected = None
        self.on_disconnected = None
        self.on_reconnect = None

    def __del__(self):
        self.close()

    def set_reconnect_function(self, f):
        if self.connect_fun is None:
            self.connect_fun = f

    def reconnect(self):
        if self.connect_fun is not None:
            log('Reconnecting to hsdev...', log_info)
            call_callback(self.on_reconnect, name = 'HsDev.on_reconnect')
            self.connect_fun()
        else:
            log('No reconnect function')


    @staticmethod
    def run_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "run"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        log('Starting hsdev server', log_info)
        p = call_and_wait(cmd, wait = False)
        if not p:
            log('Failed creating hsdev process', log_error)
            return None
        while True:
            output = crlf2lf(decode_bytes(p.stdout.readline()))
            m = re.match(r'^.*?hsdev> Server started at port (?P<port>\d+)$', output)
            if m:
                log('hsdev server started at port {0}'.format(m.group('port')))
                p.stdout.close()
                p.stderr.close()
                return p

    @staticmethod
    def start_server(port = 4567, cache = None, log_file = None, log_config = None):
        cmd = concat_args([
            (True, ["hsdev", "start"]),
            (port, ["--port", str(port)]),
            (cache, ["--cache", cache]),
            (log_file, ["--log", log_file]),
            (log_config, ["--log-config", log_config])])

        def parse_response(s):
            try:
                return {} if s.isspace() else json.loads(s)
            except Exception as e:
                return {'error': 'Invalid response', 'details': s}

        log('Starting hsdev server', log_info)

        ret = call_and_wait_tool(cmd, 'hsdev', '', None, None, None, check_enabled = False)
        if ret is not None:
            return ret
        return None


    @staticmethod
    def client(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect(autoconnect = autoconnect)
        return h

    @staticmethod
    def client_async(port = 4567, cache = None, autoconnect = False):
        start_server(port = port, cache = cache)
        h = HsDev(port = port)
        h.connect_async(autoconnect = autoconnect)
        return h


    @connect_function
    @reconnect_function
    def connect(self, tries = 10, delay = 1.0):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        for n in range(0, tries):
            try:
                log('connecting to hsdev server ({0})...'.format(n), log_info)
                self.socket.connect(('127.0.0.1', self.port))
                self.hsdev_socket = self.socket
                self.hsdev_address = '127.0.0.1'
                self.set_connected()
                self.listener = threading.Thread(target = self.listen)
                self.listener.start()
                log('connected to hsdev server', log_info)
                call_callback(self.on_connected, name = 'HsDev.on_connected')
                return True
            except Exception as e:
                log('failed to connect to hsdev server ({0})'.format(n), log_warning)
                time.sleep(delay)

        return False

    @reconnect_function
    def connect_async(self, tries = 10, delay = 1.0):
        thread = threading.Thread(
            target = self.connect,
            kwargs = { 'tries' : tries, 'delay': delay, 'just_connect' : True })
        thread.start()
  
    def wait(self, timeout = None):
        return self.connected.wait(timeout)

    def close(self):
        if self.is_unconnected():
            return
        self.connected.clear()
        if self.hsdev_socket:
            self.hsdev_socket.close()
            self.hsdev_socket = None
        self.socket.close()

    def is_connecting(self):
        return self.connecting.is_set()

    def is_connected(self):
        return self.connected.is_set()

    def is_unconnected(self):
        return (not self.is_connecting()) and (not self.is_connected())

    def set_unconnected(self):
        if self.connecting.is_set():
            self.connecting.clear()
        if self.connected.is_set():
            self.connected.clear()

    def set_connecting(self):
        self.set_unconnected()
        self.connecting.set()

    def set_connected(self):
        if self.is_connecting():
            self.connected.set()
            self.connecting.clear()
        else:
            log('HsDev.set_connected called while not in connecting state', log_debug)

    def on_receive(self, id, command, on_response = None, on_notify = None, on_error = None):
        with self.map as m:
            m[id] = HsDevCallbacks(id, command, on_response, on_notify, on_error)

    def verify_connected(self):
        if self.is_connected():
            return True
        else:
            self.connection_lost('verify_connected', 'no connection')
            return self.is_connected()

    def connection_lost(self, fn, e):
        if self.is_unconnected():
            return
        self.close()
        log('{0}: connection to hsdev lost: {1}'.format(fn, e), log_error)
        call_callback(self.on_disconnected, name = 'HsDev.on_disconnected')

        with self.map as m:
            for on_msg in m.values():
                on_msg.on_error('connection lost')
            m.clear()

        self.id = 1
        self.part = ''

        if self.autoconnect:
            self.reconnect()

    def call(self, command, opts = {}, on_response = None, on_notify = None, on_error = None, wait = False, timeout = None, id = None):
        args_cmd = 'hsdev {0}'.format(command)
        call_cmd = 'hsdev {0} with {1}'.format(command, opts)

        if not self.verify_connected():
            return None if wait else False

        try:
            wait_receive = threading.Event() if wait else None

            x = {}

            def on_response_(r):
                x['result'] = r
                call_callback(on_response, r)
                if wait_receive:
                    wait_receive.set()

            def on_error_(e, ds):
                call_callback(on_error, e, ds)
                if wait_receive:
                    wait_receive.set()

            if wait or on_response or on_notify or on_error:
                if id is None:
                    id = str(self.id)
                    self.id = self.id + 1
                self.on_receive(id, args_cmd, on_response_, on_notify, on_error_)

            opts.update({'no-file': True})
            opts.update({'id': id, 'command': command})
            msg = json.dumps(opts, separators = (',', ':'))

            self.hsdev_socket.sendall(msg.encode('utf-8'))
            self.hsdev_socket.sendall('\n'.encode('utf-8'))
            log(call_cmd, log_trace)

            if wait:
                wait_receive.wait(timeout)
                return x.get('result')

            return True
        except Exception as e:
            log('{0} fails with exception: {1}'.format(call_cmd, e), log_error)
            self.connection_lost('call', e)
            return False

    def listen(self):
        while self.verify_connected():
            try:
                resp = json.loads(self.get_response())
                if 'id' in resp:
                    callbacks = None
                    with self.map as m:
                        if resp['id'] in m:
                            callbacks = m[resp['id']]
                    if callbacks:
                        if 'notify' in resp:
                            callbacks.call_notify(resp['notify'])
                        if 'error' in resp:
                            err = resp.pop("error")
                            callbacks.call_error(err, resp)
                            with self.map as m:
                                m.pop(resp['id'])
                        if 'result' in resp:
                            callbacks.call_response(resp['result'])
                            with self.map as m:
                                m.pop(resp['id'])
            except Exception as e:
                self.connection_lost('listen', e)
                return

    def get_response(self):
        while not '\n' in self.part:
            self.part = self.part + self.socket.recv(65536).decode('utf-8')
        (r, _, post) = self.part.partition('\n')
        self.part = post
        return r


    @command
    def link(self, hold = False, **kwargs):
        return cmd('link', {
            'hold': hold })

    @command
    def ping(self):
        return cmd('ping', {}, lambda r: r and ('message' in r) and (r['message'] == 'pong'))

    @async_command
    def scan(self, cabal = False, sandboxes = [], projects = [], files = [], paths = [], ghc = [], contents = {}, docs = False, infer = False):
        return cmd('scan', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'paths': paths,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'docs': docs,
            'infer': infer })

    @async_command
    def docs(self, projects = [], files = [], modules = []):
        return cmd('docs', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_command
    def infer(self, projects = [], files = [], modules = []):
        return cmd('infer', {
            'projects': projects,
            'files': files,
            'modules': modules })

    @async_list_command
    def remove(self, cabal = False, sandboxes = [], projects = [], files = [], packages = []):
        return cmd('remove', {
            'projects': projects,
            'cabal': cabal,
            'sandboxes': sandboxes,
            'files': files,
            'packages': packages })

    @command
    def remove_all(self):
        return cmd('remove-all', {})

    @list_command
    def list_modules(self, project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('modules', {'filters': fs}, parse_modules_brief)

    @list_command
    def list_packages(self):
        return cmd('packages', {})

    @list_command
    def list_projects(self):
        return cmd('projects', {})

    @list_command
    def symbol(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False, locals = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('symbol', {'query': q, 'filters': fs, 'locals': locals}, parse_decls)

    @command
    def module(self, input = "", search_type = 'prefix', project = None, file = None, module = None, deps = None, sandbox = None, cabal = False, db = None, package = None, source = False, standalone = False):
        q = {'input': input, 'type': search_type}

        fs = []
        if project:
            fs.append({'project': project})
        if file:
            fs.append({'file': file})
        if module:
            fs.append({'module': module})
        if deps:
            fs.append({'deps': deps})
        if sandbox:
            fs.append({'cabal':{'sandbox': sandbox}})
        if cabal:
            fs.append({'cabal':'cabal'})
        if db:
            fs.append({'db': encode_package_db(db)})
        if package:
            fs.append({'package': package})
        if source:
            fs.append('sourced')
        if standalone:
            fs.append('standalone')

        return cmd('module', {'query': q, 'filters': fs}, parse_modules)

    @command
    def resolve(self, file, exports = False):
        return cmd('resolve', {'file': file, 'exports': exports}, parse_module)

    @command
    def project(self, project = None, path = None):
        return cmd('project', {'name': project} if project else {'path': path})

    @command
    def sandbox(self, path):
        return cmd('sandbox', {'path': path})

    @list_command
    def lookup(self, name, file):
        return cmd('lookup', {'name': name, 'file': file}, parse_decls)

    @list_command
    def whois(self, name, file):
        return cmd('whois', {'name': name, 'file': file}, parse_declarations)

    @list_command
    def scope_modules(self, file, input = '', search_type = 'prefix'):
        return cmd('scope modules', {'query': {'input': input, 'type': search_type}, 'file': file}, parse_modules_brief)

    @list_command
    def scope(self, file, input = '', search_type = 'prefix', global_scope = False):
        return cmd('scope', {'query': {'input': input, 'type': search_type}, 'global': global_scope, 'file': file}, parse_declarations)

    @list_command
    def complete(self, input, file, wide = False):
        return cmd('complete', {'prefix': input, 'wide': wide, 'file': file}, parse_declarations)

    @list_command
    def hayoo(self, query, page = None, pages = None):
        return cmd('hayoo', {'query': query, 'page': page or 0, 'pages': pages or 1}, parse_decls)

    @list_command
    def cabal_list(self, packages):
        cmd('cabal list', {'packages': packages}, lambda r: [parse_cabal_package(s) for s in r] if r else None)

    @list_command
    def lint(self, files = [], contents = {}, hlint = []):
        return cmd('lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'hlint-opts': hlint})

    @list_command
    def check(self, files = [], contents = {}, ghc = []):
        return cmd('check', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @list_command
    def check_lint(self, files = [], contents = {}, ghc = [], hlint = []):
        return cmd('check-lint', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc,
            'hlint-opts': hlint})

    @list_command
    def types(self, files = [], contents = {}, ghc = []):
        return cmd('types', {
            'files': files,
            'contents': [{'file': f, 'contents': cts} for f, cts in contents.items()],
            'ghc-opts': ghc})

    @command
    def ghcmod_lang(self):
        return cmd('ghc-mod lang')

    @command
    def ghcmod_flags(self):
        return cmd('ghc-mod flags')

    @list_command
    def ghcmod_type(self, file, line, column = 1, ghc = []):
        return cmd('ghc-mod type', {
            'position': {'line': int(line),'column': int(column)},
            'file': file,
            'ghc-opts': ghc })

    @list_command
    def ghcmod_check(self, files, ghc = []):
        return cmd('ghc-mod check', {'files': files, 'ghc-opts': ghc})

    @list_command
    def ghcmod_lint(self, files, hlint = []):
        return cmd('ghc-mod lint', {'files': files, 'hlint-opts': hlint})

    @list_command
    def ghcmod_check_lint(self, files, ghc = [], hlint = []):
        return cmd('ghc-mod check-lint', {'files': files, 'ghc-opts': ghc, 'hlint-opts': hlint})

    @list_command
    def autofix_show(self, messages):
        return cmd('autofix show', {'messages': messages}, parse_corrections)

    @list_command
    def autofix_fix(self, messages, rest = [], pure = False):
        return cmd('autofix fix', {'messages': messages, 'rest': rest, 'pure': pure}, parse_corrections)

    @list_command
    def ghc_eval(self, exprs):
        return cmd('ghc eval', {'exprs': exprs})

    @command
    def exit(self):
        return cmd('exit', {})

def wait_result(fn, *args, **kwargs):
    wait_receive = threading.Event()
    x = {'result': None}

    on_resp = kwargs.get('on_response')
    on_err = kwargs.get('on_error')

    def wait_response(r):
        x['result'] = r
        if on_resp:
            on_resp(r)
        wait_receive.set()
    def wait_error(e, ds):
        log('hsdev call fails with: {0}, {1}'.format(e, format_error_details(ds)))
        if on_err:
            on_err(e, ds)
        wait_receive.set()

    tm = kwargs.pop('timeout', 0.1)

    kwargs['on_response'] = wait_response
    kwargs['on_error'] = wait_error

    fn(*args, **kwargs)

    wait_receive.wait(tm)
    return x['result']

class HsDevProcess(threading.Thread):
    def __init__(self, port = 4567, cache = None, log_file = None, log_config = None):
        super(HsDevProcess, self).__init__()
        self.process = None
        self.on_start = None
        self.on_exit = None
        self.stop_event = threading.Event()
        self.create_event = threading.Event()
        self.port = port
        self.cache = cache
        self.log_file = log_file
        self.log_config = log_config

    def run(self):
        while True:
            self.create_event.wait()
            self.create_event.clear()
            while not self.stop_event.is_set():
                self.process = HsDev.run_server(port = self.port, cache = self.cache, log_file = self.log_file, log_config = self.log_config)
                if not self.process:
                    log('failed to create hsdev process', log_error)
                    self.stop_event.set()
                else:
                    call_callback(self.on_start, name = 'HsDevProcess.on_start')
                self.process.wait()
                call_callback(self.on_exit, name = 'HsDevProcess.on_exit')
            self.stop_event.clear()

    def active(self):
        return self.process.poll() is None

    def inactive(self):
        return self.process.poll() is not None

    def create(self):
        self.create_event.set()

    def stop(self):
        self.stop_event.set()




from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        

        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        

        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        

        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()
from __future__ import unicode_literals

DATE_FORMAT = r'j \d\e F \d\e Y'
TIME_FORMAT = 'G:i:s'
DATETIME_FORMAT = r'j \d\e F \d\e Y \a \l\e\s G:i'
YEAR_MONTH_FORMAT = r'F \d\e\l Y'
MONTH_DAY_FORMAT = r'j \d\e F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y G:i'

DATE_INPUT_FORMATS = (
    '%d/%m/%Y', '%d/%m/%y'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M:%S.%f',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M:%S.%f',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3




from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import difflib

import phlsys_arcconfig
import phlsys_arcrc
import phlsys_conduit


class InsufficientInfoException(Exception):

    def __init__(self, message):
        super(InsufficientInfoException, self).__init__(message)


def _make_exception(*args):
    return InsufficientInfoException("\n" + "\n\n".join(args))


def add_argparse_arguments(parser):
    


    connection = parser.add_argument_group(
        'connection arguments',
        'use these optional parameters to override settings present in your\n'
        '"~/.arcrc" or ".arcconfig" files')

    connection.add_argument(
        "--uri",
        type=str,
        metavar="ADDRESS",
        help="address of the phabricator instance to connect to.")

    connection.add_argument(
        "--user",
        type=str,
        metavar="NAME",
        help="name of the user to connect as.")

    connection.add_argument(
        "--cert",
        type=str,
        metavar="HEX",
        help="long certificate string of the user to connect as, you can find "
             "this string here: "
             "http://your.phabricator/settings/panel/conduit/. generally you "
             "wouldn't expect to enter this on the command-line and would "
             "make an ~/.arcrc file by using '$ arc install-certificate'.")

    connection.add_argument(
        '--act-as-user',
        type=str,
        metavar="NAME",
        help="name of the user to impersonate (admin only).\n")


def make_conduit(uri=None, user=None, cert=None, act_as_user=None):
    uri, user, cert, _ = get_uri_user_cert_explanation(uri, user, cert)
    return phlsys_conduit.Conduit(uri, user, cert, act_as_user)


def obscured_cert(cert):
    

    return cert[:4] + '...' + cert[-4:]


def get_uri_user_cert_explanation(uri, user, cert):
    if uri and user and cert:
        explanations = ["all parameters were supplied"]
        uri = _fix_uri(explanations, uri)
        return uri, user, cert, '\n\n'.join(explanations)

    arcrc, arcrc_path = _load_arcrc()
    arcconfig_path, arcconfig = _load_arcconfig()

    install_arc_url = str(
        "http://www.phabricator.com/docs/phabricator/article/"

    no_uri = "no uri to a Phabricator instance was specified."
    no_user = "no username for the Phabricator instance was specified."
    no_cert = "no certificate for the Phabricator instance was specified."
    no_arcconfig = (
        "couldn't find an .arcconfig, this file should contain "
        "the uri for the phabricator instance you wish to connect "
        "to.\n"
        "we search for it in the current working directory and in "
        "the parent directories\n"
        "here is an example .arcconfig:\n"
        "{\n"
        "    \"conduit_uri\" : \"https://your.phabricator/\"\n"
        "}")
    no_arcrc = (
        "couldn't find ~/.arcrc, this file should contain "
        "usernames and certificates which will allow us to authenticate with "
        "Phabricator.\n"
        "To generate a valid ~/.arcrc for a particular instance, you may "
        "run:\n"
        "\n"
        "$ arc install-certificate [URI]\n"
        "N.B. to install arc:\n" + install_arc_url)
    bad_arcrc = (
        "can't load .arcrc, it may be invalid json or not permissioned\n"
        "path used: " + str(arcrc_path))
    bad_arcconfig = (
        "can't load .arcconfig, it may be invalid json or not permissioned\n"
        "path used: " + str(arcconfig_path))
    arcrc_no_default = (
        "no default uri was discovered in .arcrc, you may add one like so:\n"
        "$ arc set-config default https://your.phabricator/\n"
        "N.B. to install arc:\n" + install_arc_url)
    arcconfig_no_uri = (
        ".arcconfig doesn't seem to contain a conduit_uri entry\n"
        "path used: " + str(arcconfig_path))

    explanations = []

    if uri is None:
        if not arcconfig_path:
            if not arcrc_path:
                raise _make_exception(no_uri, no_arcconfig, no_arcrc)
            if arcrc is None:
                raise _make_exception(no_uri, no_arcconfig, bad_arcrc)
            if "config" in arcrc:
                uri = arcrc["config"].get("default", None)
            if uri is None:
                raise _make_exception(no_uri, no_arcconfig, arcrc_no_default)
            explanations.append(
                "got uri from 'default' entry in arcrc\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcrc_path, uri))
            if arcconfig is None:
                raise _make_exception(no_uri, bad_arcconfig)
            uri = arcconfig.get("conduit_uri", None)
            if uri is None:
                raise _make_exception(no_uri, arcconfig_no_uri)
            explanations.append(
                "got uri from .arcconfig\n"
                "  path: {0}\n"
                "  uri: {1}".format(arcconfig_path, uri))

    uri = _fix_uri(explanations, uri)

    arcrc_no_entry = (
        "no entry for the uri was found in .arcrc, you may add one like so:\n"
        "$ arc install-certificate " + uri + "\n"
        "N.B. to install arc:\n" + install_arc_url)

    if user is None:
        if not arcrc_path:
            raise _make_exception(no_user, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_user, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_user, arcrc_no_entry)
            user = host.get("user", None)
            explanations.append(
                "got user from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  user: {1}".format(arcrc_path, user))
            if cert is None:
                cert = host.get("cert", None)
                explanations.append(
                    "got cert from uri's entry in .arcrc\n"
                    "  path: {0}\n"
                    "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
            if user is None:
                raise _make_exception(no_user, arcrc_no_entry)
        if user is None:
            raise _make_exception(no_user, arcrc_no_entry)

    if cert is None:
        if not arcrc_path:
            raise _make_exception(no_cert, no_arcrc)
        if arcrc is None:
            raise _make_exception(no_cert, bad_arcrc)
        if "hosts" in arcrc:
            host = phlsys_arcrc.get_host(arcrc, uri)
            if host is None:
                raise _make_exception(no_cert, arcrc_no_entry)
            cert = host.get("cert", None)
            explanations.append(
                "got cert from uri's entry in .arcrc\n"
                "  path: {0}\n"
                "  cert: {1}".format(arcrc_path, obscured_cert(cert)))
        if cert is None:
            raise _make_exception(no_cert, arcrc_no_entry)

    if not (uri and user and cert) or arcrc_path is None:
        raise Exception("unexpected error determinining uri, user or cert")

    return uri, user, cert, '\n\n'.join(explanations)


def _load_arcconfig():
    arcconfig_path = phlsys_arcconfig.find_arcconfig()
    arcconfig = None
    try:
        if arcconfig_path is not None:
            arcconfig = phlsys_arcconfig.load(arcconfig_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcconfig_path, arcconfig


def _load_arcrc():
    arcrc_path = phlsys_arcrc.find_arcrc()
    arcrc = None
    try:
        if arcrc_path is not None:
            arcrc = phlsys_arcrc.load(arcrc_path)
    except ValueError:
        pass
    except EnvironmentError:
        pass
    return arcrc, arcrc_path


def _fix_uri(explanations, uri):
    old_uri = uri
    uri = phlsys_conduit.make_conduit_uri(uri)
    if uri != old_uri:
        diff = list(difflib.Differ().compare([old_uri], [uri]))
        diff = ['  ' + s.strip() for s in diff]
        diff = '\n'.join(diff)
        explanations.append("assumed uri to conduit:\n{0}".format(diff))
    return uri


from oscar.test.testcases import WebTestCase
from oscar.test import factories
from oscar.apps.basket import models


class TestAddingToBasket(WebTestCase):

    def test_works_for_standalone_product(self):
        product = factories.ProductFactory()

        detail_page = self.get(product.get_absolute_url())
        response = detail_page.forms['add_to_basket_form'].submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)

    def test_works_for_child_product(self):
        parent = factories.ProductFactory(structure='parent', stockrecords=[])
        for x in range(3):
            factories.ProductFactory(parent=parent, structure='child')

        detail_page = self.get(parent.get_absolute_url())
        form = detail_page.forms['add_to_basket_form']
        response = form.submit()

        self.assertIsRedirect(response)
        baskets = models.Basket.objects.all()
        self.assertEqual(1, len(baskets))

        basket = baskets[0]
        self.assertEqual(1, basket.num_items)
import uuid

import six
from pyramid import httpexceptions
from pyramid.settings import asbool
from pyramid.security import NO_PERMISSION_REQUIRED, Authenticated

from cliquet.errors import raise_invalid
from cliquet.events import ACTIONS
from cliquet.utils import build_request, reapply_cors, hmac_digest
from cliquet.storage import exceptions as storage_exceptions

from kinto.authorization import RouteFactory
from kinto.views.buckets import Bucket
from kinto.views.collections import Collection


def create_bucket(request, bucket_id):
    

    bucket_put = (request.method.lower() == 'put' and
                  request.path.endswith('buckets/default'))
    if bucket_put:
        return

    already_created = request.bound_data.setdefault('buckets', {})
    if bucket_id in already_created:
        return

    bucket = resource_create_object(request=request,
                                    resource_cls=Bucket,
                                    uri='/buckets/%s' % bucket_id,
                                    resource_name='bucket',
                                    obj_id=bucket_id)
    already_created[bucket_id] = bucket


def create_collection(request, bucket_id):
    subpath = request.matchdict.get('subpath')
    if not (subpath and subpath.startswith('collections/')):
        return

    collection_id = subpath.split('/')[1]
    collection_uri = '/buckets/%s/collections/%s' % (bucket_id, collection_id)

    already_created = request.bound_data.setdefault('collections', {})
    if collection_uri in already_created:
        return

    collection_put = (request.method.lower() == 'put' and
                      request.path.endswith(collection_id))
    if collection_put:
        return

    backup_matchdict = request.matchdict
    request.matchdict = dict(bucket_id=bucket_id,
                             id=collection_id,
                             **request.matchdict)
    collection = resource_create_object(request=request,
                                        resource_cls=Collection,
                                        uri=collection_uri,
                                        resource_name='collection',
                                        obj_id=collection_id)
    already_created[collection_uri] = collection
    request.matchdict = backup_matchdict


def resource_create_object(request, resource_cls, uri, resource_name, obj_id):
    

    context = RouteFactory(request)
    context.get_permission_object_id = lambda r, i: uri

    resource = resource_cls(request, context)

    if not resource.model.id_generator.match(obj_id):
        error_details = {
            'location': 'path',
            'description': "Invalid %s id" % resource_name
        }
        raise_invalid(resource.request, **error_details)

    data = {'id': obj_id}
    try:
        obj = resource.model.create_record(data)
        resource.request.current_resource_name = resource_name
        resource.postprocess(data, action=ACTIONS.CREATE)
    except storage_exceptions.UnicityError as e:
        obj = e.record
    return obj


def default_bucket(request):
    if request.method.lower() == 'options':
        path = request.path.replace('default', 'unknown')
        subrequest = build_request(request, {
            'method': 'OPTIONS',
            'path': path
        })
        return request.invoke_subrequest(subrequest)

    if Authenticated not in request.effective_principals:
        raise httpexceptions.HTTPForbidden()

    settings = request.registry.settings

    if asbool(settings['readonly']):
        raise httpexceptions.HTTPMethodNotAllowed()

    bucket_id = request.default_bucket_id
    path = request.path.replace('/buckets/default', '/buckets/%s' % bucket_id)
    querystring = request.url[(request.url.index(request.path) +
                               len(request.path)):]

    create_bucket(request, bucket_id)

    create_collection(request, bucket_id)

    subrequest = build_request(request, {
        'method': request.method,
        'path': path + querystring,
        'body': request.body
    })
    subrequest.bound_data = request.bound_data

    try:
        response = request.invoke_subrequest(subrequest)
    except httpexceptions.HTTPException as error:
        is_redirect = error.status_code < 400
        if error.content_type == 'application/json' or is_redirect:
            response = reapply_cors(subrequest, error)
        else:
            raise error
    return response


def default_bucket_id(request):
    settings = request.registry.settings
    secret = settings['userid_hmac_secret']
    digest = hmac_digest(secret, request.prefixed_userid)
    return six.text_type(uuid.UUID(digest[:32]))


def get_user_info(request):
    user_info = {
        'id': request.prefixed_userid,
        'bucket': request.default_bucket_id
    }
    return user_info


def includeme(config):
    config.add_view(default_bucket,
                    route_name='default_bucket',
                    permission=NO_PERMISSION_REQUIRED)
    config.add_view(default_bucket,
                    route_name='default_bucket_collection',
                    permission=NO_PERMISSION_REQUIRED)

    config.add_route('default_bucket_collection',
                     '/buckets/default/{subpath:.*}')
    config.add_route('default_bucket', '/buckets/default')

    config.add_request_method(default_bucket_id, reify=True)
    config.add_request_method(get_user_info)

    config.add_api_capability(
        "default_bucket",
        description="The default bucket is an alias for a personal"
                    " bucket where collections are created implicitly.",
        url="http://kinto.readthedocs.io/en/latest/api/1.x/"



import logging
import os
import sys

import numpy as np

import cellprofiler.cpmodule as cpm
import cellprofiler.measurements as cpmeas
import cellprofiler.settings as cps
import cellprofiler.utilities.rules as cprules
import cellprofiler.workspace as cpw
from cellprofiler.gui.help import USING_METADATA_TAGS_REF, USING_METADATA_HELP_REF
from cellprofiler.preferences import IO_FOLDER_CHOICE_HELP_TEXT
from cellprofiler.settings import YES, NO

logger = logging.getLogger(__name__)
C_ANY = "Flag if any fail"
C_ALL = "Flag if all fail"

S_IMAGE = "Whole-image measurement"
S_AVERAGE_OBJECT = "Average measurement for all objects in each image"
S_ALL_OBJECTS = "Measurements for all objects in each image"
S_RULES = "Rules"
S_CLASSIFIER = "Classifier"
S_ALL = [S_IMAGE, S_AVERAGE_OBJECT, S_ALL_OBJECTS, S_RULES, S_CLASSIFIER]



N_FIXED_SETTINGS = 1



N_FIXED_SETTINGS_PER_FLAG = 5

N_SETTINGS_PER_MEASUREMENT_V2 = 7
N_SETTINGS_PER_MEASUREMENT_V3 = 9


N_SETTINGS_PER_MEASUREMENT = 10


class FlagImage(cpm.CPModule):
    category = "Data Tools"
    variable_revision_number = 4
    module_name = "FlagImage"

    def create_settings(self):
        self.flags = []
        self.flag_count = cps.HiddenCount(self.flags)
        self.add_flag_button = cps.DoSomething("", "Add another flag",
                                               self.add_flag)
        self.spacer_1 = cps.Divider()
        self.add_flag(can_delete=False)

    def add_flag(self, can_delete=True):
        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("measurement_settings", [])
        group.append("measurement_count", cps.HiddenCount(group.measurement_settings))
        group.append("category",
                     cps.Text(
                             "Name the flag's category", "Metadata", doc=
 % USING_METADATA_HELP_REF))

        group.append("feature_name",
                     cps.Text(
                             "Name the flag", "QCFlag", doc=
))

        group.append("combination_choice",
                     cps.Choice(
                             "How should measurements be linked?",
                             [C_ANY, C_ALL], doc=
 % globals()))

        group.append("wants_skip",
                     cps.Binary(
                             "Skip image set if flagged?", False, doc=
 % globals()))

        group.append("add_measurement_button",
                     cps.DoSomething("",
                                     "Add another measurement",
                                     self.add_measurement, group))
        self.add_measurement(group, False if not can_delete else True)
        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this flag", self.flags, group))
        group.append("divider2", cps.Divider(line=True))
        self.flags.append(group)

    def add_measurement(self, flag_settings, can_delete=True):
        measurement_settings = flag_settings.measurement_settings

        group = cps.SettingsGroup()
        group.append("divider1", cps.Divider(line=False))
        group.append("source_choice",
                     cps.Choice(
                             "Flag is based on", S_ALL, doc=
 % globals()))

        group.append("object_name",
                     cps.ObjectNameSubscriber(
                             "Select the object to be used for flagging",
                             cps.NONE, doc=
))

        def object_fn():
            if group.source_choice == S_IMAGE:
                return cpmeas.IMAGE
            return group.object_name.value

        group.append("rules_directory",
                     cps.DirectoryPath(
                             "Rules file location", doc=
 % globals()))

        def get_directory_fn():
            

            return group.rules_directory.get_absolute_path()

        def set_directory_fn(path):
            dir_choice, custom_path = group.rules_directory.get_parts_from_path(path)
            group.rules_directory.join_parts(dir_choice, custom_path)

        group.append("rules_file_name",
                     cps.FilenameText(
                             "Rules file name", "rules.txt",
                             get_directory_fn=get_directory_fn,
                             set_directory_fn=set_directory_fn, doc=
 % globals()))

        def get_rules_class_choices(group=group):
            

            try:
                if group.source_choice == S_CLASSIFIER:
                    return self.get_bin_labels(group)
                elif group.source_choice == S_RULES:
                    rules = self.get_rules(group)
                    nclasses = len(rules.rules[0].weights[0])
                    return [str(i) for i in range(1, nclasses+1)]
                else:
                    return ["None"]
                rules = self.get_rules(group)
                nclasses = len(rules.rules[0].weights[0])
                return [str(i) for i in range(1, nclasses + 1)]
            except:
                return [str(i) for i in range(1, 3)]

        group.append("rules_class",
                     cps.MultiChoice(
                             "Class number",
                             choices=["1", "2"], doc=
 % globals()))

        group.rules_class.get_choices = get_rules_class_choices

        group.append("measurement",
                     cps.Measurement("Which measurement?", object_fn))

        group.append("wants_minimum",
                     cps.Binary(
                             "Flag images based on low values?", True, doc=
 % globals()))

        group.append("minimum_value", cps.Float("Minimum value", 0))

        group.append("wants_maximum",
                     cps.Binary(
                             "Flag images based on high values?", True, doc=
 % globals()))

        group.append("maximum_value", cps.Float("Maximum value", 1))

        if can_delete:
            group.append("remover", cps.RemoveSettingButton("", "Remove this measurement", measurement_settings, group))

        group.append("divider2", cps.Divider(line=True))
        measurement_settings.append(group)

    def settings(self):
        result = [self.flag_count]
        for flag in self.flags:
            result += [flag.measurement_count, flag.category, flag.feature_name,
                       flag.combination_choice, flag.wants_skip]
            for mg in flag.measurement_settings:
                result += [mg.source_choice, mg.object_name, mg.measurement,
                           mg.wants_minimum, mg.minimum_value,
                           mg.wants_maximum, mg.maximum_value,
                           mg.rules_directory, mg.rules_file_name,
                           mg.rules_class]
        return result

    def prepare_settings(self, setting_values):
        

        flag_count = int(setting_values[0])
        del self.flags[:]
        self.add_flag(can_delete=False)
        while len(self.flags) < flag_count:
            self.add_flag()

        setting_values = setting_values[N_FIXED_SETTINGS:]
        for flag in self.flags:
            count = int(setting_values[0])
            while len(flag.measurement_settings) < count:
                self.add_measurement(flag, can_delete=True)
            setting_values = setting_values[N_FIXED_SETTINGS_PER_FLAG +
                                            count * N_SETTINGS_PER_MEASUREMENT:]

    def visible_settings(self):
        def measurement_visibles(m_g):
            if hasattr(m_g, "remover"):
                result = [cps.Divider(line=True)]
            else:
                result = []
            result += [m_g.source_choice]

            if m_g.source_choice == S_ALL_OBJECTS or m_g.source_choice == S_AVERAGE_OBJECT:
                result += [m_g.object_name]
            if m_g.source_choice == S_RULES or\
               m_g.source_choice == S_CLASSIFIER:
                result += [m_g.rules_directory, m_g.rules_file_name,
                           m_g.rules_class]
                whatami = "Rules" if m_g.source_choice == S_RULES \
                    else "Classifier"
                for setting, s in ((m_g.rules_directory, "%s file location"),
                                   (m_g.rules_file_name, "%s file name")):
                    setting.text = s % whatami
            else:
                result += [m_g.measurement, m_g.wants_minimum]
                if m_g.wants_minimum.value:
                    result += [m_g.minimum_value]
                result += [m_g.wants_maximum]
                if m_g.wants_maximum.value:
                    result += [m_g.maximum_value]
            if hasattr(m_g, "remover"):
                result += [m_g.remover, cps.Divider(line=True)]
            return result

        def flag_visibles(flag):
            if hasattr(flag, "remover"):
                result = [cps.Divider(line=True), cps.Divider(line=True)]
            else:
                result = []
            result += [flag.category, flag.feature_name, flag.wants_skip]
            if len(flag.measurement_settings) > 1:
                result += [flag.combination_choice]
            for measurement_settings in flag.measurement_settings:
                result += measurement_visibles(measurement_settings)
            result += [flag.add_measurement_button]
            if hasattr(flag, "remover"):
                result += [flag.remover, cps.Divider(line=True), cps.Divider(line=True)]
            return result

        result = []
        for flag in self.flags:
            result += flag_visibles(flag)

        result += [self.add_flag_button]
        return result

    def validate_module(self, pipeline):
        

        for flag in self.flags:
            for measurement_setting in flag.measurement_settings:
                if measurement_setting.source_choice == S_RULES:
                    try:
                        rules = self.get_rules(measurement_setting)
                    except Exception, instance:
                        logger.warning("Failed to load rules: %s", str(instance), exc_info=True)
                        raise cps.ValidationError(str(instance),
                                                  measurement_setting.rules_file_name)
                    if not np.all([r.object_name == cpmeas.IMAGE for r in rules.rules]):
                        raise cps.ValidationError(
                                "The rules listed in %s describe objects instead of images." % measurement_setting.rules_file_name.value,
                                measurement_setting.rules_file_name)
                    rule_features = [r.feature for r in rules.rules]
                    measurement_cols = [c[1] for c in pipeline.get_measurement_columns(self)]
                    undef_features = list(set(rule_features).difference(measurement_cols))
                    if len(undef_features) > 0:
                        raise cps.ValidationError("The rule described by %s has not been measured earlier in the pipeline."%undef_features[0],
                                                    measurement_setting.rules_file_name)
                elif measurement_setting.source_choice == S_CLASSIFIER:
                    try:
                        self.get_classifier(measurement_setting)
                        self.get_classifier_features(measurement_setting)
                        self.get_bin_labels(measurement_setting)
                    except IOError:
                        raise cps.ValidationError(
                            "Failed to load classifier file %s" % 
                            measurement_setting.rules_file_name.value, 
                            measurement_setting.rules_file_name)
                    except:
                        raise cps.ValidationError(
                        "Unable to load %s as a classifier file" %
                        measurement_setting.rules_file_name.value, 
                        measurement_setting.rules_file_name)

    def prepare_to_create_batch(self, workspace, fn_alter_path):
        for flag_settings in self.flags:
            for group in flag_settings.measurement_settings:
                group.rules_directory.alter_for_create_batch_files(
                        fn_alter_path)

    def run(self, workspace):
        col_labels = ("Flag", "Source", "Measurement", "Value", "Pass/Fail")
        statistics = []
        for flag in self.flags:
            statistics += self.run_flag(workspace, flag)
        if self.show_window:
            workspace.display_data.statistics = statistics
            workspace.display_data.col_labels = col_labels

    def display(self, workspace, figure):
        figure.set_subplots((1, 1))
        figure.subplot_table(0, 0, workspace.display_data.statistics,
                             col_labels=workspace.display_data.col_labels)

    def run_as_data_tool(self, workspace):
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.is_first_image = True
        image_set_count = m.image_set_count
        for i in range(image_set_count):
            self.run(workspace)
            img_stats = workspace.display_data.statistics
            if i == 0:
                header = ["Image set"]
                for flag_name, object_name, feature, value, pf in img_stats:
                    header.append(flag_name)
                header.append("Pass/Fail")
                statistics = [header]
            row = [str(i + 1)]
            ok = True
            for flag_name, object_name, feature, value, pf in img_stats:
                ok = ok and (pf == "Pass")
                row.append(str(value))
            row.append("Pass" if ok else "Fail")
            statistics.append(row)
            if i < image_set_count - 1:
                m.next_image_set()
        self.show_window = False
        if image_set_count > 0:
            import wx
            from wx.grid import Grid, PyGridTableBase, EVT_GRID_LABEL_LEFT_CLICK
            from cellprofiler.gui import get_cp_icon

            frame = wx.Frame(workspace.frame, -1, "Flag image results")
            sizer = wx.BoxSizer(wx.VERTICAL)
            frame.SetSizer(sizer)
            grid = Grid(frame, -1)
            sizer.Add(grid, 1, wx.EXPAND)
            sort_order = np.arange(len(statistics) - 1)
            sort_col = [None]
            sort_ascending = [None]

            def on_label_clicked(event):
                col = event.GetCol()
                if sort_col[0] == col:
                    sort_ascending[0] = not sort_ascending[0]
                else:
                    sort_ascending[0] = True
                sort_col[0] = col
                data = [x[col] for x in statistics[1:]]
                try:
                    data = np.array(data, float)
                except ValueError:
                    data = np.array(data)
                if sort_ascending[0]:
                    sort_order[:] = np.lexsort((data,))
                else:
                    sort_order[::-1] = np.lexsort((data,))
                grid.ForceRefresh()

            grid.Bind(EVT_GRID_LABEL_LEFT_CLICK, on_label_clicked)

            class FlagTable(PyGridTableBase):
                def __init__(self):
                    PyGridTableBase.__init__(self)

                def GetColLabelValue(self, col):
                    if col == sort_col[0]:
                        if sort_ascending[0]:

                            return statistics[0][col] + " v"
                        else:
                            return statistics[0][col] + " ^"
                    return statistics[0][col]

                def GetNumberRows(self):
                    return len(statistics) - 1

                def GetNumberCols(self):
                    return len(statistics[0])

                def GetValue(self, row, col):
                    return statistics[sort_order[row] + 1][col]

            grid.SetTable(FlagTable())
            frame.Fit()
            max_size = int(wx.SystemSettings.GetMetric(wx.SYS_SCREEN_Y) * 3 / 4)
            if frame.Size[1] > max_size:
                frame.SetSize((frame.Size[0], max_size))
            frame.SetIcon(get_cp_icon())
            frame.Show()

    def measurement_name(self, flag):
        return "_".join((flag.category.value, flag.feature_name.value))

    def get_rules(self, measurement_group):
        

        rules_file = measurement_group.rules_file_name.value
        rules_directory = measurement_group.rules_directory.get_absolute_path()
        path = os.path.join(rules_directory, rules_file)
        if not os.path.isfile(path):
            raise cps.ValidationError("No such rules file: %s" % path, rules_file)
        else:
            rules = cprules.Rules()
            rules.parse(path)
            return rules

    def load_classifier(self, measurement_group):
        

        d = self.get_dictionary()
        file_ = measurement_group.rules_file_name.value
        directory_ = measurement_group.rules_directory.get_absolute_path()
        path_ = os.path.join(directory_, file_)
        if path_ not in d:
            if not os.path.isfile(path_):
                raise cps.ValidationError("No such rules file: %s" % path_, 
                                          self.rules_file_name)
            else:
                from sklearn.externals import joblib
                d[path_] = joblib.load(path_)
        return d[path_]
    
    def get_classifier(self, measurement_group):
        return self.load_classifier(measurement_group)[0]
    
    def get_bin_labels(self, measurement_group):
        return self.load_classifier(measurement_group)[1]
    
    def get_classifier_features(self, measurement_group):
        return self.load_classifier(measurement_group)[3]

    def run_flag(self, workspace, flag):
        ok, stats = self.eval_measurement(workspace,
                                          flag.measurement_settings[0])
        statistics = [tuple([self.measurement_name(flag)] + list(stats))]
        for measurement_setting in flag.measurement_settings[1:]:
            ok_1, stats = self.eval_measurement(workspace, measurement_setting)
            statistics += [tuple([self.measurement_name(flag)] + list(stats))]
            if flag.combination_choice == C_ALL:
                ok = ok or ok_1
            elif flag.combination_choice == C_ANY:
                ok = ok and ok_1
            else:
                raise NotImplementedError("Unimplemented combination choice: %s" %
                                          flag.combination_choice.value)
        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        m.add_image_measurement(self.measurement_name(flag), 0 if ok else 1)
        if (not ok) and flag.wants_skip:
            workspace.disposition = cpw.DISPOSITION_SKIP
        return statistics

    def eval_measurement(self, workspace, ms):
        

        m = workspace.measurements
        assert isinstance(m, cpmeas.Measurements)
        fail = False
        if ms.source_choice == S_IMAGE:
            value = m.get_current_image_measurement(ms.measurement.value)
            min_value = max_value = value
            display_value = str(round(value, 3))
            source = cpmeas.IMAGE
        elif ms.source_choice == S_AVERAGE_OBJECT:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = max_value = np.mean(data)
                display_value = str(round(min_value, 3))
            source = "Ave. %s" % ms.object_name.value
        elif ms.source_choice == S_ALL_OBJECTS:
            data = m.get_current_measurement(ms.object_name.value,
                                             ms.measurement.value)
            source = ms.object_name.value
            if len(data) == 0:
                min_value = max_value = np.NaN
                fail = True
                display_value = "No objects"
            else:
                min_value = np.min(data)
                max_value = np.max(data)
                if min_value == max_value:
                    display_value = str(min_value)
                else:
                    display_value = "%.3f - %.3f" % (min_value, max_value)
        elif ms.source_choice == S_RULES:
            rules = self.get_rules(ms)
            scores = rules.score(workspace.measurements)
            rules_classes = np.array(
                    [int(x) - 1 for x in ms.rules_class.get_selections()])
            is_not_nan = np.any(~ np.isnan(scores), 1)
            objclass = np.argmax(scores[is_not_nan, :], 1).flatten()
            hit_count = np.sum(
                    objclass[:, np.newaxis] == rules_classes[np.newaxis, :])
            fail = hit_count > scores.shape[0] - hit_count
            source = cpmeas.IMAGE
            if len(scores) > 1:
                display_value = "%d of %d" % (hit_count, scores.shape[0])
            else:
                display_value = "--"
        elif ms.source_choice == S_CLASSIFIER:
            classifier = self.get_classifier(ms)
            target_idxs = [
                self.get_bin_labels(ms).index(_)
                for _ in ms.rules_class.get_selections()]
            features = []
            image_features = workspace.measurements.get_feature_names(
                cpmeas.IMAGE)
            for feature_name in self.get_classifier_features(ms):
                feature_name = feature_name.split("_", 1)[1]
                features.append(feature_name)
    
            feature_vector = np.array([
                0 if feature_name not in image_features else
                workspace.measurements[cpmeas.IMAGE, feature_name] 
                for feature_name in features]).reshape(1, len(features))
            predicted_class = classifier.predict(feature_vector)[0]
            predicted_idx = \
                np.where(classifier.classes_==predicted_class)[0][0]
            fail = predicted_idx in target_idxs
            display_value = self.get_bin_labels(ms)[predicted_idx]
            source = cpmeas.IMAGE
        else:
            raise NotImplementedError("Source choice of %s not implemented" %
                                      ms.source_choice)
        is_rc = ms.source_choice in (S_RULES, S_CLASSIFIER)
        is_meas = not is_rc
        fail = (( is_meas and 
                 (fail or (ms.wants_minimum.value and 
                           min_value < ms.minimum_value.value) or
                  (ms.wants_maximum.value and
                   max_value > ms.maximum_value.value))) or
                (is_rc and fail))
        
        return ((not fail), (source, 
                             ms.measurement.value if is_meas 
                             else ms.source_choice.value, 
                             display_value, 
                             "Fail" if fail else "Pass"))

    def get_measurement_columns(self, pipeline):
        

        return [(cpmeas.IMAGE, self.measurement_name(flag), cpmeas.COLTYPE_INTEGER)
                for flag in self.flags]

    def get_categories(self, pipeline, object_name):
        if object_name == cpmeas.IMAGE:
            return [flag.category.value for flag in self.flags]
        return []

    def get_measurements(self, pipeline, object_name, category):
        if object_name != cpmeas.IMAGE:
            return []
        return [flag.feature_name.value for flag in self.flags
                if flag.category.value == category]

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and (variable_revision_number == 1 or variable_revision_number == 2):

            if variable_revision_number == 1:
                image_name, category, feature_num_or_name, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values
                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name))
            elif variable_revision_number == 2:
                image_name, category, feature_num_or_name, scale, min_value, max_value, \
                new_or_append, new_name, old_name = setting_values

                measurement_name = '_'.join((category, feature_num_or_name,
                                             image_name, scale))
            if min_value == 'No minimum':
                wants_minimum = cps.NO
                min_value = "0"
            else:
                wants_minimum = cps.YES
            if max_value == 'No maximum':
                wants_maximum = cps.NO
                max_value = "1"
            else:
                wants_maximum = cps.YES
            if new_or_append == "Append existing flag":
                logger.warning(
                        "CellProfiler 2.0 can't combine flags from multiple FlagImageForQC modules imported from version 1.0")

            new_name_split = new_name.find('_')
            if new_name_split == -1:
                flag_category = 'Metadata'
                flag_feature = new_name
            else:
                flag_category = new_name[:new_name_split]
                flag_feature = new_name[new_name_split + 1:]
                              flag_category,
                              flag_feature,
                              measurement_name,
                              wants_minimum,
                              min_value,
                              wants_maximum,
                              max_value]
            from_matlab = False
            variable_revision_number = 1

        if (not from_matlab) and variable_revision_number == 1:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + 4] + [cps.NO]
                meas_count = int(setting_values[idx])
                idx += 4
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    if (measurement_source.startswith("Measurement for all") or
                                measurement_source == "All objects"):
                        measurement_source = S_ALL_OBJECTS
                    elif measurement_source == "Average for objects":
                        measurement_source = S_AVERAGE_OBJECT
                    elif measurement_source == "Image":
                        measurement_source = S_IMAGE
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + 7)]
                    idx += 7
            setting_values = new_setting_values
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            new_setting_values = [setting_values[0]]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += setting_values[idx:idx + N_FIXED_SETTINGS_PER_FLAG]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    measurement_source = setting_values[idx]
                    new_setting_values += [measurement_source]
                    new_setting_values += setting_values[(idx + 1):(idx + N_SETTINGS_PER_MEASUREMENT_V2)] + \
                                          [cps.DirectoryPath.static_join_string(cps.DEFAULT_INPUT_FOLDER_NAME,
                                                                                cps.NONE), "rules.txt"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V2
            setting_values = new_setting_values

            variable_revision_number = 3

        if (not from_matlab) and variable_revision_number == 3:
            new_setting_values = setting_values[:1]
            idx = 1
            for flag_idx in range(int(setting_values[0])):
                new_setting_values += \
                    setting_values[idx:(idx + N_FIXED_SETTINGS_PER_FLAG)]
                meas_count = int(setting_values[idx])
                idx += N_FIXED_SETTINGS_PER_FLAG
                for meas_idx in range(meas_count):
                    new_setting_values += \
                        setting_values[idx:(idx + N_SETTINGS_PER_MEASUREMENT_V3)]
                    new_setting_values += ["1"]
                    idx += N_SETTINGS_PER_MEASUREMENT_V3
            setting_values = new_setting_values
            variable_revision_number = 4

        return setting_values, variable_revision_number, from_matlab
import logging





OXA_PATH="/opt/ofelia/oxa/"




OXA_LOG="/opt/ofelia/oxa/log/"

LOG_LEVEL="WARNING"



XMLRPC_SERVER_LISTEN_PORT=9229


OXA_DEFAULT_SWAP_SIZE_MB=512



OXA_FILEHD_USE_CACHE=True



OXA_FILEHD_CACHE_VMS="/opt/ofelia/oxa/cache/vms/"



OXA_FILEHD_REMOTE_VMS="/opt/ofelia/oxa/remote/vms/"



OXA_FILEHD_CACHE_TEMPLATES="/opt/ofelia/oxa/cache/templates/"



OXA_FILEHD_REMOTE_TEMPLATES="/opt/ofelia/oxa/remote/templates/"



OXA_FILEHD_CREATE_SPARSE_DISK=False



OXA_FILEHD_NICE_PRIORITY=15



OXA_FILEHD_IONICE_CLASS=2


OXA_FILEHD_IONICE_PRIORITY=5


OXA_FILEHD_DD_BS_KB=32



OXA_XEN_SERVER_KERNEL="/boot/vmlinuz-2.6.32-5-xen-amd64"
OXA_XEN_SERVER_INITRD="/boot/initrd.img-2.6.32-5-xen-amd64"



OXA_DEBIAN_INTERFACES_FILE_LOCATION = "/etc/network/interfaces"
OXA_DEBIAN_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_DEBIAN_HOSTNAME_FILE_LOCATION="/etc/hostname"
OXA_DEBIAN_SECURITY_ACCESS_FILE_LOCATION="/etc/security/access.conf"



OXA_REDHAT_INTERFACES_FILE_LOCATION = "/etc/sysconfig/network-scripts/"
OXA_REDHAT_UDEV_FILE_LOCATION = "/etc/udev/rules.d/70-persistent-net.rules"
OXA_REDHAT_HOSTNAME_FILE_LOCATION="/etc/hostname"


import webbrowser

from jirafs.plugin import CommandPlugin


class Command(CommandPlugin):
    

    MIN_VERSION = '1.15'
    MAX_VERSION = '1.99.99'

    def main(self, folder, *args, **kwargs):
        return webbrowser.open(folder.cached_issue.permalink())

from test import *

from swgpy.object import *	

def create(kernel):
	result = Tangible()

	result.template = "object/tangible/ship/components/armor/shared_arm_mandal_double_plated_composite_j7.iff"
	result.attribute_template_id = 8
	result.stfName("space/space_item","arm_mandal_double_plated_composite_j7_n")		
	
	
	return resultimport os

def random_bytes(n):
    


    return os.urandom(n)

import json
import logging

from urbanairship import common

logger = logging.getLogger('urbanairship')


class ChannelUninstall(object):
    _airship = None
    url = common.CHANNEL_URL + 'uninstall/'

    def __init__(self, airship):
        self._airship = airship

    def uninstall(self, channels):
        chan_num = len(channels)

        if chan_num > 200:
            raise ValueError(
                ('Maximum of 200 channel uninstalls exceeded. '
                 '({0} channels)').format(chan_num)
            )

        body = json.dumps(channels)
        response = self._airship._request('POST', body, self.url, version=3)
        logger.info('Successfully uninstalled {0} channels'.format(chan_num))
        return response



__revision__ = "src/engine/SCons/Tool/packaging/__init__.py 3897 2009/01/13 06:45:54 scons"

import SCons.Environment
from SCons.Variables import *
from SCons.Errors import *
from SCons.Util import is_List, make_path_relative
from SCons.Warnings import warn, Warning

import os, imp
import SCons.Defaults

__all__ = [ 'src_targz', 'src_tarbz2', 'src_zip', 'tarbz2', 'targz', 'zip', 'rpm', 'msi', 'ipk' ]

def Tag(env, target, source, *more_tags, **kw_tags):
    

    if not target:
        target=source
        first_tag=None
    else:
        first_tag=source

    if first_tag:
        kw_tags[first_tag[0]] = ''

    if len(kw_tags) == 0 and len(more_tags) == 0:
        raise UserError, "No tags given."

    for x in more_tags:
        kw_tags[x] = ''

    if not SCons.Util.is_List(target):
        target=[target]
    else:
        target=env.Flatten(target)

    for t in target:
        for (k,v) in kw_tags.items():
            if k[:10] != 'PACKAGING_':
                k='PACKAGING_'+k
            setattr(t, k, v)

def Package(env, target=None, source=None, **kw):
    

    if not source:
        source = env.FindInstalledFiles()

    if len(source)==0:
        raise UserError, "No source for Package() given"

    try: kw['PACKAGETYPE']=env['PACKAGETYPE']
    except KeyError: pass

    if not kw.get('PACKAGETYPE'):
        from SCons.Script import GetOption
        kw['PACKAGETYPE'] = GetOption('package_type')

    if kw['PACKAGETYPE'] == None:
        if env['BUILDERS'].has_key('Tar'):
            kw['PACKAGETYPE']='targz'
        elif env['BUILDERS'].has_key('Zip'):
            kw['PACKAGETYPE']='zip'
        else:
            raise UserError, "No type for Package() given"

    PACKAGETYPE=kw['PACKAGETYPE']
    if not is_List(PACKAGETYPE):
        PACKAGETYPE=string.split(PACKAGETYPE, ',')

    def load_packager(type):
        try:
            file,path,desc=imp.find_module(type, __path__)
            return imp.load_module(type, file, path, desc)
        except ImportError, e:
            raise EnvironmentError("packager %s not available: %s"%(type,str(e)))

    packagers=map(load_packager, PACKAGETYPE)

    try:
        if not target: target = []

        size_diff      = len(PACKAGETYPE)-len(target)
        default_name   = "%(NAME)s-%(VERSION)s"

        if size_diff>0:
            default_target = default_name%kw
            target.extend( [default_target]*size_diff )

        if not kw.has_key('PACKAGEROOT'):
            kw['PACKAGEROOT'] = default_name%kw

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s'"%e.args[0] )

    source=env.arg2nodes(source, env.fs.Entry)

    targets=[]
    try:
        for packager in packagers:
            t=[target.pop(0)]
            t=apply(packager.package, [env,t,source], kw)
            targets.extend(t)

        assert( len(target) == 0 )

    except KeyError, e:
        raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                      % (e.args[0],packager.__name__) )
    except TypeError, e:
        from inspect import getargspec
        args,varargs,varkw,defaults=getargspec(packager.package)
        if defaults!=None:
        map(args.remove, 'env target source'.split())
        args=filter(lambda x, kw=kw: not kw.has_key(x), args)

        if len(args)==0:
        elif len(args)==1:
            raise SCons.Errors.UserError( "Missing Packagetag '%s' for %s packager"\
                                          % (args[0],packager.__name__) )
        else:
            raise SCons.Errors.UserError( "Missing Packagetags '%s' for %s packager"\
                                          % (", ".join(args),packager.__name__) )

    target=env.arg2nodes(target, env.fs.Entry)
    targets.extend(env.Alias( 'package', targets ))
    return targets


added = None

def generate(env):
    from SCons.Script import AddOption
    global added
    if not added:
        added = 1
        AddOption('--package-type',
                  dest='package_type',
                  default=None,
                  type="string",
                  action="store",
                  help='The type of package to create.')

    try:
        env['BUILDERS']['Package']
        env['BUILDERS']['Tag']
    except KeyError:
        env['BUILDERS']['Package'] = Package
        env['BUILDERS']['Tag'] = Tag

def exists(env):
    return 1

def options(opts):
    opts.AddVariables(
        EnumVariable( 'PACKAGETYPE',
                     'the type of package to create.',
                     None, allowed_values=map( str, __all__ ),
                     ignorecase=2
                  )
    )


def copy_attr(f1, f2):
    

    copyit = lambda x, f2=f2: not hasattr(f2, x) and x[:10] == 'PACKAGING_'
    pattrs = filter(copyit, dir(f1))
    for attr in pattrs:
        setattr(f2, attr, getattr(f1, attr))
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    

    if SCons.Util.is_String(pkgroot):  pkgroot=env.Dir(pkgroot)
    if not SCons.Util.is_List(source): source=[source]

    new_source = []
    for file in source:
        if SCons.Util.is_String(file): file = env.File(file)

        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if hasattr(file, 'PACKAGING_INSTALL_LOCATION') and\
                       honor_install_location:
                new_name=make_path_relative(file.PACKAGING_INSTALL_LOCATION)
            else:
                new_name=make_path_relative(file.get_path())

            new_file=pkgroot.File(new_name)
            new_file=env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)

    return (target, new_source)

def stripinstallbuilder(target, source, env):
    

    def has_no_install_location(file):
        return not (file.has_builder() and\
            hasattr(file.builder, 'name') and\
            (file.builder.name=="InstallBuilder" or\
             file.builder.name=="InstallAsBuilder"))

    if len(filter(has_no_install_location, source)):
        warn(Warning, "there are files to package which have no\
        InstallBuilder attached, this might lead to irreproducible packages")

    n_source=[]
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                setattr(ss, 'PACKAGING_INSTALL_LOCATION', s.get_path())

    return (target, n_source)



from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools


class Vector:
    typecode = 'd'

    def __init__(self, components):
        self._components = array(self.typecode, components)

    def __iter__(self):
        return iter(self._components)

    def __repr__(self):
        components = reprlib.repr(self._components)
        components = components[components.find('['):-1]
        return 'Vector({})'.format(components)

    def __str__(self):
        return str(tuple(self))

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) +
                bytes(self._components))

    def __eq__(self, other):
        return (len(self) == len(other) and
                all(a == b for a, b in zip(self, other)))

    def __hash__(self):
        hashes = (hash(x) for x in self)
        return functools.reduce(operator.xor, hashes, 0)

    def __abs__(self):
        return math.sqrt(sum(x * x for x in self))

    def __neg__(self):

    def __pos__(self):

    def __bool__(self):
        return bool(abs(self))

    def __len__(self):
        return len(self._components)

    def __getitem__(self, index):
        cls = type(self)
        if isinstance(index, slice):
            return cls(self._components[index])
        elif isinstance(index, numbers.Integral):
            return self._components[index]
        else:
            msg = '{.__name__} indices must be integers'
            raise TypeError(msg.format(cls))

    shortcut_names = 'xyzt'

    def __getattr__(self, name):
        cls = type(self)
        if len(name) == 1:
            pos = cls.shortcut_names.find(name)
            if 0 <= pos < len(self._components):
                return self._components[pos]
        msg = '{.__name__!r} object has no attribute {!r}'
        raise AttributeError(msg.format(cls, name))

    def angle(self, n):
        r = math.sqrt(sum(x * x for x in self[n:]))
        a = math.atan2(r, self[n-1])
        if (n == len(self) - 1) and (self[-1] < 0):
            return math.pi * 2 - a
        else:
            return a

    def angles(self):
        return (self.angle(n) for n in range(1, len(self)))

    def __format__(self, fmt_spec=''):
            fmt_spec = fmt_spec[:-1]
            coords = itertools.chain([abs(self)],
                                     self.angles())
            outer_fmt = '<{}>'
        else:
            coords = self
            outer_fmt = '({})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(', '.join(components))

    @classmethod
    def frombytes(cls, octets):
        typecode = chr(octets[0])
        memv = memoryview(octets[1:]).cast(typecode)
        return cls(memv)

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return Vector(a + b for a, b in pairs)
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other





from google.appengine.api import yaml_errors
import yaml


_EVENT_METHOD_MAP = {
  yaml.events.StreamStartEvent: 'StreamStart',
  yaml.events.StreamEndEvent: 'StreamEnd',
  yaml.events.DocumentStartEvent: 'DocumentStart',
  yaml.events.DocumentEndEvent: 'DocumentEnd',
  yaml.events.AliasEvent: 'Alias',
  yaml.events.ScalarEvent: 'Scalar',
  yaml.events.SequenceStartEvent: 'SequenceStart',
  yaml.events.SequenceEndEvent: 'SequenceEnd',
  yaml.events.MappingStartEvent: 'MappingStart',
  yaml.events.MappingEndEvent: 'MappingEnd',
}


class EventHandler(object):
  

  def StreamStart(self, event, loader):
    


  def StreamEnd(self, event, loader):
    


  def DocumentStart(self, event, loader):
    


  def DocumentEnd(self, event, loader):
    


  def Alias(self, event, loader):
    


  def Scalar(self, event, loader):
    


  def SequenceStart(self, event, loader):
    


  def SequenceEnd(self, event, loader):
    


  def MappingStart(self, event, loader):
    


  def MappingEnd(self, event, loader):
    



class EventListener(object):
  

        key1: value1
        ---
        key2: value2
        


  def __init__(self, event_handler):
    

    if not isinstance(event_handler, EventHandler):
      raise yaml_errors.ListenerConfigurationError(
        'Must provide event handler of type yaml_listener.EventHandler')
    self._event_method_map = {}
    for event, method in _EVENT_METHOD_MAP.iteritems():
      self._event_method_map[event] = getattr(event_handler, method)

  def HandleEvent(self, event, loader=None):
    

    if event.__class__ not in _EVENT_METHOD_MAP:
      raise yaml_errors.IllegalEvent(
            "%s is not a valid PyYAML class" % event.__class__.__name__)
    if event.__class__ in self._event_method_map:
      self._event_method_map[event.__class__](event, loader)

  def _HandleEvents(self, events):
    

    for event in events:
      try:
        self.HandleEvent(*event)
      except Exception, e:
        event_object, loader = event
        raise yaml_errors.EventError(e, event_object)

  def _GenerateEventParameters(self,
                               stream,
                               loader_class=yaml.loader.SafeLoader):
    

    assert loader_class is not None
    try:
      loader = loader_class(stream)
      while loader.check_event():
        yield (loader.get_event(), loader)
    except yaml.error.YAMLError, e:
      raise yaml_errors.EventListenerYAMLError(e)

  def Parse(self, stream, loader_class=yaml.loader.SafeLoader):
    

    self._HandleEvents(self._GenerateEventParameters(stream, loader_class))
from __future__ import unicode_literals
from ..requests import UserEmailResponse
from ..requests import UserEmailListResponse

class UsersEmails:
  def __init__(self, client):
    self.client = client

  def list(self):
    return self.client.get(
      'user/emails', msg_type=UserEmailListResponse)

  def add(self, emails):
    return self.client.post(
      'user/emails', data=emails, msg_type=UserEmailResponse)

  def delete(self, emails):
    return self.client.delete(
      'user/emails', data=emails)
from __future__ import absolute_import, division, print_function, unicode_literals

from gratipay.elsewhere import PlatformOAuth2
from gratipay.elsewhere._extractors import key
from gratipay.elsewhere._paginators import header_links_paginator


class GitHub(PlatformOAuth2):

    name = 'github'
    display_name = 'GitHub'
    account_url = 'https://github.com/{user_name}'
    allows_team_connect = True

    auth_url = 'https://github.com/login/oauth/authorize'
    access_token_url = 'https://github.com/login/oauth/access_token'
    oauth_email_scope = 'user:email'
    oauth_default_scope = ['read:org']

    api_format = 'json'
    api_paginator = header_links_paginator()
    api_url = 'https://api.github.com'
    api_user_info_path = '/user/{user_id}'
    api_user_name_info_path = '/users/{user_name}'
    api_user_self_info_path = '/user'
    api_team_members_path = '/orgs/{user_name}/public_members'
    api_friends_path = '/users/{user_name}/following'
    ratelimit_headers_prefix = 'x-ratelimit-'

    x_user_id = key('id')
    x_user_name = key('login')
    x_display_name = key('name')
    x_email = key('email')
    x_gravatar_id = key('gravatar_id')
    x_avatar_url = key('avatar_url')
    x_is_team = key('type', clean=lambda t: t.lower() == 'organization')

    def is_team_admin(self, team_name, sess):
        user_teams = self.api_parser(self.api_get('/user/teams', sess=sess))
        return any(team.get('organization', {}).get('login') == team_name and
                   team.get('permission') == 'admin'
                   for team in user_teams)





from __future__ import unicode_literals

import datetime
import os
import subprocess

from sellmo.utils.lru_cache import lru_cache


def get_version(version=None):
    "Returns a PEP 386-compliant version number from VERSION."
    version = get_complete_version(version)


    main = get_main_version(version)

    sub = ''
    if version[3] == 'alpha' and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = '.dev%s' % git_changeset

    elif version[3] != 'final':
        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}
        sub = mapping[version[3]] + str(version[4])

    return str(main + sub)


def get_main_version(version=None):
    "Returns main version (X.Y[.Z]) from VERSION."
    version = get_complete_version(version)
    parts = 2 if version[2] == 0 else 3
    return '.'.join(str(x) for x in version[:parts])


def get_complete_version(version=None):
    

    if version is None:
        from sellmo import VERSION as version
    else:
        assert len(version) == 5
        assert version[3] in ('alpha', 'beta', 'rc', 'final')

    return version


def get_docs_version(version=None):
    version = get_complete_version(version)
    if version[3] != 'final':
        return 'dev'
    else:
        return '%d.%d' % version[:2]


@lru_cache()
def get_git_changeset():
    

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(
        'git log --pretty=format:%ct --quiet -1 HEAD',
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True,
        cwd=repo_dir,
        universal_newlines=True
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
    except ValueError:
        return None
    return timestamp.strftime('%Y%m%d%H%M%S')
import time
import ztq_core
import transaction
from ztq_demo.tasks import send

ztq_core.setup_redis('default','localhost', 6379, 3)

ztq_core.enable_transaction(True)

send('transaction send 1')
send('transaction send 2')

send('no transaction msg show first', ztq_transaction=False)

print 'send, waitting for commit'
time.sleep(5)

transaction.commit()
print 'committed'




import sys
from hachoir_core.stream import StringInputStream
from hachoir_subfile.search import SearchSubfile
from collections import OrderedDict

def EXTRACT_EMBEDDED(s, buff):

   EXTRACT_FILES = {}
   CHILD_BUFF = {}
   
   stream = StringInputStream(buff)
   subfile = SearchSubfile(stream)
   subfile.loadParsers(categories=None, parser_ids=None)

   subfile.stats = {}
   subfile.next_offset = None
   counter = 0

   last_start = 0
   last_end = 0

   while subfile.current_offset < subfile.size:
      subfile.datarate.update(subfile.current_offset)
      for offset, parser in subfile.findMagic(subfile.current_offset):
         if offset != 0 and parser.content_size != subfile.size \
         and parser.content_size < subfile.size and parser.content_size:
            start = offset//8
            end = start + parser.content_size//8
            if start >= last_end:
               EXTRACT_FILES['Object_%s' % counter] = OrderedDict([('Start', '%s bytes' % start),
                                                                   ('End', '%s bytes' % end),
                                                                   ('Description', parser.description),
                                                                   ('Buffer',  buff[start:end])])
               counter += 1
               last_start = start
               last_end = end

      subfile.current_offset += subfile.slice_size
      if subfile.next_offset:
         subfile.current_offset = max(subfile.current_offset, subfile.next_offset)
      subfile.current_offset = min(subfile.current_offset, subfile.size)

   return EXTRACT_FILES

if __name__ == '__main__':
   print EXTRACT_EMBEDDED(None, sys.stdin.read())
import unittest

import mock

from djangorest_alchemy.apibuilder import APIModelBuilder


class TestAPIBuilder(unittest.TestCase):

    def test_urls(self):
        


        class Model(object):
            pass

        class Model2(object):
            pass

        class SessionMixin(object):
            def __init__(self):
                self.session = mock.Mock()

        builder = APIModelBuilder([Model, Model2], SessionMixin)
        self.assertIsNotNone(builder.urls)


import socket
import warnings

from collections import deque
from copy import copy
from Queue import Queue, Empty as QueueEmpty

from amqplib.client_0_8.connection import AMQPConnectionException
from carrot.backends import get_backend_cls
from carrot.utils import retry_over_time

SETTING_PREFIX = "BROKER"
COMPAT_SETTING_PREFIX = "AMQP"
ARG_TO_DJANGO_SETTING = {
        "hostname": "HOST",
        "userid": "USER",
        "password": "PASSWORD",
        "virtual_host": "VHOST",
        "port": "PORT",
}
SETTING_DEPRECATED_FMT = "Setting %s has been renamed to %s and is " \
                         "scheduled for removal in version 1.0."


class BrokerConnection(object):
    

    virtual_host = "/"
    port = None
    insist = False
    connect_timeout = DEFAULT_CONNECT_TIMEOUT
    ssl = False
    _closed = True
    backend_cls = None

    ConnectionException = AMQPConnectionException

    @property
    def host(self):
        

        return ":".join([self.hostname, str(self.port)])

    def __init__(self, hostname=None, userid=None, password=None,
            virtual_host=None, port=None, pool=None, **kwargs):
        self.hostname = hostname
        self.userid = userid
        self.password = password
        self.virtual_host = virtual_host or self.virtual_host
        self.port = port or self.port
        self.insist = kwargs.get("insist", self.insist)
        self.pool = pool
        self.connect_timeout = kwargs.get("connect_timeout",
                                          self.connect_timeout)
        self.ssl = kwargs.get("ssl", self.ssl)
        self.backend_cls = (kwargs.get("backend_cls") or
                                kwargs.get("transport"))
        self._closed = None
        self._connection = None

    def __copy__(self):
        return self.__class__(self.hostname, self.userid, self.password,
                              self.virtual_host, self.port,
                              insist=self.insist,
                              connect_timeout=self.connect_timeout,
                              ssl=self.ssl,
                              backend_cls=self.backend_cls,
                              pool=self.pool)

    @property
    def connection(self):
        if self._closed == True:
            return
        if not self._connection:
            self._connection = self._establish_connection()
            self._closed = False
        return self._connection

    def __enter__(self):
        return self

    def __exit__(self, e_type, e_value, e_trace):
        if e_type:
            raise e_type(e_value)
        self.close()

    def _establish_connection(self):
        return self.create_backend().establish_connection()

    def get_backend_cls(self):
        

        backend_cls = self.backend_cls
        if not backend_cls or isinstance(backend_cls, basestring):
            backend_cls = get_backend_cls(backend_cls)
        return backend_cls

    def create_backend(self):
        

        backend_cls = self.get_backend_cls()
        return backend_cls(connection=self)

    def channel(self):
        

        return self.create_backend()

    def get_channel(self):
        

        return self.connection.channel()

    def connect(self):
        

        self._closed = False
        return self.connection

    def drain_events(self, **kwargs):
        return self.connection.drain_events(**kwargs)

    def ensure_connection(self, errback=None, max_retries=None,
            interval_start=2, interval_step=2, interval_max=30):
        

        retry_over_time(self.connect, self.connection_errors, (), {},
                        errback, max_retries,
                        interval_start, interval_step, interval_max)
        return self

    def close(self):
        

        try:
            if self._connection:
                backend = self.create_backend()
                backend.close_connection(self._connection)
        except socket.error:
            pass
        self._closed = True

    def release(self):
        if not self.pool:
            raise NotImplementedError(
                    "Trying to release connection not part of a pool")
        self.pool.release(self)

    def info(self):
        

        backend_cls = self.backend_cls or "amqplib"
        port = self.port or self.create_backend().default_port
        return {"hostname": self.hostname,
                "userid": self.userid,
                "password": self.password,
                "virtual_host": self.virtual_host,
                "port": port,
                "insist": self.insist,
                "ssl": self.ssl,
                "transport_cls": backend_cls,
                "backend_cls": backend_cls,
                "connect_timeout": self.connect_timeout}

    @property
    def connection_errors(self):
        

        return self.create_backend().connection_errors

    @property
    def channel_errors(self):
        

        return self.create_backend().channel_errors

AMQPConnection = BrokerConnection


class ConnectionLimitExceeded(Exception):
    



class ConnectionPool(object):

    def __init__(self, source_connection, min=2, max=None, preload=True):
        self.source_connection = source_connection
        self.min = min
        self.max = max
        self.preload = preload
        self.source_connection.pool = self

        self._connections = Queue()
        self._dirty = deque()

        self._connections.put(self.source_connection)
        for i in range(min - 1):
            self._connections.put_nowait(self._new_connection())

    def acquire(self, block=False, timeout=None, connect_timeout=None):
        try:
            conn = self._connections.get(block=block, timeout=timeout)
        except QueueEmpty:
            conn = self._new_connection()
        self._dirty.append(conn)
        if connect_timeout is not None:
            conn.connect_timeout = connect_timeout
        return conn

    def release(self, connection):
        self._dirty.remove(connection)
        self._connections.put_nowait(connection)

    def _new_connection(self):
        if len(self._dirty) >= self.max:
            raise ConnectionLimitExceeded(self.max)
        return copy(self.source_connection)





def get_django_conninfo(settings=None):
    ci = {}
    if settings is None:
        from django.conf import settings

    ci["backend_cls"] = getattr(settings, "CARROT_BACKEND", None)

    for arg_name, setting_name in ARG_TO_DJANGO_SETTING.items():
        setting = "%s_%s" % (SETTING_PREFIX, setting_name)
        compat_setting = "%s_%s" % (COMPAT_SETTING_PREFIX, setting_name)
        if hasattr(settings, setting):
            ci[arg_name] = getattr(settings, setting, None)
        elif hasattr(settings, compat_setting):
            ci[arg_name] = getattr(settings, compat_setting, None)
            warnings.warn(DeprecationWarning(SETTING_DEPRECATED_FMT % (
                compat_setting, setting)))

    if "hostname" not in ci:
        if hasattr(settings, "AMQP_SERVER"):
            ci["hostname"] = settings.AMQP_SERVER
            warnings.warn(DeprecationWarning(
                "AMQP_SERVER has been renamed to BROKER_HOST and is"
                "scheduled for removal in version 1.0."))

    return ci


class DjangoBrokerConnection(BrokerConnection):
    

    def __init__(self, *args, **kwargs):
        settings = kwargs.pop("settings", None)
        kwargs = dict(get_django_conninfo(settings), **kwargs)
        super(DjangoBrokerConnection, self).__init__(*args, **kwargs)

DjangoAMQPConnection = DjangoBrokerConnection
from watson.framework import views
from watson.framework.views.decorators import view


class MyController(object):
    @view(format='xml')
    def xml_action(self, *args, **kwargs):
        return {}

    @view(format='xml/text')
    def xml_full_mime_action(self):
        return {}

    @view(format='html', template='test')
    def html_action(self):
        return {}

    @view(format='json')
    def bool_action(self):
        return True


class TestViewDecorator(object):

    def test_view_model_format(self):
        controller = MyController()
        controller_response = controller.xml_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'xml'
        assert controller.xml_full_mime_action().format == 'xml/text'

    def test_view_model_template(self):
        controller = MyController()
        controller_response = controller.html_action()
        assert isinstance(controller_response, views.Model)
        assert controller_response.format == 'html'
        assert controller_response.template == 'test'

    def test_view_model_response(self):
        controller = MyController()
        controller_response = controller.bool_action()
        assert controller_response.data['content']
import numpy as np


def init_conv_w(shape):
    

    input_n = np.prod(shape[1:])
    std = 1./np.sqrt(input_n)
    return np.random.normal(0, std, shape)


def init_conv_w_kaiming(shape, gain=2.):
    

    input_n = np.prod(shape[1:])
    std = np.sqrt(gain/float(input_n))
    return np.random.normal(0., std, shape)


def init_affine_wb(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.normal(0, 3e-4, shape)
    b = np.zeros(shape[1])
    return w, b


def init_affine_wb_th(shape):
    

    std = 1./np.sqrt(shape[0])
    w = np.random.uniform(-std, std, shape)
    b = np.random.uniform(-std, std, shape[1])
    return w, b


def init_bn_w(n_ch):
    

    return np.random.normal(.6, 0.005, size=n_ch)


def init_bn_w_disp(n_ch):
    

    return np.random.normal(.7, .04, size=n_ch)


def init_bn_w_gcr(n_ch):
    

    return np.random.normal(1., 2e-3, n_ch)

import time
import threading
import pprint
import sys
import traceback
import random
import telepot
from telepot.namedtuple import InlineQuery, ChosenInlineResult, InlineQueryResultArticle, InlineQueryResultPhoto, InputTextMessageContent

def equivalent(data, nt):
    if type(data) is dict:
        keys = data.keys()

        if len(keys) != len([f for f in nt._fields if getattr(nt, f) is not None]):
            return False

        fields = list(map(lambda k: k+'_' if k in ['from'] else k, keys))

        return all(map(equivalent, [data[k] for k in keys], [getattr(nt, f) for f in fields]))
    elif type(data) is list:
        return all(map(equivalent, data, nt))
    else:
        return data==nt

def examine(result, type):
    try:
        print('Examining %s ......' % type)

        nt = type(**result)
        assert equivalent(result, nt), 'Not equivalent:::::::::::::::\n%s\n::::::::::::::::\n%s' % (result, nt)

        pprint.pprint(result)
        pprint.pprint(nt)
        print()
    except AssertionError:
        traceback.print_exc()
        answer = raw_input('Do you want to continue? [y] ')
        if answer != 'y':
            exit(1)

def on_inline_query(msg):
    def compute():
        articles = [InlineQueryResultArticle(
                       id='abc', title='HK', input_message_content=InputTextMessageContent(message_text='Hong Kong'), url='https://www.google.com', hide_url=True),
                   {'type': 'article',
                       'id': 'def', 'title': 'SZ', 'input_message_content': {'message_text': 'Shenzhen'}, 'url': 'https://www.yahoo.com'}]

        photos = [InlineQueryResultPhoto(
                      id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
                  {'type': 'photo',
                      'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'input_message_content': {'message_text': 'Shenzhen'}}]

        results = random.choice([articles, photos])
        return results

    query_id, from_id, query = telepot.glance(msg, flavor='inline_query')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, InlineQuery)
    answerer.answer(msg, compute)


def on_chosen_inline_result(msg):
    result_id, from_id, query = telepot.glance(msg, flavor='chosen_inline_result')

    if from_id != USER_ID:
        print('Unauthorized user:', from_id)
        return

    examine(msg, ChosenInlineResult)

    print('Chosen inline query:')
    pprint.pprint(msg)


def compute(inline_query):
    articles = [InlineQueryResultArticle(
                   id='abc', title='HK', message_text='Hong Kong', url='https://www.google.com', hide_url=True),
               {'type': 'article',
                   'id': 'def', 'title': 'SZ', 'message_text': 'Shenzhen', 'url': 'https://www.yahoo.com'}]

    photos = [InlineQueryResultPhoto(
                  id='123', photo_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf', thumb_url='https://core.telegram.org/file/811140934/1/tbDSLHSaijc/fdcc7b6d5fb3354adf'),
              {'type': 'photo',
                  'id': '345', 'photo_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'thumb_url': 'https://core.telegram.org/file/811140184/1/5YJxx-rostA/ad3f74094485fb97bd', 'caption': 'Caption', 'title': 'Title', 'message_text': 'Message Text'}]

    results = random.choice([articles, photos])
    return results


TOKEN = sys.argv[1]
USER_ID = int(sys.argv[2])

bot = telepot.Bot(TOKEN)
answerer = telepot.helper.Answerer(bot)

bot.sendMessage(USER_ID, 'Please give me an inline query.')

bot.message_loop({'inline_query': on_inline_query,
                     'chosen_inline_result': on_chosen_inline_result}, run_forever=True)

import mock
from oslo_serialization import jsonutils
from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova.objects import base
from nova.objects import request_spec
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit import fake_request_spec
from nova.tests.unit.objects import test_objects
from nova.tests import uuidsentinel as uuids


class _TestRequestSpecObject(object):

    def test_image_meta_from_image_as_object(self):
        image_meta = objects.ImageMeta(name='foo')

        spec = objects.RequestSpec()
        spec._image_meta_from_image(image_meta)
        self.assertEqual(image_meta, spec.image)

    @mock.patch.object(objects.ImageMeta, 'from_dict')
    def test_image_meta_from_image_as_dict(self, from_dict):
        image_meta = objects.ImageMeta(name='foo')
        from_dict.return_value = image_meta

        spec = objects.RequestSpec()
        spec._image_meta_from_image({'name': 'foo'})
        self.assertEqual(image_meta, spec.image)

    def test_image_meta_from_image_as_none(self):
        spec = objects.RequestSpec()
        spec._image_meta_from_image(None)
        self.assertIsNone(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image(self, obj_to_primitive):
        spec = objects.RequestSpec(image=objects.ImageMeta())
        fake_dict = mock.Mock()
        obj_to_primitive.return_value = fake_dict

        self.assertEqual(fake_dict, spec._to_legacy_image())
        obj_to_primitive.assert_called_once_with(spec.image)

    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_image_with_none(self, obj_to_primitive):
        spec = objects.RequestSpec(image=None)

        self.assertEqual({}, spec._to_legacy_image())
        self.assertFalse(obj_to_primitive.called)

    def test_from_instance_as_object(self):
        instance = objects.Instance()
        instance.uuid = uuidutils.generate_uuid()
        instance.numa_topology = None
        instance.pci_requests = None
        instance.project_id = fakes.FAKE_PROJECT_ID
        instance.availability_zone = 'nova'

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(getattr(instance, field),
                                 getattr(spec, field))

    def test_from_instance_as_dict(self):
        instance = dict(uuid=uuidutils.generate_uuid(),
                        numa_topology=None,
                        pci_requests=None,
                        project_id=fakes.FAKE_PROJECT_ID,
                        availability_zone='nova')

        spec = objects.RequestSpec()
        spec._from_instance(instance)
        instance_fields = ['numa_topology', 'pci_requests', 'uuid',
                           'project_id', 'availability_zone']
        for field in instance_fields:
            if field == 'uuid':
                self.assertEqual(instance.get(field),
                                 getattr(spec, 'instance_uuid'))
            else:
                self.assertEqual(instance.get(field), getattr(spec, field))

    @mock.patch.object(objects.InstancePCIRequests,
                       'from_request_spec_instance_props')
    def test_from_instance_with_pci_requests(self, pci_from_spec):
        fake_pci_requests = objects.InstancePCIRequests()
        pci_from_spec.return_value = fake_pci_requests

        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            numa_topology=None,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests={
                'instance_uuid': 'fakeid',
                'requests': [{'count': 1, 'spec': [{'vendor_id': '8086'}]}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        pci_from_spec.assert_called_once_with(instance['pci_requests'])
        self.assertEqual(fake_pci_requests, spec.pci_requests)

    def test_from_instance_with_numa_stuff(self):
        instance = dict(
            uuid=uuidutils.generate_uuid(),
            root_gb=10,
            ephemeral_gb=0,
            memory_mb=10,
            vcpus=1,
            project_id=fakes.FAKE_PROJECT_ID,
            availability_zone='nova',
            pci_requests=None,
            numa_topology={'cells': [{'id': 1, 'cpuset': ['1'], 'memory': 8192,
                                      'pagesize': None, 'cpu_topology': None,
                                      'cpu_pinning_raw': None}]})
        spec = objects.RequestSpec()

        spec._from_instance(instance)
        self.assertIsInstance(spec.numa_topology, objects.InstanceNUMATopology)
        cells = spec.numa_topology.cells
        self.assertEqual(1, len(cells))
        self.assertIsInstance(cells[0], objects.InstanceNUMACell)

    def test_from_flavor_as_object(self):
        flavor = objects.Flavor()

        spec = objects.RequestSpec()
        spec._from_flavor(flavor)
        self.assertEqual(flavor, spec.flavor)

    def test_from_flavor_as_dict(self):
        flavor_dict = dict(id=1)
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)

        spec._from_flavor(flavor_dict)
        self.assertIsInstance(spec.flavor, objects.Flavor)
        self.assertEqual({'id': 1}, spec.flavor.obj_get_changes())

    def test_to_legacy_instance(self):
        spec = objects.RequestSpec()
        spec.flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=10,
                                     vcpus=1)
        spec.numa_topology = None
        spec.pci_requests = None
        spec.project_id = fakes.FAKE_PROJECT_ID
        spec.availability_zone = 'nova'

        instance = spec._to_legacy_instance()
        self.assertEqual({'root_gb': 10,
                          'ephemeral_gb': 0,
                          'memory_mb': 10,
                          'vcpus': 1,
                          'numa_topology': None,
                          'pci_requests': None,
                          'project_id': fakes.FAKE_PROJECT_ID,
                          'availability_zone': 'nova'}, instance)

    def test_to_legacy_instance_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec._to_legacy_instance())

    def test_from_retry(self):
        retry_dict = {'num_attempts': 1,
                      'hosts': [['fake1', 'node1']]}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsInstance(spec.retry, objects.SchedulerRetries)
        self.assertEqual(1, spec.retry.num_attempts)
        self.assertIsInstance(spec.retry.hosts, objects.ComputeNodeList)
        self.assertEqual(1, len(spec.retry.hosts))
        self.assertEqual('fake1', spec.retry.hosts[0].host)
        self.assertEqual('node1', spec.retry.hosts[0].hypervisor_hostname)

    def test_from_retry_missing_values(self):
        retry_dict = {}
        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec(ctxt)
        spec._from_retry(retry_dict)
        self.assertIsNone(spec.retry)

    def test_populate_group_info(self):
        filt_props = {}
        filt_props['group_updated'] = True
        filt_props['group_policies'] = set(['affinity'])
        filt_props['group_hosts'] = set(['fake1'])
        filt_props['group_members'] = set(['fake-instance1'])

        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsInstance(spec.instance_group, objects.InstanceGroup)
        self.assertEqual(['affinity'], spec.instance_group.policies)
        self.assertEqual(['fake1'], spec.instance_group.hosts)
        self.assertEqual(['fake-instance1'], spec.instance_group.members)

    def test_populate_group_info_missing_values(self):
        filt_props = {}
        spec = objects.RequestSpec()
        spec._populate_group_info(filt_props)
        self.assertIsNone(spec.instance_group)

    def test_from_limits(self):
        limits_dict = {'numa_topology': None,
                       'vcpu': 1.0,
                       'disk_gb': 1.0,
                       'memory_mb': 1.0}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertEqual(1, spec.limits.vcpu)
        self.assertEqual(1, spec.limits.disk_gb)
        self.assertEqual(1, spec.limits.memory_mb)

    def test_from_limits_missing_values(self):
        limits_dict = {}
        spec = objects.RequestSpec()
        spec._from_limits(limits_dict)
        self.assertIsInstance(spec.limits, objects.SchedulerLimits)
        self.assertIsNone(spec.limits.numa_topology)
        self.assertIsNone(spec.limits.vcpu)
        self.assertIsNone(spec.limits.disk_gb)
        self.assertIsNone(spec.limits.memory_mb)

    def test_from_hints(self):
        hints_dict = {'foo_str': '1',
                      'bar_list': ['2']}
        spec = objects.RequestSpec()
        spec._from_hints(hints_dict)
        expected = {'foo_str': ['1'],
                    'bar_list': ['2']}
        self.assertEqual(expected, spec.scheduler_hints)

    def test_from_hints_with_no_hints(self):
        spec = objects.RequestSpec()
        spec._from_hints(None)
        self.assertIsNone(spec.scheduler_hints)

    @mock.patch.object(objects.SchedulerLimits, 'from_dict')
    def test_from_primitives(self, mock_limits):
        spec_dict = {'instance_type': objects.Flavor(),
                     'instance_properties': objects.Instance(
                         uuid=uuidutils.generate_uuid(),
                         numa_topology=None,
                         pci_requests=None,
                         project_id=1,
                         availability_zone='nova')}
        filt_props = {}

        mock_limits.return_value = None

        ctxt = context.RequestContext('fake', 'fake')
        spec = objects.RequestSpec.from_primitives(ctxt, spec_dict, filt_props)
        mock_limits.assert_called_once_with({})
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertTrue(spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    def test_from_components(self):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {}
        instance_group = None

        spec = objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)
        for field in [f for f in spec.obj_fields if f != 'id']:
            self.assertEqual(True, spec.obj_attr_is_set(field),
                             'Field: %s is not set' % field)
        self.assertEqual(ctxt, spec._context)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_with_instance_group(self, mock_pgi):
        ctxt = context.RequestContext('fake-user', 'fake-project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}
        instance_group = objects.InstanceGroup()

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, instance_group, instance.availability_zone)

        self.assertFalse(mock_pgi.called)

    @mock.patch('nova.objects.RequestSpec._populate_group_info')
    def test_from_components_without_instance_group(self, mock_pgi):
        ctxt = context.RequestContext(fakes.FAKE_USER_ID,
                                      fakes.FAKE_PROJECT_ID)
        instance = fake_instance.fake_instance_obj(ctxt)
        image = {'id': uuids.image_id, 'properties': {'mappings': []},
                 'status': 'fake-status', 'location': 'far-away'}
        flavor = fake_flavor.fake_flavor_obj(ctxt)
        filter_properties = {'fake': 'property'}

        objects.RequestSpec.from_components(ctxt, instance.uuid, image,
                flavor, instance.numa_topology, instance.pci_requests,
                filter_properties, None, instance.availability_zone)

        mock_pgi.assert_called_once_with(filter_properties)

    def test_get_scheduler_hint(self):
        spec_obj = objects.RequestSpec(scheduler_hints={'foo_single': ['1'],
                                                        'foo_mul': ['1', '2']})
        self.assertEqual('1', spec_obj.get_scheduler_hint('foo_single'))
        self.assertEqual(['1', '2'], spec_obj.get_scheduler_hint('foo_mul'))
        self.assertIsNone(spec_obj.get_scheduler_hint('oops'))
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    def test_get_scheduler_hint_with_no_hints(self):
        spec_obj = objects.RequestSpec()
        self.assertEqual('bar', spec_obj.get_scheduler_hint('oops',
                                                            default='bar'))

    @mock.patch.object(objects.RequestSpec, '_to_legacy_instance')
    @mock.patch.object(base, 'obj_to_primitive')
    def test_to_legacy_request_spec_dict(self, image_to_primitive,
                                         spec_to_legacy_instance):
        fake_image_dict = mock.Mock()
        image_to_primitive.return_value = fake_image_dict
        fake_instance = {'root_gb': 1.0,
                         'ephemeral_gb': 1.0,
                         'memory_mb': 1.0,
                         'vcpus': 1,
                         'numa_topology': None,
                         'pci_requests': None,
                         'project_id': fakes.FAKE_PROJECT_ID,
                         'availability_zone': 'nova',
                         'uuid': '1'}
        spec_to_legacy_instance.return_value = fake_instance

        fake_flavor = objects.Flavor(root_gb=10,
                                     ephemeral_gb=0,
                                     memory_mb=512,
                                     vcpus=1)
        spec = objects.RequestSpec(num_instances=1,
                                   image=objects.ImageMeta(),
                                   numa_topology=None,
                                   pci_requests=None,
                                   project_id=1,
                                   availability_zone='nova',
                                   instance_uuid=uuids.instance,
                                   flavor=fake_flavor)
        spec_dict = spec.to_legacy_request_spec_dict()
        expected = {'num_instances': 1,
                    'image': fake_image_dict,
                    'instance_properties': fake_instance,
                    'instance_type': fake_flavor}
        self.assertEqual(expected, spec_dict)

    def test_to_legacy_request_spec_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({'num_instances': 1,
                          'image': {},
                          'instance_properties': {},
                          'instance_type': {}},
                         spec.to_legacy_request_spec_dict())

    def test_to_legacy_filter_properties_dict(self):
        fake_numa_limits = objects.NUMATopologyLimits()
        fake_computes_obj = objects.ComputeNodeList(
            objects=[objects.ComputeNode(host='fake1',
                                         hypervisor_hostname='node1')])
        spec = objects.RequestSpec(
            ignore_hosts=['ignoredhost'],
            force_hosts=['fakehost'],
            force_nodes=['fakenode'],
            retry=objects.SchedulerRetries(num_attempts=1,
                                           hosts=fake_computes_obj),
            limits=objects.SchedulerLimits(numa_topology=fake_numa_limits,
                                           vcpu=1.0,
                                           disk_gb=10.0,
                                           memory_mb=8192.0),
            instance_group=objects.InstanceGroup(hosts=['fake1'],
                                                 policies=['affinity']),
            scheduler_hints={'foo': ['bar']})
        expected = {'ignore_hosts': ['ignoredhost'],
                    'force_hosts': ['fakehost'],
                    'force_nodes': ['fakenode'],
                    'retry': {'num_attempts': 1,
                              'hosts': [['fake1', 'node1']]},
                    'limits': {'numa_topology': fake_numa_limits,
                               'vcpu': 1.0,
                               'disk_gb': 10.0,
                               'memory_mb': 8192.0},
                    'group_updated': True,
                    'group_hosts': set(['fake1']),
                    'group_policies': set(['affinity']),
                    'scheduler_hints': {'foo': 'bar'}}
        self.assertEqual(expected, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_nullable_values(self):
        spec = objects.RequestSpec(force_hosts=None,
                                   force_nodes=None,
                                   retry=None,
                                   limits=None,
                                   instance_group=None,
                                   scheduler_hints=None)
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    def test_to_legacy_filter_properties_dict_with_unset_values(self):
        spec = objects.RequestSpec()
        self.assertEqual({}, spec.to_legacy_filter_properties_dict())

    @mock.patch.object(request_spec.RequestSpec,
            '_get_by_instance_uuid_from_db')
    def test_get_by_instance_uuid(self, get_by_uuid):
        fake_spec = fake_request_spec.fake_db_spec()
        get_by_uuid.return_value = fake_spec

        req_obj = request_spec.RequestSpec.get_by_instance_uuid(self.context,
                fake_spec['instance_uuid'])

        self.assertEqual(1, req_obj.num_instances)
        self.assertEqual(['host2', 'host4'], req_obj.ignore_hosts)
        self.assertEqual('fake', req_obj.project_id)
        self.assertEqual({'hint': ['over-there']}, req_obj.scheduler_hints)
        self.assertEqual(['host1', 'host3'], req_obj.force_hosts)
        self.assertIsNone(req_obj.availability_zone)
        self.assertEqual(['node1', 'node2'], req_obj.force_nodes)
        self.assertIsInstance(req_obj.image, objects.ImageMeta)
        self.assertIsInstance(req_obj.numa_topology,
                objects.InstanceNUMATopology)
        self.assertIsInstance(req_obj.pci_requests,
                objects.InstancePCIRequests)
        self.assertIsInstance(req_obj.flavor, objects.Flavor)
        self.assertIsInstance(req_obj.retry, objects.SchedulerRetries)
        self.assertIsInstance(req_obj.limits, objects.SchedulerLimits)
        self.assertIsInstance(req_obj.instance_group, objects.InstanceGroup)

    def _check_update_primitive(self, req_obj, changes):
        self.assertEqual(req_obj.instance_uuid, changes['instance_uuid'])
        serialized_obj = objects.RequestSpec.obj_from_primitive(
                jsonutils.loads(changes['spec']))

        for field in ['instance_uuid', 'num_instances', 'ignore_hosts',
                'project_id', 'scheduler_hints', 'force_hosts',
                'availability_zone', 'force_nodes']:
            self.assertEqual(getattr(req_obj, field),
                    getattr(serialized_obj, field))

        for field in ['image', 'numa_topology', 'pci_requests', 'flavor',
                'retry', 'limits', 'instance_group']:
            self.assertDictEqual(
                    getattr(req_obj, field).obj_to_primitive(),
                    getattr(serialized_obj, field).obj_to_primitive())

    def test_create(self):
        req_obj = fake_request_spec.fake_spec_obj(remove_id=True)

        def _test_create_args(self2, context, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_create_in_db',
                _test_create_args):
            req_obj.create()

    def test_create_id_set(self):
        req_obj = request_spec.RequestSpec(self.context)
        req_obj.id = 3

        self.assertRaises(exception.ObjectActionError, req_obj.create)

    def test_save(self):
        req_obj = fake_request_spec.fake_spec_obj()

        def _test_save_args(self2, context, instance_uuid, changes):
            self._check_update_primitive(req_obj, changes)
            changes['id'] = 42
            return changes

        with mock.patch.object(request_spec.RequestSpec, '_save_in_db',
                _test_save_args):
            req_obj.save()

    def test_reset_forced_destinations(self):
        req_obj = fake_request_spec.fake_spec_obj()
        self.assertIsNotNone(req_obj.force_hosts)
        self.assertIsNotNone(req_obj.force_nodes)

        with mock.patch.object(req_obj, 'obj_reset_changes') as mock_reset:
            req_obj.reset_forced_destinations()
        self.assertIsNone(req_obj.force_hosts)
        self.assertIsNone(req_obj.force_nodes)
        mock_reset.assert_called_once_with(['force_hosts', 'force_nodes'])


class TestRequestSpecObject(test_objects._LocalTest,
                            _TestRequestSpecObject):
    pass


class TestRemoteRequestSpecObject(test_objects._RemoteTest,
                                  _TestRequestSpecObject):
    pass

import re
from . import interface, exc, meta, request

def validate(obj):
    

    members = [
        'filter',
        ]
    metas = [
        'baseurl',
        'resource',
        'path',
        'request',
        ]
    interface.validate(IResource, obj, members)

class IResource(interface.Interface):
    


class ResourceHandler(meta.MetaMixin):
    

    class Meta:
        pass

    def __init__(self, api_obj, name, path, **kw):
        super(ResourceHandler, self).__init__(**kw)
        self.api = api_obj
        self.path = path
        self.name = name

    def filter(self, params):
        

        return params

class RESTResourceHandler(ResourceHandler):
    

    def __init__(self, api_obj, name, path, **kw):
        super(RESTResourceHandler, self).__init__(api_obj, name, path, **kw)

    def get(self, resource_id=None, params=None):
        

        if params is None:
        if resource_id:
            path = '/%s/%s' % (self.path, resource_id)
        else:
            path = '/%s' % self.path

        try:
            response = self.api.make_request('GET', path,
                                             params=self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def create(self, params=None):
        

        if params is None:

        return self.post(params)

    def post(self, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s' % self.path

        try:
            response = self.api.make_request('POST', path, self.filter(params))
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s)" % (e.msg, self.name)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def update(self, resource_id, params=None):
        

        if params is None:

        return self.put(resource_id, params)

    def put(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PUT', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def patch(self, resource_id, params=None):
        

        if params is None:

        params = self.filter(params)
        path = '/%s/%s' % (self.path, resource_id)

        try:
            response = self.api.make_request('PATCH', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

    def delete(self, resource_id, params=None):
        

        if params is None:
        path = '/%s/%s' % (self.path, resource_id)
        try:
            response = self.api.make_request('DELETE', path, params)
        except exc.dRestRequestError as e:
            msg = "%s (resource: %s, id: %s)" % (e.msg, self.name,
                                                 resource_id)
            raise exc.dRestRequestError(msg, e.response)

        return response

class TastyPieResourceHandler(RESTResourceHandler):
    

    class Meta:
        

        request = request.TastyPieRequestHandler
        


        collection_name = 'objects'
        


    def __init__(self, api_obj, name, path, **kw):
        super(TastyPieResourceHandler, self).__init__(api_obj, name, path, **kw)
        self._schema = None

    def get_by_uri(self, resource_uri, params=None):
        

        if params is None:

        resource_uri = resource_uri.rstrip('/')
        pk = resource_uri.split('/')[-1]
        return self.get(pk, params)

    def patch_list(self, create_objects=[], delete_objects=[]):
        

        create_objects = [self.filter(o) for o in create_objects]
        delete_objects = [self.filter(o) for o in delete_objects]
        delete_collection_name = "deleted_%s" % self._meta.collection_name
        data = {
            self._meta.collection_name: create_objects,
            delete_collection_name: delete_objects,
        }
        return self.api.make_request('PATCH', self.path, data)

    @property
    def schema(self):
        

        if not self._schema:
            response = self.api.make_request('GET', '%s/schema' % self.path)
            self._schema = response.data

        return self._schema

class NestedResource(object):
    pass

import logging
import itertools
import json
import re

from itertools import imap, izip
from operator import itemgetter

from django.utils.translation import ugettext as _

from desktop.lib import thrift_util
from desktop.conf import DEFAULT_USER
from hadoop import cluster

from TCLIService import TCLIService
from TCLIService.ttypes import TOpenSessionReq, TGetTablesReq, TFetchResultsReq,\
  TStatusCode, TGetResultSetMetadataReq, TGetColumnsReq, TTypeId,\
  TExecuteStatementReq, TGetOperationStatusReq, TFetchOrientation,\
  TCloseSessionReq, TGetSchemasReq, TGetLogReq, TCancelOperationReq,\
  TCloseOperationReq, TFetchResultsResp, TRowSet, TProtocolVersion

from beeswax import conf as beeswax_conf
from beeswax import hive_site
from beeswax.hive_site import hiveserver2_use_ssl
from beeswax.conf import CONFIG_WHITELIST, LIST_PARTITIONS_LIMIT
from beeswax.models import Session, HiveServerQueryHandle, HiveServerQueryHistory
from beeswax.server.dbms import Table, NoSuchObjectException, DataTable,\
                                QueryServerException


LOG = logging.getLogger(__name__)

IMPALA_RESULTSET_CACHE_SIZE = 'impala.resultset.cache.size'
DEFAULT_USER = DEFAULT_USER.get()


class HiveServerTable(Table):
  


  def __init__(self, table_results, table_schema, desc_results, desc_schema):
    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      if not table_results.columns:
        raise NoSuchObjectException()
      self.table = table_results.columns
      if not table_results.rows:
        raise NoSuchObjectException()
      self.table = table_results.rows and table_results.rows[0] or ''

    self.table_schema = table_schema
    self.desc_results = desc_results
    self.desc_schema = desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def name(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_NAME')

  @property
  def is_view(self):
    return HiveServerTRow(self.table, self.table_schema).col('TABLE_TYPE') == 'VIEW'

  @property
  def partition_keys(self):
    try:
      return [PartitionKeyCompatible(row['col_name'], row['data_type'], row['comment']) for row in self._get_partition_column()]
    except:
      LOG.exception('failed to get partition keys')
      return []

  @property
  def path_location(self):
    try:
      rows = self.describe
      rows = [row for row in rows if row['col_name'].startswith('Location:')]
      if rows:
        return rows[0]['data_type']
    except:
      LOG.exception('failed to get path location')
      return None

  @property
  def cols(self):
    rows = self.describe
    col_row_index = 2
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index] + self._get_partition_column()
      return rows[col_row_index:]
    except:
      return rows

  def _get_partition_column(self):
    rows = self.describe
    try:
      end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
      return rows[col_row_index:][:end_cols_index]
    except:
      return []

  @property
  def comment(self):
    return HiveServerTRow(self.table, self.table_schema).col('REMARKS')

  @property
  def properties(self):
    rows = self.describe
    col_row_index = 2
    end_cols_index = map(itemgetter('col_name'), rows[col_row_index:]).index('')
    return [{
          'col_name': prop['col_name'].strip() if prop['col_name'] else prop['col_name'],
          'data_type': prop['data_type'].strip() if prop['data_type'] else prop['data_type'],
          'comment': prop['comment'].strip() if prop['comment'] else prop['comment']
        } for prop in rows[col_row_index + end_cols_index + 1:]
    ]

  @property
  def stats(self):
    rows = self.properties
    col_row_index = map(itemgetter('col_name'), rows).index('Table Parameters:') + 1
    end_cols_index = map(itemgetter('data_type'), rows[col_row_index:]).index(None)
    return rows[col_row_index:][:end_cols_index]

  @property
  def has_complex(self):
    has_complex = False
    complex_types = ["struct", "array", "map", "uniontype"]
    patterns = [re.compile(typ) for typ in complex_types]

    for column in self.cols:
      if isinstance(column, dict) and 'data_type' in column:
        column_type = column['data_type']
        column_type = column.type
      if column_type and any(p.match(column_type.lower()) for p in patterns):
        has_complex = True
        break

    return has_complex

  @property
  def details(self):
    if self._details is None:
      props = dict([(stat['col_name'], stat['data_type']) for stat in self.properties if stat['col_name'] != 'Table Parameters:'])
      serde = props.get('SerDe Library:', '')
      
      self._details = {
          'stats': dict([(stat['data_type'], stat['comment']) for stat in self.stats]),
          'properties': {
            'owner': props.get('Owner:'),
            'create_time': props.get('CreateTime:'),
            'compressed': props.get('Compressed:', 'No') != 'No',
            'format': 'parquet' if 'ParquetHiveSerDe' in serde else ('text' if 'LazySimpleSerDe' in serde else serde.rsplit('.', 1)[-1])
        } 
      }

    return self._details


class HiveServerTRowSet2:
  def __init__(self, row_set, schema):
    self.row_set = row_set
    self.rows = row_set.rows
    self.schema = schema
    self.startRowOffset = row_set.startRowOffset

  def is_empty(self):
    return not self.row_set.columns or not HiveServerTColumnValue2(self.row_set.columns[0]).val

  def cols(self, col_names):
    cols_rows = []

    rs = HiveServerTRow2(self.row_set.columns, self.schema)
    cols = [rs.full_col(name) for name in col_names]

    for cols_row in itertools.izip(*cols):
      cols_rows.append(dict(itertools.izip(col_names, cols_row)))

    return cols_rows

  def __iter__(self):
    return self

  def next(self):
    if self.row_set.columns:
      return HiveServerTRow2(self.row_set.columns, self.schema)
    else:
      raise StopIteration


class HiveServerTRow2:
  def __init__(self, cols, schema):
    self.cols = cols
    self.schema = schema

  def col(self, colName):
    pos = self._get_col_position(colName)

  def full_col(self, colName):
    pos = self._get_col_position(colName)

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

  def fields(self):
    try:
      return [HiveServerTColumnValue2(field).val.pop(0) for field in self.cols]
    except IndexError:
      raise StopIteration


class HiveServerTColumnValue2:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.stringVal:
      return self._get_val(self.column_value.stringVal)
    elif self.column_value.i16Val is not None:
      return self._get_val(self.column_value.i16Val)
    elif self.column_value.i32Val is not None:
      return self._get_val(self.column_value.i32Val)
    elif self.column_value.i64Val is not None:
      return self._get_val(self.column_value.i64Val)
    elif self.column_value.doubleVal is not None:
      return self._get_val(self.column_value.doubleVal)
    elif self.column_value.boolVal is not None:
      return self._get_val(self.column_value.boolVal)
    elif self.column_value.byteVal is not None:
      return self._get_val(self.column_value.byteVal)
    elif self.column_value.binaryVal is not None:
      return self._get_val(self.column_value.binaryVal)

  @classmethod
  def _get_val(cls, column):
    column.values = cls.set_nulls(column.values, column.nulls)
    return column.values

  @classmethod
  def mark_nulls(cls, values, bytestring):
    mask = bytearray(bytestring)

    for n in mask:
      yield n & 0x01
      yield n & 0x02
      yield n & 0x04
      yield n & 0x08

      yield n & 0x10
      yield n & 0x20
      yield n & 0x40
      yield n & 0x80

  @classmethod
  def set_nulls(cls, values, bytestring):
      return values
    else:
      _values = [None if is_null else value for value, is_null in itertools.izip(values, cls.mark_nulls(values, bytestring))]
        _values.extend(values[len(_values):])
      return _values


class HiveServerDataTable(DataTable):
  def __init__(self, results, schema, operation_handle, query_server):
    self.schema = schema and schema.schema
    self.row_set = HiveServerTRowSet(results.results, schema)
    self.operation_handle = operation_handle
    if query_server['server_name'] == 'impala':
      self.has_more = results.hasMoreRows
    else:

  @property
  def ready(self):
    return True

  def cols(self):
    if self.schema:
      return [HiveServerTColumnDesc(col) for col in self.schema.columns]
    else:
      return []

  def rows(self):
    for row in self.row_set:
      yield row.fields()



class HiveServerTTableSchema:
  def __init__(self, columns, schema):
    self.columns = columns
    self.schema = schema

  def cols(self):
    try:
      return HiveServerTRowSet(self.columns, self.schema).cols(('col_name', 'data_type', 'comment'))
    except:
      cols = HiveServerTRowSet(self.columns, self.schema).cols(('name', 'type', 'comment'))
      for col in cols:
        col['col_name'] = col.pop('name')
        col['data_type'] = col.pop('type')
      return cols

  def col(self, colName):
    pos = self._get_col_position(colName)
    return HiveServerTColumnDesc(self.columns[pos]).val

  def _get_col_position(self, column_name):
    return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]


if beeswax_conf.THRIFT_VERSION.get() >= 7:
  HiveServerTRow = HiveServerTRow2
  HiveServerTRowSet = HiveServerTRowSet2
else:
  class HiveServerTRow:
    def __init__(self, row, schema):
      self.row = row
      self.schema = schema

    def col(self, colName):
      pos = self._get_col_position(colName)
      return HiveServerTColumnValue(self.row.colVals[pos]).val

    def _get_col_position(self, column_name):
      return filter(lambda (i, col): col.columnName == column_name, enumerate(self.schema.columns))[0][0]

    def fields(self):
      return [HiveServerTColumnValue(field).val for field in self.row.colVals]

  class HiveServerTRowSet:
    def __init__(self, row_set, schema):
      self.row_set = row_set
      self.rows = row_set.rows
      self.schema = schema
      self.startRowOffset = row_set.startRowOffset

    def is_empty(self):
      return len(self.rows) == 0

    def cols(self, col_names):
      cols_rows = []
      for row in self.rows:
        row = HiveServerTRow(row, self.schema)
        cols = {}
        for col_name in col_names:
          cols[col_name] = row.col(col_name)
        cols_rows.append(cols)
      return cols_rows

    def __iter__(self):
      return self

    def next(self):
      if self.rows:
        return HiveServerTRow(self.rows.pop(0), self.schema)
      else:
        raise StopIteration


class HiveServerTColumnValue:
  def __init__(self, tcolumn_value):
    self.column_value = tcolumn_value

  @property
  def val(self):
    if self.column_value.boolVal is not None:
      return self.column_value.boolVal.value
    elif self.column_value.byteVal is not None:
      return self.column_value.byteVal.value
    elif self.column_value.i16Val is not None:
      return self.column_value.i16Val.value
    elif self.column_value.i32Val is not None:
      return self.column_value.i32Val.value
    elif self.column_value.i64Val is not None:
      return self.column_value.i64Val.value
    elif self.column_value.doubleVal is not None:
      return self.column_value.doubleVal.value
    elif self.column_value.stringVal is not None:
      return self.column_value.stringVal.value


class HiveServerTColumnDesc:
  def __init__(self, column):
    self.column = column

  @property
  def name(self):
    return self.column.columnName

  @property
  def comment(self):
    return self.column.comment

  @property
  def type(self):
    return self.get_type(self.column.typeDesc)

  @classmethod
  def get_type(self, typeDesc):
    for ttype in typeDesc.types:
      if ttype.primitiveEntry is not None:
        return TTypeId._VALUES_TO_NAMES[ttype.primitiveEntry.type]
      elif ttype.mapEntry is not None:
        return ttype.mapEntry
      elif ttype.unionEntry is not None:
        return ttype.unionEntry
      elif ttype.arrayEntry is not None:
        return ttype.arrayEntry
      elif ttype.structEntry is not None:
        return ttype.structEntry
      elif ttype.userDefinedTypeEntry is not None:
        return ttype.userDefinedTypeEntry


class HiveServerClient:
  HS2_MECHANISMS = {
      'KERBEROS': 'GSSAPI',
      'NONE': 'PLAIN',
      'NOSASL': 'NOSASL',
      'LDAP': 'PLAIN',
      'PAM': 'PLAIN'
  }

  DEFAULT_TABLE_TYPES = [
    'TABLE',
    'VIEW',
  ]

  def __init__(self, query_server, user):
    self.query_server = query_server
    self.user = user

    use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password = self.get_security()
    LOG.info(
        '%s: use_sasl=%s, mechanism=%s, kerberos_principal_short_name=%s, impersonation_enabled=%s, auth_username=%s' % (
        self.query_server['server_name'], use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username)
    )

    self.use_sasl = use_sasl
    self.kerberos_principal_short_name = kerberos_principal_short_name
    self.impersonation_enabled = impersonation_enabled

    if self.query_server['server_name'] == 'impala':
      from impala import conf as impala_conf

      ssl_enabled = impala_conf.SSL.ENABLED.get()
      ca_certs = impala_conf.SSL.CACERTS.get()
      keyfile = impala_conf.SSL.KEY.get()
      certfile = impala_conf.SSL.CERT.get()
      validate = impala_conf.SSL.VALIDATE.get()
      timeout = impala_conf.SERVER_CONN_TIMEOUT.get()
    else:
      ssl_enabled = hiveserver2_use_ssl()
      ca_certs = beeswax_conf.SSL.CACERTS.get()
      keyfile = beeswax_conf.SSL.KEY.get()
      certfile = beeswax_conf.SSL.CERT.get()
      validate = beeswax_conf.SSL.VALIDATE.get()
      timeout = beeswax_conf.SERVER_CONN_TIMEOUT.get()

    if auth_username:
      username = auth_username
      password = auth_password
    else:
      username = user.username
      password = None

    thrift_class = TCLIService
    if self.query_server['server_name'] == 'impala':
      from ImpalaService import ImpalaHiveServer2Service
      thrift_class = ImpalaHiveServer2Service

    self._client = thrift_util.get_client(thrift_class.Client,
                                          query_server['server_host'],
                                          query_server['server_port'],
                                          service_name=query_server['server_name'],
                                          kerberos_principal=kerberos_principal_short_name,
                                          use_sasl=use_sasl,
                                          mechanism=mechanism,
                                          username=username,
                                          password=password,
                                          timeout_seconds=timeout,
                                          use_ssl=ssl_enabled,
                                          ca_certs=ca_certs,
                                          keyfile=keyfile,
                                          certfile=certfile,
                                          validate=validate,
                                          transport_mode=query_server.get('transport_mode', 'socket'),
                                          http_url=query_server.get('http_url', ''))


  def get_security(self):
    principal = self.query_server['principal']
    impersonation_enabled = False
    auth_password = self.query_server['auth_password']

    if principal:
      kerberos_principal_short_name = principal.split('/', 1)[0]
    else:
      kerberos_principal_short_name = None

    if self.query_server['server_name'] == 'impala':
        use_sasl = True
        mechanism = HiveServerClient.HS2_MECHANISMS['NONE']
      else:
        cluster_conf = cluster.get_cluster_conf_for_job_submission()
        use_sasl = cluster_conf is not None and cluster_conf.SECURITY_ENABLED.get()
        mechanism = HiveServerClient.HS2_MECHANISMS['KERBEROS']
      impersonation_enabled = self.query_server['impersonation_enabled']
    else:
      hive_mechanism = hive_site.get_hiveserver2_authentication()
      if hive_mechanism not in HiveServerClient.HS2_MECHANISMS:
        raise Exception(_('%s server authentication not supported. Valid are %s.') % (hive_mechanism, HiveServerClient.HS2_MECHANISMS.keys()))
      use_sasl = hive_mechanism in ('KERBEROS', 'NONE', 'LDAP', 'PAM')
      mechanism = HiveServerClient.HS2_MECHANISMS[hive_mechanism]
      impersonation_enabled = hive_site.hiveserver2_impersonation_enabled()

    return use_sasl, mechanism, kerberos_principal_short_name, impersonation_enabled, auth_username, auth_password


  def open_session(self, user):
    kwargs = {
        'client_protocol': beeswax_conf.THRIFT_VERSION.get() - 1,
        'configuration': {},
    }

    if self.impersonation_enabled:
      kwargs.update({'username': DEFAULT_USER})

        kwargs['configuration'].update({'impala.doas.user': user.username})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})
      kwargs['configuration'].update({'hive.server2.logging.operation.verbose': 'true'})
      kwargs['configuration'].update({'hive.server2.logging.operation.level': 'VERBOSE'})

      kwargs['configuration'].update({'hive.server2.proxy.user': user.username})

    if self.query_server['server_name'] == 'impala' and self.query_server['SESSION_TIMEOUT_S'] > 0:
      kwargs['configuration'].update({'idle_session_timeout': str(self.query_server['SESSION_TIMEOUT_S'])})

    LOG.info('Opening %s thrift session for user %s' % (self.query_server['server_name'], user.username))

    req = TOpenSessionReq(**kwargs)
    res = self._client.OpenSession(req)

    if res.status is not None and res.status.statusCode not in (TStatusCode.SUCCESS_STATUS,):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)

    sessionId = res.sessionHandle.sessionId
    LOG.info('Session %s opened' % repr(sessionId.guid))

    encoded_status, encoded_guid = HiveServerQueryHandle(secret=sessionId.secret, guid=sessionId.guid).get()
    properties = json.dumps(res.configuration)

    session = Session.objects.create(owner=user,
                                     application=self.query_server['server_name'],
                                     status_code=res.status.statusCode,
                                     secret=encoded_status,
                                     guid=encoded_guid,
                                     server_protocol_version=res.serverProtocolVersion,
                                     properties=properties)

    if not session.get_properties():
      session.properties = json.dumps(self.get_configuration())
      session.save()

    return session


  def call(self, fn, req, status=TStatusCode.SUCCESS_STATUS):
    session = Session.objects.get_session(self.user, self.query_server['server_name'])

    if session is None:
      session = self.open_session(self.user)

    if hasattr(req, 'sessionHandle') and req.sessionHandle is None:
      req.sessionHandle = session.get_handle()

    res = fn(req)

    if res.status.statusCode == TStatusCode.ERROR_STATUS and \
        re.search('Invalid SessionHandle|Invalid session|Client session expired', res.status.errorMessage or '', re.I):
      LOG.info('Retrying with a new session because for %s of %s' % (self.user, res))

      session = self.open_session(self.user)
      req.sessionHandle = session.get_handle()

      res = getattr(self._client, fn.attr)(req)

    if status is not None and res.status.statusCode not in (
        TStatusCode.SUCCESS_STATUS, TStatusCode.SUCCESS_WITH_INFO_STATUS, TStatusCode.STILL_EXECUTING_STATUS):
      if hasattr(res.status, 'errorMessage') and res.status.errorMessage:
        message = res.status.errorMessage
      else:
        message = ''
      raise QueryServerException(Exception('Bad status for request %s:\n%s' % (req, res)), message=message)
    else:
      return res


  def close_session(self, sessionHandle):
    req = TCloseSessionReq(sessionHandle=sessionHandle)
    return self._client.CloseSession(req)


  def get_databases(self, schemaName=None):
    req = TGetSchemasReq()
    if schemaName is not None:
      req.schemaName = schemaName
    if self.query_server['server_name'] == 'impala':
      req.schemaName = None

    res = self.call(self._client.GetSchemas, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    col = 'TABLE_SCHEM'
    return HiveServerTRowSet(results.results, schema.schema).cols((col,))


  def get_database(self, database):
    if self.query_server['server_name'] == 'impala':
      raise NotImplementedError(_("Impala has not implemented the 'DESCRIBE DATABASE' command: %(issue_ref)s") % {
        'issue_ref': "https://issues.cloudera.org/browse/IMPALA-2196"
      })

    query = 'DESCRIBE DATABASE EXTENDED `%s`' % (database)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=5000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    cols = ('db_name', 'comment', 'location','owner_name', 'owner_type', 'parameters')

    if len(HiveServerTRowSet(desc_results.results, desc_schema.schema).cols(cols)) != 1:
      raise ValueError(_("%(query)s returned more than 1 row") % {'query': query})



  def get_tables_meta(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    cols = ('TABLE_NAME', 'TABLE_TYPE', 'REMARKS')
    return HiveServerTRowSet(results.results, schema.schema).cols(cols)


  def get_tables(self, database, table_names, table_types=None):
    if not table_types:
      table_types = self.DEFAULT_TABLE_TYPES
    req = TGetTablesReq(schemaName=database, tableName=table_names, tableTypes=table_types)
    res = self.call(self._client.GetTables, req)

    results, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=5000)
    self.close_operation(res.operationHandle)

    return HiveServerTRowSet(results.results, schema.schema).cols(('TABLE_NAME',))


  def get_table(self, database, table_name, partition_spec=None):
    req = TGetTablesReq(schemaName=database, tableName=table_name)
    res = self.call(self._client.GetTables, req)

    table_results, table_schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    if partition_spec:
      query = 'DESCRIBE FORMATTED `%s`.`%s` PARTITION(%s)' % (database, table_name, partition_spec)
    else:
      query = 'DESCRIBE FORMATTED `%s`.`%s`' % (database, table_name)

    (desc_results, desc_schema), operation_handle = self.execute_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(operation_handle)

    return HiveServerTable(table_results.results, table_schema.schema, desc_results.results, desc_schema.schema)


  def execute_query(self, query, max_rows=1000):
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement=query.query['query'], max_rows=max_rows, configuration=configuration)


  def execute_query_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_FIRST):
    (results, schema), operation_handle = self.execute_statement(statement=statement, max_rows=max_rows, configuration=configuration, orientation=orientation)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def execute_async_query(self, query, statement=0):
    if statement == 0:
      if self.query_server['server_name'] == 'beeswax':
        for resource in query.get_configuration_statements():
          self.execute_statement(resource.strip())

    configuration = {}

    if self.query_server['server_name'] == 'impala' and self.query_server['querycache_rows'] > 0:
      configuration[IMPALA_RESULTSET_CACHE_SIZE] = str(self.query_server['querycache_rows'])

    configuration.update(self._get_query_configuration(query))
    query_statement = query.get_query_statement(statement)

    return self.execute_async_statement(statement=query_statement, confOverlay=configuration)


  def execute_statement(self, statement, max_rows=1000, configuration={}, orientation=TFetchOrientation.FETCH_NEXT):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      configuration['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=configuration)
    res = self.call(self._client.ExecuteStatement, req)

    return self.fetch_result(res.operationHandle, max_rows=max_rows, orientation=orientation), res.operationHandle


  def execute_async_statement(self, statement, confOverlay):
    if self.query_server['server_name'] == 'impala' and self.query_server['QUERY_TIMEOUT_S'] > 0:
      confOverlay['QUERY_TIMEOUT_S'] = str(self.query_server['QUERY_TIMEOUT_S'])

    req = TExecuteStatementReq(statement=statement.encode('utf-8'), confOverlay=confOverlay, runAsync=True)
    res = self.call(self._client.ExecuteStatement, req)

    return HiveServerQueryHandle(secret=res.operationHandle.operationId.secret,
                                 guid=res.operationHandle.operationId.guid,
                                 operation_type=res.operationHandle.operationType,
                                 has_result_set=res.operationHandle.hasResultSet,
                                 modified_row_count=res.operationHandle.modifiedRowCount)


  def fetch_data(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    results, schema = self.fetch_result(operation_handle, orientation, max_rows)
    return HiveServerDataTable(results, schema, operation_handle, self.query_server)


  def cancel_operation(self, operation_handle):
    req = TCancelOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CancelOperation, req)


  def close_operation(self, operation_handle):
    req = TCloseOperationReq(operationHandle=operation_handle)
    return self.call(self._client.CloseOperation, req)


  def get_columns(self, database, table):
    req = TGetColumnsReq(schemaName=database, tableName=table)
    res = self.call(self._client.GetColumns, req)

    res, schema = self.fetch_result(res.operationHandle, orientation=TFetchOrientation.FETCH_NEXT)
    self.close_operation(res.operationHandle)

    return res, schema


  def fetch_result(self, operation_handle, orientation=TFetchOrientation.FETCH_FIRST, max_rows=1000):
    if operation_handle.hasResultSet:
      fetch_req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows)
      res = self.call(self._client.FetchResults, fetch_req)
    else:
      res = TFetchResultsResp(results=TRowSet(startRowOffset=0, rows=[], columns=[]))

      meta_req = TGetResultSetMetadataReq(operationHandle=operation_handle)
      schema = self.call(self._client.GetResultSetMetadata, meta_req)
    else:
      schema = None

    return res, schema


  def fetch_log(self, operation_handle, orientation=TFetchOrientation.FETCH_NEXT, max_rows=1000):
    req = TFetchResultsReq(operationHandle=operation_handle, orientation=orientation, maxRows=max_rows, fetchType=1)
    res = self.call(self._client.FetchResults, req)

    if beeswax_conf.THRIFT_VERSION.get() >= 7:
      lines = res.results.columns[0].stringVal.values
    else:
      lines = imap(lambda r: r.colVals[0].stringVal.value, res.results.rows)

    return '\n'.join(lines)


  def get_operation_status(self, operation_handle):
    req = TGetOperationStatusReq(operationHandle=operation_handle)
    return self.call(self._client.GetOperationStatus, req)


  def explain(self, query):
    query_statement = query.get_query_statement(0)
    configuration = self._get_query_configuration(query)
    return self.execute_query_statement(statement='EXPLAIN %s' % query_statement, configuration=configuration, orientation=TFetchOrientation.FETCH_NEXT)


  def get_log(self, operation_handle):
    try:
      req = TGetLogReq(operationHandle=operation_handle)
      res = self.call(self._client.GetLog, req)
      return res.log
    except:
      LOG.exception('server does not support GetLog')

      return 'Server does not support GetLog()'


  def get_partitions(self, database, table_name, partition_spec=None, max_parts=None, reverse_sort=True):
    table = self.get_table(database, table_name)

    query = 'SHOW PARTITIONS `%s`.`%s`' % (database, table_name)
    if self.query_server['server_name'] == 'beeswax' and partition_spec:
      query += ' PARTITION(%s)' % partition_spec

    partition_table = self.execute_query_statement(query, max_rows=10000, orientation=TFetchOrientation.FETCH_NEXT)

    if self.query_server['server_name'] == 'impala':
      try:
        cols = [col.name for col in partition_table.cols()]
        partition_keys = cols[:stop]
        num_parts = len(partition_keys)

        rows = partition_table.rows()
        partition_values = [partition[:num_parts] for partition in rows]

        partition_values = partition_values[:-1]
        partitions_formatted = []

        for values in partition_values:
          zipped_parts = izip(partition_keys, values)
          partitions_formatted.append(['/'.join(['%s=%s' % (part[0], part[1]) for part in zipped_parts])])

        partitions = [PartitionValueCompatible(partition, table) for partition in partitions_formatted]
      except Exception, e:
        raise ValueError(_('Failed to determine partition keys for Impala table: `%s`.`%s`') % (database, table_name))
    else:
      partitions = [PartitionValueCompatible(partition, table) for partition in partition_table.rows()]

    if reverse_sort:
      partitions.reverse()

    if max_parts is None or max_parts <= 0:
      max_parts = LIST_PARTITIONS_LIMIT.get()

    return partitions[:max_parts]


  def get_configuration(self):
    configuration = {}

      query = 'SET'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_NEXT)
      configuration = dict((row[0], row[1]) for row in results.rows())
      query = 'SET -v'
      results = self.execute_query_statement(query, orientation=TFetchOrientation.FETCH_FIRST)
      config_whitelist = [config.lower() for config in CONFIG_WHITELIST.get()]
      properties = [(row[0].split('=')[0], row[0].split('=')[1]) for row in results.rows() if '=' in row[0]]
      configuration = dict((prop, value) for prop, value in properties if prop.lower() in config_whitelist)

    return configuration


  def _get_query_configuration(self, query):
    return dict([(setting['key'], setting['value']) for setting in query.settings])


class HiveServerTableCompatible(HiveServerTable):
  


  def __init__(self, hive_table):
    self.table = hive_table.table
    self.table_schema = hive_table.table_schema
    self.desc_results = hive_table.desc_results
    self.desc_schema = hive_table.desc_schema

    self.describe = HiveServerTTableSchema(self.desc_results, self.desc_schema).cols()
    self._details = None

  @property
  def cols(self):
    return [
        type('Col', (object,), {
          'name': col.get('col_name', '').strip() if col.get('col_name') else '',
          'type': col.get('data_type', '').strip() if col.get('data_type') else '',
          'comment': col.get('comment', '').strip() if col.get('comment') else ''
        }) for col in HiveServerTable.cols.fget(self)
  ]


class ResultCompatible:

  def __init__(self, data_table):
    self.data_table = data_table
    self.rows = data_table.rows
    self.has_more = data_table.has_more
    self.start_row = data_table.startRowOffset
    self.ready = True

  @property
  def columns(self):
    return self.cols()

  def cols(self):
    return [col.name for col in self.data_table.cols()]


class PartitionKeyCompatible:

  def __init__(self, name, type, comment):
    self.name = name
    self.type = type
    self.comment = comment

  def __eq__(self, other):
    return isinstance(other, PartitionKeyCompatible) and \
        self.name == other.name and \
        self.type == other.type and \
        self.comment == other.comment

  def __repr__(self):
    return 'PartitionKey(name:%s, type:%s, comment:%s)' % (self.name, self.type, self.comment)


class PartitionValueCompatible:

  def __init__(self, partition_row, table, properties=None):
    if properties is None:
      properties = {}
    partition = partition_row[0]
    parts = partition.split('/')
    self.partition_spec = ','.join(["`%s`='%s'" % (pv[0], pv[1]) for pv in [part.split('=') for part in parts]])
    self.values = [pv[1] for pv in [part.split('=') for part in parts]]
    self.sd = type('Sd', (object,), properties,)

  def __repr__(self):
    return 'PartitionValueCompatible(spec:%s, values:%s, sd:%s)' % (self.partition_spec, self.values, self.sd)


class ExplainCompatible:

  def __init__(self, data_table):
    self.textual = '\n'.join([line[0] for line in data_table.rows()])


class ResultMetaCompatible:

  def __init__(self):
    self.in_tablename = True


class HiveServerClientCompatible(object):
  


  def __init__(self, client):
    self._client = client
    self.user = client.user
    self.query_server = client.query_server


  def query(self, query, statement=0):
    return self._client.execute_async_query(query, statement)


  def get_state(self, handle):
    operationHandle = handle.get_rpc_handle()
    res = self._client.get_operation_status(operationHandle)
    return HiveServerQueryHistory.STATE_MAP[res.operationState]


  def get_operation_status(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.get_operation_status(operationHandle)


  def use(self, query):
    data = self._client.execute_query(query)
    self._client.close_operation(data.operation_handle)
    return data


  def explain(self, query):
    data_table = self._client.explain(query)
    data = ExplainCompatible(data_table)
    self._client.close_operation(data_table.operation_handle)
    return data


  def fetch(self, handle, start_over=False, max_rows=None):
    operationHandle = handle.get_rpc_handle()
    if max_rows is None:
      max_rows = 1000

      orientation = TFetchOrientation.FETCH_FIRST
    else:
      orientation = TFetchOrientation.FETCH_NEXT

    data_table = self._client.fetch_data(operationHandle, orientation=orientation, max_rows=max_rows)

    return ResultCompatible(data_table)


  def cancel_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.cancel_operation(operationHandle)


  def close(self, handle):
    return self.close_operation(handle)


  def close_operation(self, handle):
    operationHandle = handle.get_rpc_handle()
    return self._client.close_operation(operationHandle)


  def close_session(self, session):
    operationHandle = session.get_handle()
    return self._client.close_session(operationHandle)


  def dump_config(self):
    return 'Does not exist in HS2'


  def get_log(self, handle, start_over=True):
    operationHandle = handle.get_rpc_handle()

    if beeswax_conf.USE_GET_LOG_API.get() or self.query_server['server_name'] == 'impala':
      return self._client.get_log(operationHandle)
    else:
      if start_over:
        orientation = TFetchOrientation.FETCH_FIRST
      else:
        orientation = TFetchOrientation.FETCH_NEXT

      return self._client.fetch_log(operationHandle, orientation=orientation, max_rows=-1)


  def get_databases(self, schemaName=None):
    col = 'TABLE_SCHEM'
    return [table[col] for table in self._client.get_databases(schemaName)]


  def get_database(self, database):
    return self._client.get_database(database)


  def get_tables_meta(self, database, table_names, table_types=None):
    tables = self._client.get_tables_meta(database, table_names, table_types)
    massaged_tables = []
    for table in tables:
      massaged_tables.append({
        'name': table['TABLE_NAME'],
        'comment': table['REMARKS'],
        'type': table['TABLE_TYPE'].capitalize()}
      )
    return massaged_tables


  def get_tables(self, database, table_names, table_types=None):
    tables = [table['TABLE_NAME'] for table in self._client.get_tables(database, table_names, table_types)]
    tables.sort()
    return tables


  def get_table(self, database, table_name, partition_spec=None):
    table = self._client.get_table(database, table_name, partition_spec)
    return HiveServerTableCompatible(table)


  def get_columns(self, database, table):
    return self._client.get_columns(database, table)


  def get_default_configuration(self, *args, **kwargs):
    return []


  def get_results_metadata(self, handle):
    return ResultMetaCompatible()


  def create_database(self, name, description): raise NotImplementedError()


  def alter_table(self, dbname, tbl_name, new_tbl): raise NotImplementedError()


  def open_session(self, user):
    return self._client.open_session(user)


  def add_partition(self, new_part): raise NotImplementedError()


  def get_partition(self, *args, **kwargs): raise NotImplementedError()


  def get_partitions(self, database, table_name, partition_spec, max_parts, reverse_sort=True):
    return self._client.get_partitions(database, table_name, partition_spec, max_parts, reverse_sort)


  def alter_partition(self, db_name, tbl_name, new_part): raise NotImplementedError()

  def get_configuration(self):
    return self._client.get_configuration()





from __future__ import absolute_import

import os
from sys import argv

try:
    from collections import OrderedDict
except ImportError:
    from .packages.ordereddict import OrderedDict

from .utils import expand_path, is_collection

__all__ = ('Args', )


class Args(object):
    


    def __init__(self, args=None, no_argv=False):
        if not args:
            if not no_argv:
                self._args = argv[1:]
            else:
                self._args = []
        else:
            self._args = args


    def __len__(self):
        return len(self._args)


    def __repr__(self):
        return '<args %s>' % (repr(self._args))


    def __getitem__(self, i):
        try:
            return self.all[i]
        except IndexError:
            return None


    def __contains__(self, x):
        return self.first(x) is not None


    def get(self, x):
        

        try:
            return self.all[x]
        except IndexError:
            return None


    def get_with(self, x):
        

        return self.all[self.first_with(x)]


    def remove(self, x):
        


        def _remove(x):
            found = self.first(x)
            if found is not None:
                self._args.pop(found)

        if is_collection(x):
            for item in x:
                _remove(x)
        else:
            _remove(x)


    def pop(self, x):
        

        try:
            return self._args.pop(x)
        except IndexError:
            return None


    def any_contain(self, x):
        


        return bool(self.first_with(x))


    def contains(self, x):
        


        return self.__contains__(x)


    def first(self, x):
        


        def _find( x):
            try:
                return self.all.index(str(x))
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found is not None:
                    return found
            return None
        else:
            return _find(x)


    def first_with(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def first_without(self, x):
        


        def _find(x):
            try:
                for arg in self.all:
                    if x not in arg:
                        return self.all.index(arg)
            except ValueError:
                return None

        if is_collection(x):
            for item in x:
                found = _find(item)
                if found:
                    return found
            return None
        else:
            return _find(x)


    def start_with(self, x):
           


           _args = []

           for arg in self.all:
               if is_collection(x):
                   for _x in x:
                       if arg.startswith(x):
                           _args.append(arg)
                           break
               else:
                   if arg.startswith(x):
                       _args.append(arg)

           return Args(_args, no_argv=True)


    def contains_at(self, x, index):
        


        try:
            if is_collection(x):
                for _x in x:
                    if (_x in self.all[index]) or (_x == self.all[index]):
                        return True
                    else:
                        return False
            else:
                return (x in self.all[index])

        except IndexError:
            return False


    def has(self, x):
        


        try:
            self.all[x]
            return True
        except IndexError:
            return False


    def value_after(self, x):
        


        try:
            try:
                i = self.all.index(x)
            except ValueError:
                return None

            return self.all[i + 1]

        except IndexError:
            return None


    @property
    def grouped(self):
        


        collection = OrderedDict(_=Args(no_argv=True))

        _current_group = None

        for arg in self.all:
            if arg.startswith('-'):
                _current_group = arg
                collection.setdefault(arg, Args(no_argv=True))
            else:
                if _current_group:
                    collection[_current_group]._args.append(arg)
                else:
                    collection['_']._args.append(arg)

        return collection


    @property
    def last(self):
        


        try:
            return self.all[-1]
        except IndexError:
            return None


    @property
    def all(self):
        


        return self._args


    def all_with(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x in arg:
                        _args.append(arg)
                        break
            else:
                if x in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    def all_without(self, x):
        


        _args = []

        for arg in self.all:
            if is_collection(x):
                for _x in x:
                    if _x not in arg:
                        _args.append(arg)
                        break
            else:
                if x not in arg:
                    _args.append(arg)

        return Args(_args, no_argv=True)


    @property
    def flags(self):
        


        return self.start_with('-')


    @property
    def not_flags(self):
        


        return self.all_without('-')


    @property
    def files(self, absolute=False):
        


        _paths = []

        for arg in self.all:
            for path in expand_path(arg):
                if os.path.exists(path):
                    if absolute:
                        _paths.append(os.path.abspath(path))
                    else:
                        _paths.append(path)

        return _paths


    @property
    def not_files(self):
        


        _args = []

        for arg in self.all:
            if not len(expand_path(arg)):
                if not os.path.exists(arg):
                    _args.append(arg)

        return Args(_args, no_argv=True)

    @property
    def copy(self):
        


        return Args(self.all)


import sys
import pysam
import argparse
import random
import subprocess
import os
import bamsurgeon.replacereads as rr
import bamsurgeon.aligners as aligners
import bamsurgeon.mutation as mutation
import traceback

from bamsurgeon.common import *
from uuid import uuid4
from re import sub
from shutil import move
from multiprocessing import Pool
from collections import defaultdict as dd

sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 0)


def countReadCoverage(bam,chrom,start,end):
    

    coverage = []
    start = int(start)
    end = int(end)
    for i in range(end-start+1):
        coverage.append(0.0)

    i = 0
    if chrom in bam.references:
        for pcol in bam.pileup(chrom,start,end):
            n = 0
            if pcol.pos >= start and pcol.pos <= end:
                for read in pcol.pileups:
                    if read.alignment.mapq >= 0 and not read.alignment.is_duplicate:
                        n += 1
                coverage[i] = n
                i += 1

    return coverage


def replace(origbamfile, mutbamfile, outbamfile, seed=None):
    

    origbam = pysam.Samfile(origbamfile, 'rb')
    mutbam  = pysam.Samfile(mutbamfile, 'rb')
    outbam  = pysam.Samfile(outbamfile, 'wb', template=origbam)

    rr.replaceReads(origbam, mutbam, outbam, keepqual=True, seed=seed)

    origbam.close()
    mutbam.close()
    outbam.close()


def get_mutstr(chrom, start, end, ins, ref):
    return 'FIX get_mutstr'


def dictlist(fn):
    d = {}
    with open(fn, 'r') as inlist:
        for name in inlist:
            d[name.strip()] = True
    return d


def makemut(args, chrom, start, end, vaf, ins, avoid, alignopts):
    


    if args.seed is not None: random.seed(int(args.seed) + int(start))

    mutid = chrom + '_' + str(start) + '_' + str(end) + '_' + str(vaf)
    if ins is None:
        mutid += ':DEL'
    else:
        mutid += ':INS:' + ins

    try:
        bamfile = pysam.Samfile(args.bamFileName, 'rb')
        reffile = pysam.Fastafile(args.refFasta)
        tmpbams = []

        is_insertion = ins is not None
        is_deletion  = ins is None

        snvfrac = float(args.snvfrac)

        mutstr = get_mutstr(chrom, start, end, ins, reffile)

        del_ln = 0
        if is_deletion:
            del_ln = end-start

        mutpos = start
        mutpos_list = [start]

        cnv = None
        if (args.cnvfile):
            cnv = pysam.Tabixfile(args.cnvfile, 'r')

        log = open('addindel_logs_' + os.path.basename(args.outBamFile) + '/' + os.path.basename(args.outBamFile) + "." + "_".join((chrom,str(start),str(end))) + ".log",'w')

        tmpoutbamname = args.tmpdir + "/" + mutid + ".tmpbam." + str(uuid4()) + ".bam"
        outbam_muts = pysam.Samfile(tmpoutbamname, 'wb', template=bamfile)

        mutfail, hasSNP, maxfrac, outreads, mutreads, mutmates = mutation.mutate(args, log, bamfile, bammate, chrom, mutpos, mutpos+del_ln+1, mutpos_list, avoid=avoid, mutid_list=[mutid], is_insertion=is_insertion, is_deletion=is_deletion, ins_seq=ins, reffile=reffile, indel_start=start, indel_end=end)

        if mutfail:
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        readlist = []
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                readlist.append(extqname)

        print "len(readlist):",str(len(readlist))
        readlist.sort()
        random.shuffle(readlist)

        if len(readlist) < int(args.mindepth):
            sys.stderr.write("WARN\t" + now() + "\t" + mutid + "\tskipped, too few reads in region: " + str(len(readlist)) + "\n")
            outbam_muts.close()
            os.remove(tmpoutbamname)
            return None

        if vaf is None:
            if chrom in cnv.contigs:
                for cnregion in cnv.fetch(chrom,start,end):
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\t" + ' '.join(("copy number in snp region:",chrom,str(start),str(end),"=",str(cn))) + "\n")
                    if float(cn) > 0.0:
                        vaf = 1.0/float(cn)
                    else:
                        vaf = 0.0
                    sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tadjusted VAF: " + str(vaf) + "\n")
        else:
            sys.stdout.write("INFO\t" + now() + "\t" + mutid + "\tselected VAF: " + str(vaf) + "\n")

        lastread = int(len(readlist)*vaf)

        if lastread < int(args.minmutreads):
            if len(readlist) > int(args.minmutreads):
                lastread = int(args.minmutreads)
                sys.stdout.write("WARN\t" + now() + "\t" + mutid + "\tforced " + str(lastread) + " reads.\n")
            else:
                print "WARN\t" + now() + "\t" + mutid + "\tdropped site with fewer reads than --minmutreads"
                os.remove(tmpoutbamname)
                return None

        readtrack = dd(list)

        for readname in readlist:
            orig_name, readpos, pairend = readname.split(',')
            readtrack[orig_name].append('%s,%s' % (readpos, pairend))

        usedreads = 0
        newreadlist = []

        for orig_name in readtrack:
            for read_instance in readtrack[orig_name]:
                newreadlist.append(orig_name + ',' + read_instance)
                usedreads += 1

            if usedreads >= lastread:
                break

        readlist = newreadlist

        print "INFO\t" + now() + "\t" + mutid + "\tpicked: " + str(len(readlist)) + " reads"

        wrote = 0
        nmut = 0
        mut_out = {}
        for extqname,read in outreads.iteritems():
            if read.seq != mutreads[extqname]:
                if not args.nomut and extqname in readlist:
                    read.qual = qual
                    nmut += 1
            if not hasSNP or args.force:
                wrote += 1
                mut_out[extqname] = read

        muts_written = {}

        for extqname in mut_out:
            if extqname not in muts_written:
                outbam_muts.write(mut_out[extqname])
                muts_written[extqname] = True

                if mutmates[extqname] is not None:
                    mate_read = mutmates[extqname]

                    if mate_read.is_read2:
                    if not mate_read.is_paired:

                    mateqname = ','.join((mate_read.qname,str(mate_read.pos),pairname))

                    if mateqname in mut_out:
                        outbam_muts.write(mut_out[mateqname])
                        muts_written[mateqname] = True

                    else:
                        outbam_muts.write(mate_read)

        print "INFO\t" + now() + "\t" + mutid + "\twrote: " + str(wrote) + " reads, mutated: " + str(nmut) + " reads"

        if not hasSNP or args.force:
            outbam_muts.close()
            aligners.remap_bam(args.aligner, tmpoutbamname, args.refFasta, alignopts, mutid=mutid, paired=(not args.single), picardjar=args.picardjar)

            outbam_muts = pysam.Samfile(tmpoutbamname,'rb')
            coverwindow = 1
            incover  = countReadCoverage(bamfile,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)
            outcover = countReadCoverage(outbam_muts,chrom,mutpos-coverwindow,mutpos+del_ln+coverwindow)

            avgincover  = float(sum(incover))/float(len(incover)) 
            avgoutcover = float(sum(outcover))/float(len(outcover))
            spikein_frac = 0.0
            if wrote > 0:
                spikein_frac = float(nmut)/float(wrote)

            if (avgoutcover > 0 and avgincover > 0 and avgoutcover/avgincover >= float(args.coverdiff)) or args.force:
                tmpbams.append(tmpoutbamname)
                indelstr = ''
                if is_insertion:
                    indelstr = ':'.join(('INS', chrom, str(start), ins))
                else:
                    indelstr = ':'.join(('DEL', chrom, str(start), str(end)))

                snvstr = chrom + ":" + str(start) + "-" + str(end) + " (VAF=" + str(vaf) + ")"
                log.write("\t".join(("indel",indelstr,str(mutpos),mutstr,str(avgincover),str(avgoutcover),str(spikein_frac),str(maxfrac)))+"\n")
            else:
                outbam_muts.close()
                os.remove(tmpoutbamname)
                if os.path.exists(tmpoutbamname + '.bai'):
                    os.remove(tmpoutbamname + '.bai')
                    
                print "WARN\t" + now() + "\t" + mutid + "\tdropped for outcover/incover < " + str(args.coverdiff)
                return None

        outbam_muts.close()
        bamfile.close()
        bammate.close()
        log.close() 

        return sorted(tmpbams)
        
    except Exception, e:
        sys.stderr.write("*"*60 + "\nencountered error in mutation spikein: " + mutid + "\n")
        traceback.print_exc(file=sys.stdout)
        sys.stderr.write("*"*60 + "\n")
        if os.path.exists(tmpoutbamname):
            os.remove(tmpoutbamname)
        if os.path.exists(tmpoutbamname + '.bai'):
            os.remove(tmpoutbamname + '.bai')
        return None


def main(args):
    print "INFO\t" + now() + "\tstarting " + sys.argv[0] + " called with args: " + ' '.join(sys.argv) + "\n"
    bedfile = open(args.varFileName, 'r')
    reffile = pysam.Fastafile(args.refFasta)

    if not os.path.exists(args.bamFileName + '.bai'):
        sys.stderr.write("ERROR\t" + now() + "\tinput bam must be indexed, not .bai file found for " + args.bamFileName + " \n")
        sys.exit(1)

    alignopts = {}
    if args.alignopts is not None:
        alignopts = dict([o.split(':') for o in args.alignopts.split(',')])

    aligners.checkoptions(args.aligner, alignopts, args.picardjar)

    avoid = None
    if args.avoidreads is not None:
        avoid = dictlist(args.avoidreads)

    outbam_mutsfile = "addindel." + str(uuid4()) + ".muts.bam"
    bamfile = pysam.Samfile(args.bamFileName, 'rb')
    outbam_muts = pysam.Samfile(outbam_mutsfile, 'wb', template=bamfile)
    outbam_muts.close()
    bamfile.close()
    tmpbams = []

    if not os.path.exists(args.tmpdir):
        os.mkdir(args.tmpdir)
        print "INFO\t" + now() + "\tcreated tmp directory: " + args.tmpdir

    if not os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)):
        os.mkdir('addindel_logs_' + os.path.basename(args.outBamFile))
        print "created directory: addindel_logs_" + os.path.basename(args.outBamFile)

    assert os.path.exists('addindel_logs_' + os.path.basename(args.outBamFile)), "could not create output directory!"
    assert os.path.exists(args.tmpdir), "could not create temporary directory!"

    pool = Pool(processes=int(args.procs))
    results = []

    ntried = 0
    for bedline in bedfile:
        if ntried < int(args.numsnvs) or int(args.numsnvs) == 0:
            c = bedline.strip().split()
            chrom = c[0]
            start = int(c[1])
            end   = int(c[2])
            vaf   = float(c[3])
            type  = c[4]
            ins   = None

            assert type in ('INS', 'DEL')
            if type == 'INS':
                ins = c[5]

            result = pool.apply_async(makemut, [args, chrom, start, end, vaf, ins, avoid, alignopts])
            results.append(result)
            ntried += 1

    for result in results:
        try:
            tmpbamlist = result.get()
            if tmpbamlist is not None:
                for tmpbam in tmpbamlist:
                    if os.path.exists(tmpbam):
                        tmpbams.append(tmpbam)
        except AssertionError:
            print "****************************************************"
            print "* WARNING: assertion failed somewhere, check logs. *"
            print "****************************************************"

    if len(tmpbams) == 0:
        print "INFO\t" + now() + "\tno succesful mutations"
        sys.exit()

    tmpbams.sort()

    if len(tmpbams) == 1:
        os.rename(tmpbams[0],outbam_mutsfile)
    elif len(tmpbams) > 1:
        mergebams(tmpbams,outbam_mutsfile,maxopen=int(args.maxopen))

    bedfile.close()

    for bam in tmpbams:
        if os.path.exists(bam):
            os.remove(bam)
        if os.path.exists(bam + '.bai'):
            os.remove(bam + '.bai')

    if args.skipmerge:
        print "INFO\t" + now() + "\tskipping merge, plase merge reads from", outbam_mutsfile, "manually."
    else:
        if args.tagreads:
            from bamsurgeon.markreads import markreads
            tmp_tag_bam = 'tag.%s.bam' % str(uuid4())
            markreads(outbam_mutsfile, tmp_tag_bam)
            move(tmp_tag_bam, outbam_mutsfile)
            print "INFO\t" + now() + "\ttagged reads."

        print "INFO\t" + now() + "\tdone making mutations, merging mutations into", args.bamFileName, "-->", args.outBamFile
        replace(args.bamFileName, outbam_mutsfile, args.outBamFile, seed=args.seed)

        os.remove(outbam_mutsfile)
    
def run():
    parser = argparse.ArgumentParser(description='adds INDELs to reads, outputs modified reads as .bam along with mates')
    parser.add_argument('-v', '--varfile', dest='varFileName', required=True, help='Target regions to try and add a SNV, as BED')
    parser.add_argument('-f', '--bamfile', dest='bamFileName', required=True, help='sam/bam file from which to obtain reads')
    parser.add_argument('-r', '--reference', dest='refFasta', required=True, help='reference genome, fasta indexed with bwa index -a stdsw _and_ samtools faidx')
    parser.add_argument('-o', '--outbam', dest='outBamFile', required=True, help='.bam file name for output')
    parser.add_argument('-s', '--snvfrac', dest='snvfrac', default=1, help='maximum allowable linked SNP MAF (for avoiding haplotypes) (default = 1)')
    parser.add_argument('-m', '--mutfrac', dest='mutfrac', default=0.5, help='allelic fraction at which to make SNVs (default = 0.5)')
    parser.add_argument('-n', '--numsnvs', dest='numsnvs', default=0, help="maximum number of mutations to try (default: entire input)")
    parser.add_argument('-c', '--cnvfile', dest='cnvfile', default=None, help="tabix-indexed list of genome-wide absolute copy number values (e.g. 2 alleles = no change)")
    parser.add_argument('-d', '--coverdiff', dest='coverdiff', default=0.1, help="allow difference in input and output coverage (default=0.1)")
    parser.add_argument('-p', '--procs', dest='procs', default=1, help="split into multiple processes (default=1)")
    parser.add_argument('--picardjar', default=None, help='path to picard.jar')
    parser.add_argument('--mindepth', default=10, help='minimum read depth to make mutation (default = 10)')
    parser.add_argument('--maxdepth', default=2000, help='maximum read depth to make mutation (default = 2000)')
    parser.add_argument('--minmutreads', default=3, help='minimum number of mutated reads to output per site')
    parser.add_argument('--avoidreads', default=None, help='file of read names to avoid (mutations will be skipped if overlap)')
    parser.add_argument('--nomut', action='store_true', default=False, help="dry run")
    parser.add_argument('--det', action='store_true', default=False, help="deterministic base changes: make transitions only")
    parser.add_argument('--force', action='store_true', default=False, help="force mutation to happen regardless of nearby SNP or low coverage")
    parser.add_argument('--single', action='store_true', default=False, help="input BAM is simgle-ended (default is paired-end)")
    parser.add_argument('--maxopen', dest='maxopen', default=1000, help="maximum number of open files during merge (default 1000)")
    parser.add_argument('--requirepaired', action='store_true', default=False, help='skip mutations if unpaired reads are present')
    parser.add_argument('--aligner', default='backtrack', help='supported aligners: ' + ','.join(aligners.supported_aligners_bam))
    parser.add_argument('--alignopts', default=None, help='aligner-specific options as comma delimited list of option1:value1,option2:value2,...')
    parser.add_argument('--tagreads', action='store_true', default=False, help='add BS tag to altered reads')
    parser.add_argument('--skipmerge', action='store_true', default=False, help="final output is tmp file to be merged")
    parser.add_argument('--ignorepileup', action='store_true', default=False, help="do not check pileup depth in mutation regions")
    parser.add_argument('--tmpdir', default='addindel.tmp', help='temporary directory (default=addindel.tmp)')
    parser.add_argument('--seed', default=None, help='seed random number generation')
    args = parser.parse_args()
    main(args)

if __name__ == '__main__':
    run()





import contextlib


@contextlib.contextmanager
def looking_glass():
    import sys
    original_write = sys.stdout.write

    def reverse_write(text):
        original_write(text[::-1])

    sys.stdout.write = reverse_write
    try:
        yield 'JABBERWOCKY'
        msg = 'Please DO NOT divide by zero!'
    finally:
        if msg:





import sys, os
import vcf
import argparse
import pysam
from collections import OrderedDict


def match(subrec, trurec, vtype='SNV'):
    assert vtype in ('SNV', 'SV', 'INDEL')

    if vtype == 'SNV' and subrec.is_snp and trurec.is_snp:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'INDEL' and subrec.is_indel and trurec.is_indel:
        if subrec.POS == trurec.POS and subrec.REF == trurec.REF and subrec.ALT == trurec.ALT:
            return True

    if vtype == 'SV' and subrec.is_sv and trurec.is_sv:
        trustart, truend = expand_sv_ends(trurec)
        substart, subend = expand_sv_ends(subrec)

        if min(truend, subend) - max(trustart, substart) > 0:
            return True

    return False


def expand_sv_ends(rec):
    

    startpos, endpos = rec.start, rec.end
    assert rec.is_sv

    try:
        endpos = int(rec.INFO.get('END')[0])

        if rec.INFO.get('CIPOS'):
            ci = map(int, rec.INFO.get('CIPOS'))
            if ci[0] < 0:
                startpos += ci[0]

        if rec.INFO.get('CIEND'):
            ci = map(int, rec.INFO.get('CIEND')) 
            if ci[0] > 0:
                endpos += ci[0]

    except TypeError as e:
        sys.stderr.write("error expanding sv interval: " + str(e) + " for record: " + str(rec) + "\n")

    if startpos > endpos:
        endpos, startpos = startpos, endpos

    return startpos, endpos


def relevant(rec, vtype, ignorechroms):
    

    rel = (rec.is_snp and vtype == 'SNV') or (rec.is_sv and vtype == 'SV') or (rec.is_indel and vtype == 'INDEL')
    return rel and (ignorechroms is None or rec.CHROM not in ignorechroms)

def passfilter(rec, disabled=False):
    

    if disabled:
        return True
    if rec.FILTER is None or rec.FILTER == '.' or not rec.FILTER:
        return True
    return False


def svmask(rec, vcfh, truchroms):
    

    if rec.is_snp and rec.CHROM in truchroms:
        for overlap_rec in vcfh.fetch(rec.CHROM, rec.POS-1, rec.POS):
            if overlap_rec.is_sv:
                return True
    return False

def var_dist(v1, v2):
    

    assert v1.CHROM == v2.CHROM
    return abs(v1.POS-v2.POS)


def get_close_matches(var, vcf_fh, win, indels_only=True):
    


    matches = list(vcf_fh.fetch(var.CHROM, var.POS-win, var.POS+1+win))
    if indels_only:
        matches = [m for m in matches if m.is_indel]
    if len(matches) == 0:
        return []

    dist_map = [(m, var_dist(m, var)) for m in matches]
    return sorted(dist_map, key=lambda x: x[1])


def have_identical_haplotypes(v1, v2, ref):
    


    assert (v1.is_indel or v1.is_snp) and (v2.is_indel or v2.is_snp)

    if v1.CHROM != v2.CHROM:
        return False

    if v1.is_snp and v2.is_snp:
        assert v1.REF.upper() == v2.REF.upper()
        return str(v1.ALT[0]).upper() == str(v2.ALT[0]).upper()
    if v1.is_snp or v2.is_snp:
        return False

    assert v1.is_indel and v2.is_indel
    assert len(v1.ALT) == 1 and len(v2.ALT) == 1, (
    +        "Can't handle multi-allelic entries")

    start = min([v1.POS, v2.POS])-1
    end = max([v1.POS + max([len(v1.REF), len(v1.ALT[0])]),
               v2.POS + max([len(v2.REF), len(v2.ALT[0])])
               ])
    seq = list(ref.fetch(chrom, start, end).upper())

    if len(seq) != end-start:
        sys.stderr.write("WARN: Couldn't fetch full sequence window. Skipping"
                     " allele-aware comparison, otherwise indices would"
                     " be off\n")
        raise NotImplementedError

    v1_offset = v1.POS-1-start
    v2_offset = v2.POS-1-start
    v1_seq = seq[:v1_offset] + list(str(v1.ALT[0]).lower()) + seq[v1_offset+len(v1.REF):]
    v2_seq = seq[:v2_offset] + list(str(v2.ALT[0]).lower()) + seq[v2_offset+len(v2.REF):]
    if False:
        print "reference sequence context\t%s" % (''.join(seq))
        print "v1 (offset %d) %s\t%s" % (v1_offset, v1, ''.join(v1_seq))
        print "v2 (offset %d) %s\t%s" % (v2_offset, v2, ''.join(v2_seq))
        print

    try:
        assert seq[v1_offset] == v1.REF[0].upper()
        assert seq[v2_offset] == v2.REF[0].upper()
        assert len(v1_seq) == len(seq) - len(v1.REF) + len(v1.ALT[0])
        assert len(v2_seq) == len(seq) - len(v2.REF) + len(v2.ALT[0])
    except AssertionError:
        raise

    return ''.join(v1_seq).upper() == ''.join(v2_seq).upper()


def evaluate(submission, truth, vtype='SNV', reffa=None, ignorechroms=None, ignorepass=False, 
             fp_vcf=None, fn_vcf=None, tp_vcf=None,
             debug=False):
    


    assert vtype in ('SNV', 'SV', 'INDEL')
    subvcfh = vcf.Reader(filename=submission)
    truvcfh = vcf.Reader(filename=truth)

    fpvcfh = fnvcfh = tpvcfh = None
    if fp_vcf:
        fpvcfh = vcf.Writer(open(fp_vcf, 'w'), template=subvcfh)
    if fn_vcf:
        fnvcfh = vcf.Writer(open(fn_vcf, 'w'), template=subvcfh)
    if tp_vcf:
        tpvcfh = vcf.Writer(open(tp_vcf, 'w'), template=subvcfh)

    reffa_fh = None
    if reffa:
        reffa_fh  = pysam.Fastafile(reffa)
        if debug:
            print "DEBUG: Using haplotype aware indel comparison"
    
    tpcount = 0
    fpcount = 0
    subrecs = 0
    trurecs = 0

    truchroms = {}
    fns = OrderedDict()

    

    for trurec in truvcfh:
        if relevant(trurec, vtype, ignorechroms):
            trurecs += 1
            truchroms[trurec.CHROM] = True
            fns[str(trurec)] = trurec
            

    

    for subrec in subvcfh:
        if passfilter(subrec, disabled=ignorepass):
            if subrec.is_snp and vtype == 'SNV':
                if not svmask(subrec, truvcfh, truchroms):
                    subrecs += 1
            if subrec.is_sv and vtype == 'SV':
                subrecs += 1
            if subrec.is_indel and vtype == 'INDEL':
                subrecs += 1

        matched = False

        startpos, endpos = subrec.start, subrec.end

        if vtype == 'SV' and subrec.is_sv:
            startpos, endpos = expand_sv_ends(subrec)
        try:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and subrec.CHROM in truchroms:
                for trurec in truvcfh.fetch(subrec.CHROM, startpos, end=endpos):
                    if match(subrec, trurec, vtype=vtype) and str(trurec) not in used_truth:
                        matched = True
                    window = 100
                    for (trurec, _) in get_close_matches(subrec, truvcfh, window, indels_only=True):
                        if str(trurec) in used_truth:
                            continue
                        if have_identical_haplotypes(subrec, trurec, reffa_fh):
                            matched = True
                            if debug:
                                print "DEBUG: Rescuing %s which has same haplotype as %s" % (subrec, trurec)
                            break
            if matched:
                used_truth[str(trurec)] = True
                    
        except ValueError as e:
            sys.stderr.write("Warning: " + str(e) + "\n")

        if matched:
            tpcount += 1
            if tpvcfh:
                tpvcfh.write_record(subrec)
            if fns.has_key(str(trurec)):
                del fns[str(trurec)]            
        else:
            if relevant(subrec, vtype, ignorechroms) and passfilter(subrec, disabled=ignorepass) and not svmask(subrec, truvcfh, truchroms): 
                if fpvcfh:
                    fpvcfh.write_record(subrec)


    if fnvcfh:
        for fn in fns.values():
            fnvcfh.write_record(fn)


    print "tpcount, fpcount, subrecs, trurecs:"
    print tpcount, fpcount, subrecs, trurecs

    recall    = float(tpcount) / float(trurecs)
    if tpcount+fpcount > 0:
        precision = float(tpcount) / float(tpcount + fpcount)
    else:
        precision = 0.0
    f1score   = 0.0 if tpcount == 0 else 2.0*(precision*recall)/(precision+recall)

    for fh in [fpvcfh, fnvcfh, tpvcfh]:
        if fh:
            fh.close()
            
    return precision, recall, f1score

def main(args):

    chromlist = None
    if args.chromlist is not None:
        chromlist = args.chromlist.split(',')

    if not args.subvcf.endswith('.vcf') and not args.subvcf.endswith('.vcf.gz'):
        sys.stderr.write("submission VCF filename does not end in .vcf or .vcf.gz\n")
        sys.exit(1)

    if not os.path.exists(args.truvcf):
        sys.stderr.write("truth VCF does not exist.\n")
        sys.exit(1)
    if not os.path.exists(args.truvcf + '.tbi'):
        sys.stderr.write("truth VCF does not appear to be indexed. bgzip + tabix index required.\n")
        sys.exit(1)

    if args.mutype not in ('SV', 'SNV', 'INDEL'):
        sys.stderr.write("-m/--mutype must be either SV, SNV, or INDEL\n")
        sys.exit(1)

    result = evaluate(args.subvcf, args.truvcf, vtype=args.mutype, 
                      reffa=args.reffa, ignorechroms=chromlist, ignorepass=args.nonpass, 
                      fp_vcf=args.fp_vcf, fn_vcf=args.fn_vcf, tp_vcf=args.tp_vcf,
                      debug=args.debug)

    print "precision, recall, F1 score: " + ','.join(map(str, result))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="check vcf output against a 'truth' vcf")
    parser.add_argument('-v',  '--vcf',    dest='subvcf', required=True, help="VCF being submitted for evaluation")
    parser.add_argument('-t',  '--truth',  dest='truvcf', required=True, help="'Truth' VCF containing true positives")
    parser.add_argument('-f',  '--ref', dest='reffa', help="Reference fasta file (enables haplotype-ware indel comparison)")
    parser.add_argument('-m', '--mutype', dest='mutype', required=True, help="Mutation type: must be either SNV, SV, or INDEL")
    parser.add_argument('--ignore', dest='chromlist', default=None, help="(optional) comma-seperated list of chromosomes to ignore")
    parser.add_argument('--nonpass', dest='nonpass', action="store_true", help="evaluate all records (not just PASS records) in VCF")
    parser.add_argument('--fp', dest='fp_vcf', help="print false positive positions to this vcf-file")
    parser.add_argument('--tp', dest='tp_vcf', help="print true positive positions to this file")
    parser.add_argument('--fn', dest='fn_vcf', help="print false negatives positions to this file")
    parser.add_argument('--debug', dest='debug', action="store_true", help=argparse.SUPPRESS)
    args = parser.parse_args()
    main(args)


from __future__ import absolute_import

import os.path

from setuptools import setup, find_packages


for m in ('multiprocessing', 'billiard'):
    try:
        __import__(m)
    except ImportError:
        pass

ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__)))

with open('requirements-test.txt') as file:
    tests_require = file.read().splitlines()

with open('requirements.txt') as file:
    install_requires = file.read().splitlines()

setup(
    name='freight',
    version='0.0.0',
    author='David Cramer',
    author_email='dcramer@gmail.com',
    url='https://github.com/getsentry/freight',
    description='A deployment service',
    long_description=open('README.rst').read(),
    packages=find_packages(exclude=['tests']),
    zip_safe=False,
    install_requires=install_requires,
    extras_require={
        'test': tests_require,
    },
    license='Apache 2.0',
    include_package_data=True,
    classifiers=[
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Operating System :: OS Independent',
        'Topic :: Software Development'
    ],
)
from django.http import HttpResponse

from comrade.utils import extract
from comrade.exceptions import BadRequestError

try:
    import piston.utils

    import piston.emitters
except ImportError:
    pass
else:
    piston.emitters.Emitter.register('text/xml', piston.emitters.XMLEmitter,
            'text/xml; charset=utf-8')
    piston.emitters.Emitter.register('application/json',
            piston.emitters.JSONEmitter, 'application/json; charset=utf-8')


class ContentNegotiationMixin(object):
    


    api_context_include_keys = set()
    api_context_exclude_keys = set()
    fields = ()

    def get_fields(self):
        return self.fields

    def get_minimal_context_keys(self, context):
        

        if self.api_context_include_keys:
            return self.api_context_include_keys
        elif self.api_context_exclude_keys:
            return set(context.keys()) - self.api_context_exclude_keys
        else:
            return context.keys()

    def get_minimal_context(self, context):
        

        return extract(context, self.get_minimal_context_keys(context))

    def get_api_context_data(self, context):
        return self.get_minimal_context(context)

    def get_api_response(self, context, **kwargs):
        

        accepted_types = self._determine_accepted_types(self.request)
        for accepted_type in accepted_types:
            try:
                emitter, ct = piston.emitters.Emitter.get(accepted_type)
            except ValueError:
                pass
            else:
                srl = emitter(self.get_api_context_data(context), {}, None,
                        self.fields)
                try:
                    stream = srl.render(self.request)
                    if not isinstance(stream, HttpResponse):
                        response = HttpResponse(stream, mimetype=ct, **kwargs)
                    else:
                        response = stream
                    return response
                except piston.utils.HttpStatusCode, e:
                    return e.response

    def render_to_response(self, context, **kwargs):
        response = self.get_api_response(context, **kwargs)
        if not response:
            response = super(ContentNegotiationMixin, self
                    ).render_to_response(context, **kwargs)
        return response

    def _determine_accepted_types(self, request):
        

        if ("text/html" in request.accepted_types
                or request.accepted_types == ['*/*']):
            return []
        return request.accepted_types


class BatchJSONMixin(object):
    

    batch_json_key = 'list'

    def get_batch_json_key(self):
        return self.batch_json_key

    def get_batch_json(self, key=None):
        key = key or self.get_batch_json_key()
        batch = self.request.data.get(key)
        if batch is None:
            raise BadRequestError("Unexpected POSTed JSON format")
        return batch

    def post_json(self, request, *args, **kwargs):
        batch = self.get_batch_json()
        form_class = self.get_form_class()
        form = None
        for data in batch:
            self.form_data = data
            form = form_class(**self.get_form_kwargs())
            if not form.is_valid():
                return self.form_invalid(form)
            form.save()
        return self.get_success_response(form)

    def post(self, request, *args, **kwargs):
        content_type = self.request.META.get('CONTENT_TYPE', '')
        if ('application/json' in content_type
                and self.get_batch_json_key() in request.data):
            return self.post_json(request, *args, **kwargs)
        return super(BatchJSONMixin, self).post(request, *args, **kwargs)
import os

try:
    from local_settings import *

    db_engine = STACKTACH_DB_ENGINE
    db_name = STACKTACH_DB_NAME
    db_host = STACKTACH_DB_HOST
    db_username = STACKTACH_DB_USERNAME
    db_password = STACKTACH_DB_PASSWORD
    db_port = STACKTACH_DB_PORT
    install_dir = os.path.expanduser(STACKTACH_INSTALL_DIR)
except ImportError:
    db_engine = os.environ.get('STACKTACH_DB_ENGINE',
                               'django.db.backends.mysql')
    db_name = os.environ['STACKTACH_DB_NAME']
    db_host = os.environ.get('STACKTACH_DB_HOST', "")
    db_username = os.environ['STACKTACH_DB_USERNAME']
    db_password = os.environ['STACKTACH_DB_PASSWORD']
    db_port = os.environ.get('STACKTACH_DB_PORT', "")
    install_dir = os.environ['STACKTACH_INSTALL_DIR']

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ADMINS = (
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': db_engine,
        'NAME': db_name,
        'USER': db_username,
        'PASSWORD': db_password,
    }
}

TIME_ZONE = None

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

USE_I18N = True

USE_L10N = True

MEDIA_ROOT = ''

MEDIA_URL = ''

STATIC_ROOT = ''

STATIC_URL = '/static/'


STATICFILES_DIRS = (
    [install_dir + "static",]
)

STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)

SECRET_KEY = 'x=rgdy5@(*!$e5ou0j!q104+m5mt1d%ud9ujyykhklss7*um3t'

TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)

ROOT_URLCONF = 'stacktach.urls'
TEMPLATE_DIRS = (
    install_dir + "templates"
)
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'stacktach',
    'south'
)

SOUTH_TESTS_MIGRATE = False

ALLOWED_HOSTS = ['*']

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler',
            'filters': []
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'
from django.core.management.base import NoArgsCommand
from django.conf import settings
import os
import sys
import re
import codecs
import commands
import urllib2
import urllib
import json
import random
from time import sleep
import math
import datetime
from django_bitcoin import Wallet
from django_bitcoin.utils import bitcoind
from decimal import Decimal
import warnings
import twitter

class Command(NoArgsCommand):
    help = 'Tweet with LocalBitcoins.com account.'

    def handle_noargs(self, **options):
        final_wallets = []
        process_num = random.randint(0, 1000)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore",category=RuntimeWarning)
            for i in range(0, 3):
                w = Wallet.objects.create()
                addr = w.receiving_address()
                final_wallets.append(w)
        for w in final_wallets:
            if w.total_balance_sql() > 0:
                print str(process_num) + " error", w.id
                raise Exception("damn!")
        w1 = final_wallets[0]
        w2 = final_wallets[1]
        w3 = final_wallets[2]
        bitcoind.send(w1.static_receiving_address(), Decimal("0.001"))
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + "loading"
        w1.send_to_wallet(w2, Decimal("0.0002"))
        w1.send_to_wallet(w3, Decimal("0.0005"))
        w3.send_to_address(w1, Decimal("0.0004"))
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)
        while w1.total_balance_sql() <= 0:
            sleep(1)
            w1 = Wallet.objects.get(id=w1.id)
        print str(process_num) + "catching"
        print str(process_num) + " w1.last_balance " + str(w1.last_balance)
        print str(process_num) + " w2.last_balance " + str(w2.last_balance)
        print str(process_num) + " w3.last_balance " + str(w3.last_balance)


from __future__ import absolute_import, unicode_literals, print_function, division

from datetime import timedelta

from flask import Flask

from restfulgit.plumbing.routes import plumbing
from restfulgit.porcelain.routes import porcelain
from restfulgit.archives import archives
from restfulgit.utils.json_err_pages import json_error_page, register_general_error_handler
from restfulgit.utils.json import jsonify
from restfulgit.utils.cors import corsify


BLUEPRINTS = (plumbing, porcelain, archives)


class DefaultConfig(object):
    RESTFULGIT_DEFAULT_COMMIT_LIST_LIMIT = 50
    RESTFULGIT_ENABLE_CORS = False
    RESTFULGIT_CORS_ALLOWED_HEADERS = []
    RESTFULGIT_CORS_ALLOW_CREDENTIALS = False
    RESTFULGIT_CORS_MAX_AGE = timedelta(days=30)
    RESTFULGIT_CORS_ALLOWED_ORIGIN = "*"


def create_app(config_obj_dotted_path=None):
    app = Flask(__name__)

    app.config.from_object(DefaultConfig)
        app.config.from_object(config_obj_dotted_path)

    register_general_error_handler(app, json_error_page)

    for blueprint in BLUEPRINTS:
        app.register_blueprint(blueprint)

    @app.route('/')
    @corsify
    @jsonify
        links = []
        for rule in app.url_map.iter_rules():
            if str(rule).startswith("/repos"):
                links.append(str(rule))
        return links

    return app
import unittest
import warnings

import pytest
from proto_lib_fixture import proto_lib


@pytest.mark.usefixtures('proto_lib')
class DecimalDefaultsTest(unittest.TestCase):
    def setUp(self):
        from test_deprecated_field_proto import TestDeprecatedField
        warnings.filterwarnings("error")
        self.message = TestDeprecatedField()

    def test_deprecation_warning(self):
        with self.assertRaises(DeprecationWarning):
            x = self.message.old_field

        with self.assertRaises(DeprecationWarning):
            self.message.old_field = 4



import sys
import types

_opcodes_all = []

__all__ = ["dis", "disassemble", "distb", "disco",
           "findlinestarts", "findlabels"] + _opcodes_all
del _opcodes_all

_have_code = (types.MethodType, types.FunctionType, types.CodeType,
              types.ClassType, type)

def dis(x=None):
    

    if x is None:
        distb()
        return
    if isinstance(x, types.InstanceType):
        x = x.__class__
    if hasattr(x, 'im_func'):
        x = x.im_func
    if hasattr(x, 'func_code'):
        x = x.func_code
    if hasattr(x, '__dict__'):
        items = x.__dict__.items()
        items.sort()
        for name, x1 in items:
            if isinstance(x1, _have_code):
                print "Disassembly of %s:" % name
                try:
                    dis(x1)
                except TypeError, msg:
                    print "Sorry:", msg
                print
    elif hasattr(x, 'co_code'):
        disassemble(x)
    elif isinstance(x, str):
        disassemble_string(x)
    else:
        raise TypeError, \
              "don't know how to disassemble %s objects" % \
              type(x).__name__

def distb(tb=None):
    

    if tb is None:
        try:
            tb = sys.last_traceback
        except AttributeError:
            raise RuntimeError, "no last traceback to disassemble"
        while tb.tb_next: tb = tb.tb_next
    disassemble(tb.tb_frame.f_code, tb.tb_lasti)

def disassemble(co, lasti=-1):
    

    code = co.co_code
    labels = findlabels(code)
    linestarts = dict(findlinestarts(co))
    n = len(code)
    i = 0
    extended_arg = 0
    free = None
    while i < n:
        c = code[i]
        op = ord(c)
        if i in linestarts:
            if i > 0:
                print
            print "%3d" % linestarts[i],
        else:
            print '   ',

        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(20),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
            extended_arg = 0
            i = i+2
            if op == EXTENDED_ARG:
                extended_arg = oparg*65536L
            print repr(oparg).rjust(5),
            if op in hasconst:
                print '(' + repr(co.co_consts[oparg]) + ')',
            elif op in hasname:
                print '(' + co.co_names[oparg] + ')',
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                print '(' + co.co_varnames[oparg] + ')',
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
            elif op in hasfree:
                if free is None:
                    free = co.co_cellvars + co.co_freevars
                print '(' + free[oparg] + ')',
        print

def disassemble_string(code, lasti=-1, varnames=None, names=None,
                       constants=None):
    labels = findlabels(code)
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        if i == lasti: print '-->',
        else: print '   ',
        if i in labels: print '>>',
        else: print '  ',
        print repr(i).rjust(4),
        print opname[op].ljust(15),
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            print repr(oparg).rjust(5),
            if op in hasconst:
                if constants:
                    print '(' + repr(constants[oparg]) + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasname:
                if names is not None:
                    print '(' + names[oparg] + ')',
                else:
                    print '(%d)'%oparg,
            elif op in hasjrel:
                print '(to ' + repr(i + oparg) + ')',
            elif op in haslocal:
                if varnames:
                    print '(' + varnames[oparg] + ')',
                else:
                    print '(%d)' % oparg,
            elif op in hascompare:
                print '(' + cmp_op[oparg] + ')',
        print


def findlabels(code):
    

    labels = []
    n = len(code)
    i = 0
    while i < n:
        c = code[i]
        op = ord(c)
        i = i+1
        if op >= HAVE_ARGUMENT:
            oparg = ord(code[i]) + ord(code[i+1])*256
            i = i+2
            label = -1
            if op in hasjrel:
                label = i+oparg
            elif op in hasjabs:
                label = oparg
            if label >= 0:
                if label not in labels:
                    labels.append(label)
    return labels

def findlinestarts(code):
    

    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]
    line_increments = [ord(c) for c in code.co_lnotab[1::2]]

    lastlineno = None
    lineno = code.co_firstlineno
    addr = 0
    for byte_incr, line_incr in zip(byte_increments, line_increments):
        if byte_incr:
            if lineno != lastlineno:
                yield (addr, lineno)
                lastlineno = lineno
            addr += byte_incr
        lineno += line_incr
    if lineno != lastlineno:
        yield (addr, lineno)

def _test():
    

    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write("usage: python dis.py [-|file]\n")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == "-":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open(fn)
    source = f.read()
    if fn is not None:
        f.close()
    else:
        fn = "<stdin>"
    code = compile(source, fn, "exec")
    dis(code)

if __name__ == "__main__":
    _test()
import random
from datetime import datetime

from olympia import amo
from olympia.applications.models import AppVersion
from olympia.files.models import File
from olympia.versions.models import ApplicationsVersions, Version


def generate_version(addon, app=None):
    

    min_app_version = '4.0'
    max_app_version = '50.0'
    version = '%.1f' % random.uniform(0, 2)
    v = Version.objects.create(addon=addon, version=version)
    v.created = v.last_updated = datetime.now()
    v.save()
        av_min, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=min_app_version)
        av_max, _ = AppVersion.objects.get_or_create(application=app.id,
                                                     version=max_app_version)
        ApplicationsVersions.objects.get_or_create(application=app.id,
                                                   version=v, min=av_min,
                                                   max=av_max)
    File.objects.create(filename='%s-%s' % (v.addon_id, v.id), version=v,
                        platform=amo.PLATFORM_ALL.id, status=amo.STATUS_PUBLIC)
    return v





import sys
import os
import commands
import time

import sct_utils as sct
from msct_parser import Parser



class Param:
    def __init__(self):
        self.debug = 0
        self.outSuffix  = "_reg"
        self.fname_mask = ''
        self.padding = 5
        self.outlier_factor = 2

class Paramreg(object):
    def __init__(self, step='1', type='im', algo='syn', metric='MeanSquares', iter='10', shrink='1', smooth='0', gradStep='0.5', poly='3', window_length = '31'):
        self.step = step
        self.type = type
        self.algo = algo
        self.metric = metric
        self.iter = iter
        self.shrink = shrink
        self.smooth = smooth
        self.gradStep = gradStep
        self.window_length = window_length

    def update(self, paramreg_user):
        list_objects = paramreg_user.split(',')
        for object in list_objects:
            if len(object)<2:
                sct.printv('Please check parameter -p (usage changed)',1,type='error')

            obj = object.split('=')
            setattr(self, obj[0], obj[1])

class ParamregMultiStep:
    

    def __init__(self, listParam=[]):
        self.steps = dict()
        for stepParam in listParam:
            if isinstance(stepParam, Paramreg):
                self.steps[stepParam.step] = stepParam
            else:
                self.addStep(stepParam)

    def addStep(self, stepParam):
        param_reg = Paramreg()
        param_reg.update(stepParam)
        if param_reg.step != 0:
            if param_reg.step in self.steps:
                self.steps[param_reg.step].update(stepParam)
            else:
                self.steps[param_reg.step] = param_reg
        else:
            sct.printv("ERROR: parameters must contain 'step'", 1, 'error')


def main():

    fname_output = ''
    fname_mask = param.fname_mask
    fname_src_seg = ''

    start_time = time.time()
    status, path_sct = commands.getstatusoutput('echo $SCT_DIR')

    step1 = Paramreg()
    paramreg = ParamregMultiStep([step0, step1])

    parser = Parser(__file__)
    parser.usage.set_description('This program co-registers two 3D volumes. The deformation is non-rigid and is '
                                 'constrained along Z direction (i.e., axial plane). Hence, this function assumes '
                                 'that orientation of the destination image is axial (RPI). If you need to register '
                                 'two volumes with large deformations and/or different contrasts, it is recommended to '
                                 'input spinal cord segmentations (binary mask) in order to achieve maximum robustness.'
                                 ' The program outputs a warping field that can be used to register other images to the'
                                 ' destination image. To apply the warping field to another image, use '
                                 'sct_apply_transfo')
    parser.add_option(name="-i",
                      type_value="file",
                      description="Image source.",
                      mandatory=True,
                      example="src.nii.gz")
    parser.add_option(name="-d",
                      type_value="file",
                      description="Image destination.",
                      mandatory=True,
                      example="dest.nii.gz")
    parser.add_option(name="-iseg",
                      type_value="file",
                      description="Segmentation source.",
                      mandatory=False,
                      example="src_seg.nii.gz")
    parser.add_option(name="-dseg",
                      type_value="file",
                      description="Segmentation destination.",
                      mandatory=False,
                      example="dest_seg.nii.gz")
    parser.add_option(name="-m",
                      type_value="file",
                      description="Mask that can be created with sct_create_mask to improve accuracy over region of interest. "
                                  "This mask will be used on the destination image.",
                      mandatory=False,
                      example="mask.nii.gz")
    parser.add_option(name="-o",
                      type_value="file_output",
                      description="Name of output file.",
                      mandatory=False,
                      example="src_reg.nii.gz")
    parser.add_option(name="-p",
                      type_value=[[':'],'str'],
                      description=
+paramreg.steps['1'].algo+
+paramreg.steps['1'].metric+
+paramreg.steps['1'].iter+
+paramreg.steps['1'].shrink+
+paramreg.steps['1'].smooth+
+paramreg.steps['1'].gradStep+
+paramreg.steps['1'].poly+
+paramreg.steps['1'].window_length,
                      mandatory=False,
                      example="step=1,type=seg,algo=slicereg,metric=MeanSquares:step=2,type=im,algo=syn,metric=MI,iter=5,shrink=2")
    parser.add_option(name="-z",
                      type_value="int",
                      description=
,
                      mandatory=False,
                      default_value=param.padding)
    parser.add_option(name="-x",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='linear',
                      example=['nn', 'linear', 'spline'])
    parser.add_option(name="-r",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1'])
    parser.add_option(name="-v",
                      type_value="multiple_choice",
                      description=
,
                      mandatory=False,
                      default_value='1',
                      example=['0', '1', '2'])
    arguments = parser.parse(sys.argv[1:])

    fname_src = arguments['-i']
    fname_dest = arguments['-d']
    if '-iseg' in arguments:
        fname_src_seg = arguments['-iseg']
    if '-dseg' in arguments:
        fname_dest_seg = arguments['-dseg']
    if '-o' in arguments:
        fname_output = arguments['-o']
    if "-m" in arguments:
        fname_mask = arguments['-m']
    padding = arguments['-z']
    if "-p" in arguments:
        paramreg_user = arguments['-p']
        for paramStep in paramreg_user:
            paramreg.addStep(paramStep)

    interp = arguments['-x']
    remove_temp_files = int(arguments['-r'])
    verbose = int(arguments['-v'])

    if param.debug:
        print '\n*** WARNING: DEBUG MODE ON ***\n'
        status, path_sct_data = commands.getstatusoutput('echo $SCT_TESTING_DATA_DIR')
        fname_dest = path_sct_data+'/mt/mt1.nii.gz'
        fname_src = path_sct_data+'/t2/t2.nii.gz'
        param_user = '10,syn,0.5,MI'
        remove_temp_files = '0'
        verbose = 1

    print '\nInput parameters:'
    print '  Source .............. '+fname_src
    print '  Destination ......... '+fname_dest
    print '  Mask ................ '+fname_mask
    print '  Output name ......... '+fname_output
    print '  Remove temp files ... '+str(remove_temp_files)
    print '  Verbose ............. '+str(verbose)

    param.verbose = verbose
    param.padding = padding
    param.fname_mask = fname_mask
    param.remove_temp_files = remove_temp_files

    sct.printv('\nCheck if input data are 3D...', verbose)
    sct.check_if_3d(fname_src)
    sct.check_if_3d(fname_dest)

    sct.printv('\nCheck if destination data is RPI...', verbose)
    sct.check_if_rpi(fname_dest)

    path_src, file_src, ext_src = sct.extract_fname(fname_src)
    path_dest, file_dest, ext_dest = sct.extract_fname(fname_dest)

    if fname_output == '':
        file_out = file_src+"_reg"
        ext_out = ext_src
    else:
        path_out, file_out, ext_out = sct.extract_fname(fname_output)

    sct.printv('\nCreate temporary folder...', verbose)
    path_tmp = 'tmp.'+time.strftime("%y%m%d%H%M%S")
    status, output = sct.run('mkdir '+path_tmp, verbose)

    from sct_convert import convert
    sct.printv('\nCopying input data to tmp folder and convert to nii...', verbose)
    convert(fname_src, path_tmp+'/src.nii')
    convert(fname_dest, path_tmp+'/dest.nii')

    if fname_src_seg:
        convert(fname_src_seg, path_tmp+'/src_seg.nii')
        convert(fname_dest_seg, path_tmp+'/dest_seg.nii')

    if not fname_mask == '':
        convert(fname_mask, path_tmp+'/mask.nii.gz')

    os.chdir(path_tmp)


    warp_forward = []
    warp_inverse = []
    for i_step in range(0, len(paramreg.steps)):
        if paramreg.steps[str(i_step)].type == 'im':
            src = 'src.nii'
            dest = 'dest.nii'
            interp_step = 'linear'
        elif paramreg.steps[str(i_step)].type == 'seg':
            src = 'src_seg.nii'
            dest = 'dest_seg.nii'
            interp_step = 'nn'
        else:
            sct.run('ERROR: Wrong image type.', 1, 'error')
        if i_step > 0:
            sct.run('sct_apply_transfo -i '+src+' -d '+dest+' -w '+','.join(warp_forward)+' -o '+sct.add_suffix(src, '_reg')+' -x '+interp_step, verbose)
            src = sct.add_suffix(src, '_reg')
        warp_forward_out, warp_inverse_out = register(src, dest, paramreg, param, str(i_step))
        warp_forward.append(warp_forward_out)
        warp_inverse.append(warp_inverse_out)

    warp_forward_0 = warp_forward.pop(0)
    warp_forward.append(warp_forward_0)

    sct.printv('\nConcatenate transformations...', verbose)
    sct.run('sct_concat_transfo -w '+','.join(warp_forward)+' -d dest.nii -o warp_src2dest.nii.gz', verbose)
    warp_inverse.reverse()
    sct.run('sct_concat_transfo -w '+','.join(warp_inverse)+' -d dest.nii -o warp_dest2src.nii.gz', verbose)

    sct.printv('\nApply transfo source --> dest...', verbose)
    sct.run('sct_apply_transfo -i src.nii -o src_reg.nii -d dest.nii -w warp_src2dest.nii.gz -x '+interp, verbose)
    sct.printv('\nApply transfo dest --> source...', verbose)
    sct.run('sct_apply_transfo -i dest.nii -o dest_reg.nii -d src.nii -w warp_dest2src.nii.gz -x '+interp, verbose)

    os.chdir('..')

    sct.printv('\nGenerate output files...', verbose)
    fname_src2dest = sct.generate_output_file(path_tmp+'/src_reg.nii', path_out+file_out+ext_out, verbose)
    sct.generate_output_file(path_tmp+'/warp_src2dest.nii.gz', path_out+'warp_'+file_src+'2'+file_dest+'.nii.gz', verbose)
    fname_dest2src = sct.generate_output_file(path_tmp+'/dest_reg.nii', path_out+file_dest+'_reg'+ext_dest, verbose)
    sct.generate_output_file(path_tmp+'/warp_dest2src.nii.gz', path_out+'warp_'+file_dest+'2'+file_src+'.nii.gz', verbose)

    if remove_temp_files:
        sct.printv('\nRemove temporary files...', verbose)
        sct.run('rm -rf '+path_tmp, verbose)

    elapsed_time = time.time() - start_time
    sct.printv('\nFinished! Elapsed time: '+str(int(round(elapsed_time)))+'s', verbose)
    sct.printv('\nTo view results, type:', verbose)
    sct.printv('fslview '+fname_dest+' '+fname_src2dest+' &', verbose, 'info')
    sct.printv('fslview '+fname_src+' '+fname_dest2src+' &\n', verbose, 'info')



def register(src, dest, paramreg, param, i_step_str):

    ants_registration_params = {'rigid': '', 'affine': '', 'compositeaffine': '', 'similarity': '', 'translation': '',
                                'bspline': ',10', 'gaussiandisplacementfield': ',3,0',
                                'bsplinedisplacementfield': ',5,10', 'syn': ',3,0', 'bsplinesyn': ',1,3'}


    if paramreg.steps[i_step_str].metric == 'MI':
    else:

    if param.fname_mask:
        fname_mask = 'mask.nii.gz'
        masking = '-x mask.nii.gz'
    else:
        fname_mask = ''
        masking = ''

    if paramreg.steps[i_step_str].algo == 'slicereg':
        from msct_image import find_zmin_zmax
        src_th = sct.add_suffix(src, '_th')
        from msct_image import Image
        nii = Image(src)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(src_th)
        nii.save()
        dest_th = sct.add_suffix(dest, '_th')
        nii = Image(dest)
        data = nii.data
        data[data < 0.1] = 0
        nii.data = data
        nii.setFileName(dest_th)
        nii.save()
        zmin_src, zmax_src = find_zmin_zmax(src_th)
        zmin_dest, zmax_dest = find_zmin_zmax(dest_th)
        zmin_total = max([zmin_src, zmin_dest])
        zmax_total = min([zmax_src, zmax_dest])
        src_crop = sct.add_suffix(src, '_crop')
        sct.run('sct_crop_image -i '+src+' -o '+src_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        dest_crop = sct.add_suffix(dest, '_crop')
        sct.run('sct_crop_image -i '+dest+' -o '+dest_crop+' -dim 2 -start '+str(zmin_total)+' -end '+str(zmax_total), param.verbose)
        src = src_crop
        dest = dest_crop
        cmd = ('isct_antsSliceRegularizedRegistration '
               '-t Translation[0.5] '
               '-m '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+',Regular,0.2] '
               '-p '+paramreg.steps[i_step_str].poly+' '
               '-i '+paramreg.steps[i_step_str].iter+' '
               '-f 1 '
               '-s '+paramreg.steps[i_step_str].smooth+' '
               +masking)
        warp_forward_out = 'step'+i_step_str+'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str+'InverseWarp.nii.gz'

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_pointwise':
        from msct_register import register_slicereg2d_pointwise
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_pointwise(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                      warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, verbose=param.verbose)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_translation':
        from msct_register import register_slicereg2d_translation
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_translation(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Translation', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                        fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                        verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_rigid':
        from msct_register import register_slicereg2d_rigid
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_rigid(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Rigid', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                  fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                  verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_affine':
        from msct_register import register_slicereg2d_affine
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_affine(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='Affine', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                   fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                   verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_syn':
        from msct_register import register_slicereg2d_syn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_syn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='SyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo == 'slicereg2d_bsplinesyn':
        from msct_register import register_slicereg2d_bsplinesyn
        warp_forward_out = 'step'+i_step_str + 'Warp.nii.gz'
        warp_inverse_out = 'step'+i_step_str + 'InverseWarp.nii.gz'
        register_slicereg2d_bsplinesyn(src, dest, window_length=paramreg.steps[i_step_str].window_length, paramreg=Paramreg(step=paramreg.steps[i_step_str].step, type=paramreg.steps[i_step_str].type, algo='BSplineSyN', metric=paramreg.steps[i_step_str].metric, iter= paramreg.steps[i_step_str].iter, shrink=paramreg.steps[i_step_str].shrink, smooth=paramreg.steps[i_step_str].smooth, gradStep=paramreg.steps[i_step_str].gradStep),
                                       fname_mask=fname_mask, warp_forward_out=warp_forward_out, warp_inverse_out=warp_inverse_out, factor=param.outlier_factor, remove_temp_files=param.remove_temp_files,
                                       verbose=param.verbose, ants_registration_params=ants_registration_params)
        cmd = ('')

    elif paramreg.steps[i_step_str].algo.lower() in ants_registration_params:
        if not paramreg.steps[i_step_str].iter == '0':
            dest_pad = sct.add_suffix(dest, '_pad')
            sct.run('sct_maths -i '+dest+' -o '+dest_pad+' -pad 0x0x'+str(param.padding))
            dest = dest_pad

        cmd = ('isct_antsRegistration '
               '--dimensionality 3 '
               '--transform '+paramreg.steps[i_step_str].algo+'['+paramreg.steps[i_step_str].gradStep +
               ants_registration_params[paramreg.steps[i_step_str].algo.lower()]+'] '
               '--metric '+paramreg.steps[i_step_str].metric+'['+dest+','+src+',1,'+metricSize+'] '
               '--convergence '+paramreg.steps[i_step_str].iter+' '
               '--shrink-factors '+paramreg.steps[i_step_str].shrink+' '
               '--smoothing-sigmas '+paramreg.steps[i_step_str].smooth+'mm '
               '--restrict-deformation 1x1x0 '
               '--output [step'+i_step_str+','+src+'_regStep'+i_step_str+'.nii] '
               '--interpolation BSpline[3] '
               +masking)
        if param.verbose >= 1:
            cmd += ' --verbose 1'
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward_out = 'step'+i_step_str+'0GenericAffine.mat'
            warp_inverse_out = '-step'+i_step_str+'0GenericAffine.mat'
        else:
            warp_forward_out = 'step'+i_step_str+'0Warp.nii.gz'
            warp_inverse_out = 'step'+i_step_str+'0InverseWarp.nii.gz'
    else:
        sct.printv('\nERROR: algo '+paramreg.steps[i_step_str].algo+' does not exist. Exit program\n', 1, 'error')

    status, output = sct.run(cmd, param.verbose)

    if not os.path.isfile(warp_forward_out):
        sct.printv('\nERROR: file '+warp_forward_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    elif not os.path.isfile(warp_inverse_out):
        sct.printv('\nERROR: file '+warp_inverse_out+' doesn\'t exist (or is not a file).\n', 1, 'error')
        sct.printv(output, 1, 'error')
        sct.printv('\nERROR: ANTs failed. Exit program.\n', 1, 'error')
    else:
        if paramreg.steps[i_step_str].algo in ['rigid', 'affine']:
            warp_forward = 'warp_forward_'+i_step_str+'.mat'
            os.rename(warp_forward_out, warp_forward)
            warp_inverse = '-warp_forward_'+i_step_str+'.mat'
        else:
            warp_forward = 'warp_forward_'+i_step_str+'.nii.gz'
            warp_inverse = 'warp_inverse_'+i_step_str+'.nii.gz'
            os.rename(warp_forward_out, warp_forward)
            os.rename(warp_inverse_out, warp_inverse)

    print '*********************************************************************************************************************\n'
    print warp_forward, warp_inverse
    print '*********************************************************************************************************************\n'

    return warp_forward, warp_inverse



